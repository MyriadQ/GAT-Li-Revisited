{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62dade44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca12bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import ABIDE_Parser as Reader\n",
    "import keras\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "import pickle as pkl\n",
    "import time\n",
    "import copy\n",
    "import scipy.spatial.distance\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec38117",
   "metadata": {},
   "source": [
    "Some functions in Tensorflow 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ea0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot(shape, name=None):\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    var = tf.Variable(initial, name=name)\n",
    "    return var\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x,y)\n",
    "    else:\n",
    "        res = tf.matmul(x,y)\n",
    "    return res\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    correct_prediction = tf.equal(tf.round(preds), labels)\n",
    "    accuracy = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy)\n",
    "\n",
    "def tens(shape, name=None):\n",
    "    initial = tf.constant(10, tf.float32, shape)\n",
    "    return tf.Variable(initial, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a5fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gat_layer(object):\n",
    "    def __init__(self, input_dim,F_, placeholders,attn_heads=1,attn_heads_reduction='concat',\n",
    "                 activation=tf.nn.relu, use_bias=True,name_=''):\n",
    "        self.dropout_rate = placeholders['dropout']\n",
    "        self.in_drop = placeholders['in_drop']\n",
    "        self.name = 'gat_layer'+name_\n",
    "        self.vars = {}\n",
    "        self.act = activation\n",
    "        self.attn_heads = attn_heads  # Number of attention heads (K in the paper)\n",
    "        self.attn_heads_reduction = attn_heads_reduction  #\n",
    "        self.bias = use_bias\n",
    "        self.A = placeholders[\"adj\"]\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        with tf.variable_scope(self.name+'_vars'):\n",
    "            for i in range(self.attn_heads):\n",
    "                self.vars['weights_'+str(i)] = glorot([input_dim, F_], name='weights_' + str(i))\n",
    "                self.vars[\"attn_self_weights_\"+str(i)] = glorot([F_, 1], name='attn_self_weights_' + str(i))\n",
    "                self.vars[\"attn_neighs_weights_\"+str(i)] = glorot([F_, 1], name='attn_neighs_weights_' + str(i))\n",
    "        if self.bias:\n",
    "            self.vars['bias'] = zeros([F_],name='bias')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        X = inputs\n",
    "        if self.in_drop != 0.0:\n",
    "            X = tf.nn.dropout(X, 1-self.in_drop)\n",
    "        outputs = []\n",
    "        dense_mask = []\n",
    "\n",
    "        for head in range(self.attn_heads):\n",
    "            # Compute inputs to attention network\n",
    "            kernel = self.vars['weights_'+str(head)]\n",
    "            features = tf.tensordot(X, kernel, axes=1)  # (N x F')\n",
    "\n",
    "            # Compute feature combinations\n",
    "            attention_self_kernel = self.vars[\"attn_self_weights_\"+str(head)]\n",
    "            attention_neighs_kernel = self.vars[\"attn_neighs_weights_\"+str(head)]\n",
    "            attn_for_self = tf.tensordot(features, attention_self_kernel, axes=1)\n",
    "            attn_for_neighs = tf.tensordot(features, attention_neighs_kernel, axes=1)\n",
    "\n",
    "            # Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]\n",
    "            dense = attn_for_self + tf.transpose(attn_for_neighs, [0,2,1])  # (N x N) via broadcasting\n",
    "\n",
    "            #print(\"plus:\", dense.shape)\n",
    "\n",
    "            # Add nonlinearty\n",
    "            dense = tf.nn.leaky_relu(dense,alpha=0.2)\n",
    "\n",
    "            zero_vec = -9e15*tf.ones_like(dense)\n",
    "            dense = tf.where(self.A > 0.0, dense, zero_vec)\n",
    "            dense_mask.append(dense)\n",
    "\n",
    "            # Apply softmax to get attention coefficients\n",
    "            dense = tf.nn.softmax(dense)  # (N x N)\n",
    "\n",
    "            # Apply dropout to features and attention coefficients\n",
    "            dropout_attn = tf.nn.dropout(dense, 1-self.dropout_rate) # (N x N)\n",
    "            dropout_feat = tf.nn.dropout(features, 1-self.dropout_rate)  # (N x F')\n",
    "\n",
    "            # Linear combination with neighbors' features\n",
    "            node_features = tf.matmul(dropout_attn, dropout_feat)  # (N x F')\n",
    "\n",
    "            if self.bias:\n",
    "                node_features += self.vars[\"bias\"]\n",
    "\n",
    "            # Add output of attention head to final output\n",
    "            if self.attn_heads_reduction == 'concat':\n",
    "                outputs.append(self.act(node_features))\n",
    "            else:\n",
    "                outputs.append(node_features)\n",
    "\n",
    "        # Aggregate the heads' output according to the reduction method\n",
    "        if self.attn_heads_reduction == 'concat':\n",
    "            output = tf.concat(outputs, axis=-1)  # (N x KF')\n",
    "        else:\n",
    "            output = tf.add_n(outputs) / self.attn_heads  # N x F')\n",
    "            output = self.act(output)\n",
    "\n",
    "        return output, dense_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa51ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_layer(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,sparse_input=False, act=tf.nn.relu, bias=False, featureless=False,name_=''):\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.name = 'fc_layer'+name_\n",
    "        self.vars = {}\n",
    "        self.act = act\n",
    "\n",
    "        self.sparse_input = sparse_input\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        with tf.variable_scope(self.name+'_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim], name='weights')\n",
    "        if self.bias:\n",
    "            self.vars['bias'] = zeros([output_dim],name='bias')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = tf.nn.dropout(x,1-self.dropout)\n",
    "\n",
    "        output = tf.tensordot(x, self.vars['weights'], axes=1)\n",
    "\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "        return self.act(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d03e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer('node_num', 110, 'Number of Graph nodes')\n",
    "\n",
    "flags.DEFINE_integer('output_dim', 1, 'Number of output_dim')\n",
    "flags.DEFINE_float('learning_rate', 0.0001, 'Initial learning rate') #0.0005，0.0001，0.00005，0.00001，0.00003\n",
    "flags.DEFINE_integer('batch_num', 10, 'Number of epochs to train')\n",
    "flags.DEFINE_integer('epochs', 1000, 'Number of epochs to train')\n",
    "flags.DEFINE_integer('attn_heads', 5, 'Number of attention head')\n",
    "\n",
    "flags.DEFINE_integer('hidden1_gat', 24, 'Number of units in hidden layer 1 of gcn')\n",
    "flags.DEFINE_integer('output_gat', 3, 'Number of units in output layer 1 of gcn')\n",
    "\n",
    "flags.DEFINE_float('dropout', 0, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('in_drop', 0, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 15, 'Tolerance for early stopping (# of epochs).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "883bb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def __init__(self, placeholders, input_dim):\n",
    "        self.placeholders = placeholders\n",
    "        self.input_dim = input_dim\n",
    "        self.name = 'gat_mil'\n",
    "\n",
    "        self.gat_layers = []\n",
    "        self.fc_layers = []\n",
    "        self.gcn_activations = []\n",
    "        self.fc_activatinos = []\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        self.outputs = None\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "\n",
    "        self.node_prob = None\n",
    "        self.dense_mask = []\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "        self.opt_op = None\n",
    "\n",
    "        self.loss_explainer = 0\n",
    "        #self.optimizer_explainer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01) since im already using tf 1.x so the below code is used\n",
    "        self.optimizer_explainer = tf.train.AdamOptimizer(learning_rate=0.01) #replaced the above line\n",
    "        self.opt_op_explainer = None\n",
    "        self.M = tens((FLAGS.node_num, FLAGS.node_num), name='mask')\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        sigmoid_M = tf.sigmoid(self.M)\n",
    "        self.inputs = tf.multiply(self.inputs, sigmoid_M)\n",
    "\n",
    "        self.gcn_activations.append(self.inputs)\n",
    "\n",
    "        for layer in self.gat_layers:\n",
    "            hidden, dense_mask = layer(self.gcn_activations[-1])\n",
    "            self.gcn_activations.append(hidden)\n",
    "            self.dense_mask.append(dense_mask)\n",
    "\n",
    "\n",
    "        p_layer = self.fc_layers[0]\n",
    "        node_prob = p_layer(self.gcn_activations[-1])\n",
    "\n",
    "        tensor = tf.reshape(node_prob, shape=(-1, FLAGS.node_num))\n",
    "        layer = self.fc_layers[1]\n",
    "        attention_prob = layer(tensor)\n",
    "\n",
    "\n",
    "        attention_mul = tf.multiply(tensor, attention_prob)\n",
    "        self.outputs = tf.reduce_sum(attention_mul, 1, keep_dims=True)\n",
    "        print(self.outputs.shape)\n",
    "\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "\n",
    "        var_list = tf.trainable_variables()\n",
    "        var_list1 = []\n",
    "        for var in var_list:\n",
    "            if var != self.M:\n",
    "                var_list1.append(var)\n",
    "            elif var == self.M:\n",
    "                #stop = input(\"M exit!!!!!!!\")\n",
    "                pass\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss, var_list = [var_list1])\n",
    "        self.loss_explainer += tf.reduce_mean(tf.losses.log_loss(labels=self.placeholders['labels'], predictions=self.outputs))\n",
    "        self.opt_op_explainer = self.optimizer_explainer.minimize(self.loss_explainer, var_list=[self.M])\n",
    "\n",
    "    def _build(self):\n",
    "        self.gat_layers.append(gat_layer(input_dim=self.input_dim,F_=FLAGS.hidden1_gat, placeholders=self.placeholders,\n",
    "                                         attn_heads=FLAGS.attn_heads,attn_heads_reduction='concat',\n",
    "                                         activation=tf.nn.leaky_relu, use_bias=True,name_='1'))\n",
    "\n",
    "        self.gat_layers.append(gat_layer(input_dim=FLAGS.hidden1_gat*FLAGS.attn_heads,F_=FLAGS.output_gat, placeholders=self.placeholders,\n",
    "                                         attn_heads=3,attn_heads_reduction='average',\n",
    "                                         activation=tf.nn.leaky_relu, use_bias=True,name_='2'))\n",
    "\n",
    "        self.fc_layers.append(fc_layer(input_dim=FLAGS.output_gat, output_dim=FLAGS.output_dim, placeholders=self.placeholders,\n",
    "                                       act=tf.nn.sigmoid, dropout=True, name_='1'))\n",
    "\n",
    "        self.fc_layers.append(fc_layer(input_dim=FLAGS.node_num, output_dim=FLAGS.node_num, placeholders=self.placeholders,\n",
    "                                       act=tf.nn.softmax, dropout=True, name_='2'))\n",
    "\n",
    "    def _loss(self):\n",
    "        for var in self.gat_layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay*tf.nn.l2_loss(var)\n",
    "\n",
    "\n",
    "        self.loss += tf.reduce_mean(tf.losses.log_loss(labels=self.placeholders['labels'], predictions=self.outputs))\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = accuracy(self.outputs, self.placeholders['labels'])\n",
    "\n",
    "    def predict(self):\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d1f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = '/home/celery/Documents/Research/dataset/Outputs/'\n",
    "data_folder = os.path.join(root_folder, 'cpac/filt_noglobal')\n",
    "subject_IDs = np.genfromtxt('/home/celery/Documents/Research/dataset/valid_subject_ids.txt', dtype=str)\n",
    "subject_IDs = subject_IDs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d92db3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get label\n",
    "label_dict = Reader.get_label(subject_IDs)\n",
    "label_list = np.array([int(label_dict[x]) for x in subject_IDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13ae19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/celery/Documents/Research/dataset/Outputs/cpac/filt_global/mat/'\n",
    "def load_connectivity(subject_list, kind, atlas_name = 'ho'):\n",
    "    all_networks = []\n",
    "    for subject in subject_list:\n",
    "        fl = os.path.join(data_folder,\n",
    "                          subject + \"_\" + atlas_name + \"_\" + kind + \".mat\")\n",
    "        matrix = sio.loadmat(fl)['connectivity']\n",
    "        if atlas_name == 'ho':\n",
    "            matrix = np.delete(matrix, 82, axis=0)\n",
    "            matrix = np.delete(matrix, 82, axis=1)\n",
    "        all_networks.append(matrix)\n",
    "    all_networks=np.array(all_networks)\n",
    "    return all_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0339601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getconn_vector(subject_name0, kind, atlas):\n",
    "    subject_name = np.array(subject_name0)\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    conn_array = load_connectivity(subject_name, kind, atlas)\n",
    "    data_x = np.array(conn_array)\n",
    "\n",
    "    for subname in subject_name:\n",
    "        data_y.append([int(label_dict[subname])])\n",
    "\n",
    "    data_y = np.array(data_y)\n",
    "    return data_x, data_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edeaf7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = getconn_vector(subject_IDs, \"correlation\", \"ho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26ecd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_x = map(abs, X)\n",
    "adjs = np.array(list(abs_x))\n",
    "features = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "632c093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b014108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'adj': tf.placeholder(tf.float32, shape=(None,FLAGS.node_num,FLAGS.node_num)),\n",
    "    'features': tf.placeholder(tf.float32, shape=(None,FLAGS.node_num,FLAGS.node_num)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, 1)),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'in_drop': tf.placeholder_with_default(0., shape=()),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5426dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feed_dict(features, support, labels, placeholders):\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['adj']: support})\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91df014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "217cf587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(adjs, features, y):\n",
    "    shuffle_ix = np.random.permutation(np.arange(len(y)))\n",
    "    adjs = adjs[shuffle_ix]\n",
    "    features = features[shuffle_ix]\n",
    "    y = y[shuffle_ix]\n",
    "    return adjs, features, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af1ff071",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_index = []\n",
    "all_train_index = []\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
    "for train_index, test_index in skf.split(subject_IDs, label_list):\n",
    "    all_train_index.append(train_index)\n",
    "    all_test_index.append(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33243a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11bec599",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "l1, l2 = [], []\n",
    "\n",
    "k = 1 #changed to 3\n",
    "train_index = all_train_index[k]\n",
    "test_index = all_test_index[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f57e21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModelSavePath0 = '/home/celery/Documents/Research/NWPU-GAT/GAT-Li-Revisited/fold_e_mask%s/gat_e%s_weights_best.ckpt' % (str(k),str(k))\n",
    "bestModelSavePath1 = '/home/celery/Documents/Research/NWPU-GAT/GAT-Li-Revisited/fold_e_mask%s/gat_e%s_weights_best.ckpt' % (str(k),str(k))\n",
    "features_train, features_test = features[train_index], features[test_index]\n",
    "support_train, support_test = adjs[train_index], adjs[test_index]\n",
    "y_train, y_test = Y[train_index], Y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2a9b2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test shape: (177, 110, 110) support shape: (707, 110, 110)\n",
      "y_test [[1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "support_train, features_train, y_train = shuffle(support_train, features_train, y_train)\n",
    "support_test, features_test, y_test = shuffle(support_test, features_test, y_test)\n",
    "print(\"test shape:\",features_test.shape, \"support shape:\", support_train.shape)\n",
    "print(\"y_test\", y_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9e6daea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-bf741a9a2ac7>:59: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "<unknown>\n"
     ]
    }
   ],
   "source": [
    "model = Model(placeholders, input_dim=features.shape[2])\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "'''设置模型保存器'''\n",
    "m_saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ab78972",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_val = []\n",
    "batch_num = FLAGS.batch_num\n",
    "train_num = features_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7724d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 0.58461 train_acc= 0.73843 test_loss= 0.65453 test_acc= 0.65441 time= 7.78901\n",
      "Epoch: 0002 train_loss= 0.58221 train_acc= 0.74125 test_loss= 0.65364 test_acc= 0.65879 time= 11.83400\n",
      "Epoch: 0003 train_loss= 0.57977 train_acc= 0.74326 test_loss= 0.65395 test_acc= 0.65871 time= 13.44772\n",
      "Epoch: 0004 train_loss= 0.57744 train_acc= 0.74467 test_loss= 0.65429 test_acc= 0.65895 time= 13.08969\n",
      "Epoch: 0005 train_loss= 0.57527 train_acc= 0.74467 test_loss= 0.65465 test_acc= 0.65927 time= 13.20847\n",
      "Epoch: 0006 train_loss= 0.57329 train_acc= 0.74608 test_loss= 0.65504 test_acc= 0.65943 time= 13.19605\n",
      "Epoch: 0007 train_loss= 0.57132 train_acc= 0.74608 test_loss= 0.65545 test_acc= 0.65982 time= 13.33724\n",
      "Epoch: 0008 train_loss= 0.56947 train_acc= 0.74748 test_loss= 0.65586 test_acc= 0.65982 time= 13.24014\n",
      "Epoch: 0009 train_loss= 0.56768 train_acc= 0.74748 test_loss= 0.65629 test_acc= 0.65982 time= 14.20096\n",
      "Epoch: 0010 train_loss= 0.56593 train_acc= 0.74889 test_loss= 0.65673 test_acc= 0.65966 time= 13.29862\n",
      "Epoch: 0011 train_loss= 0.56427 train_acc= 0.75594 test_loss= 0.65717 test_acc= 0.66006 time= 13.16034\n",
      "Epoch: 0012 train_loss= 0.56263 train_acc= 0.75594 test_loss= 0.65761 test_acc= 0.66030 time= 13.37099\n",
      "Epoch: 0013 train_loss= 0.56102 train_acc= 0.75734 test_loss= 0.65805 test_acc= 0.66006 time= 13.28809\n",
      "Epoch: 0014 train_loss= 0.55945 train_acc= 0.75734 test_loss= 0.65849 test_acc= 0.65951 time= 14.04838\n",
      "Epoch: 0015 train_loss= 0.55790 train_acc= 0.75734 test_loss= 0.65895 test_acc= 0.65903 time= 13.08568\n",
      "Epoch: 0016 train_loss= 0.55637 train_acc= 0.75734 test_loss= 0.65941 test_acc= 0.65752 time= 13.43531\n",
      "Epoch: 0017 train_loss= 0.55489 train_acc= 0.76016 test_loss= 0.65985 test_acc= 0.65672 time= 13.28643\n",
      "early_stopping...\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(FLAGS.epochs):  #FLAGS.epochs\n",
    "    batches = range(round(train_num / batch_num))\n",
    "\n",
    "    costs = np.zeros((len(batches), 2))\n",
    "    accs = np.zeros((len(batches), 2))\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    for ib in batches:\n",
    "        from_i = ib * batch_num\n",
    "        to_i = (ib+1) * batch_num\n",
    "\n",
    "        features_train_batch = features_train[from_i:to_i]\n",
    "        support_train_batch = support_train[from_i:to_i]\n",
    "        y_train_batch = y_train[from_i:to_i]\n",
    "        feed_dict = construct_feed_dict(features_train_batch, support_train_batch, y_train_batch, placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "        feed_dict.update({placeholders['in_drop']: FLAGS.in_drop})\n",
    "\n",
    "        outs = sess.run([model.opt_op, model.loss, model.accuracy],feed_dict=feed_dict)\n",
    "\n",
    "        cost,acc,duration = evaluate(features_test, support_test, y_test, placeholders)\n",
    "        costs[ib] = [outs[1], cost]\n",
    "        accs[ib] = [outs[2], acc]\n",
    "\n",
    "    costs = costs.mean(axis=0)\n",
    "    cost_train, cost_test = costs\n",
    "    cost_val.append(cost_test)\n",
    "\n",
    "    accs = accs.mean(axis=0)\n",
    "    acc_train, acc_test = accs\n",
    "\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cost_train),\n",
    "        \"train_acc=\", \"{:.5f}\".format(acc_train), \"test_loss=\", \"{:.5f}\".format(cost_test),\n",
    "        \"test_acc=\", \"{:.5f}\".format(acc_test), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"early_stopping...\")\n",
    "        break\n",
    "\n",
    "m_saver.save(sess, bestModelSavePath0)\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "261caa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cost, test_acc, test_duration = evaluate(features_test, support_test, y_test, placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ef32695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 0.65264 accuracy= 0.67232 time= 0.09985\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "    \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "l1.append(test_acc)\n",
    "test_pre = model.predict()\n",
    "feed_dict = construct_feed_dict(features_test, support_test, y_test, placeholders)\n",
    "test_pre = sess.run([test_pre],feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68d9c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for p in test_pre[0]:\n",
    "    y_pred.append(round(p[0]))\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0e674d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy, sensivity, specificity, fscore, auc: 0.6610169491525424 0.5802469135802469 0.7291666666666666 0.6103896103896104 0.715406378600823\n"
     ]
    }
   ],
   "source": [
    "[[TN, FP], [FN, TP]] = confusion_matrix(y_test, y_pred, labels=[0, 1]).astype(float)\n",
    "acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "specificity = TN/(FP+TN)\n",
    "sensitivity = recall = TP/(TP+FN)\n",
    "fscore = f1_score(y_test, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(y_test,test_pre[0])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"accuracy, sensivity, specificity, fscore, auc:\", acc, sensitivity, specificity, fscore, roc_auc)\n",
    "result = [acc, sensitivity, specificity, fscore, roc_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ea0c7a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 0.51277 train_acc= 0.76439 test_loss= 0.65954 test_acc= 0.66102 time= 8.16179\n",
      "Epoch: 0002 train_loss= 0.51277 train_acc= 0.76439 test_loss= 0.65954 test_acc= 0.66102 time= 11.35161\n",
      "Epoch: 0003 train_loss= 0.51277 train_acc= 0.76439 test_loss= 0.65954 test_acc= 0.66102 time= 14.11217\n",
      "Epoch: 0004 train_loss= 0.51277 train_acc= 0.76439 test_loss= 0.65954 test_acc= 0.66102 time= 13.34091\n",
      "Epoch: 0005 train_loss= 0.51277 train_acc= 0.76439 test_loss= 0.65954 test_acc= 0.66102 time= 8.85981\n",
      "Epoch: 0006 train_loss= 0.51277 train_acc= 0.76439 test_loss= 0.65954 test_acc= 0.66102 time= 7.69943\n",
      "Epoch: 0007 train_loss= 0.51277 train_acc= 0.76439 test_loss= 0.65954 test_acc= 0.66102 time= 7.70726\n",
      "Epoch: 0008 train_loss= 0.51276 train_acc= 0.76439 test_loss= 0.65954 test_acc= 0.66102 time= 8.13073\n",
      "Epoch: 0009 train_loss= 0.51276 train_acc= 0.76439 test_loss= 0.65954 test_acc= 0.66102 time= 9.29169\n",
      "Epoch: 0010 train_loss= 0.51276 train_acc= 0.76439 test_loss= 0.65954 test_acc= 0.66102 time= 9.28433\n",
      "Epoch: 0011 train_loss= 0.51276 train_acc= 0.76439 test_loss= 0.65954 test_acc= 0.66102 time= 9.24189\n",
      "Epoch: 0012 train_loss= 0.51276 train_acc= 0.76439 test_loss= 0.65954 test_acc= 0.66102 time= 9.07308\n",
      "Epoch: 0013 train_loss= 0.51276 train_acc= 0.76439 test_loss= 0.65954 test_acc= 0.66102 time= 7.81815\n",
      "Epoch: 0014 train_loss= 0.51276 train_acc= 0.76439 test_loss= 0.65955 test_acc= 0.66102 time= 7.92616\n",
      "Epoch: 0015 train_loss= 0.51275 train_acc= 0.76439 test_loss= 0.65955 test_acc= 0.66102 time= 7.99485\n",
      "Epoch: 0016 train_loss= 0.51274 train_acc= 0.76439 test_loss= 0.65956 test_acc= 0.66102 time= 8.15165\n",
      "Epoch: 0017 train_loss= 0.51273 train_acc= 0.76439 test_loss= 0.65958 test_acc= 0.66102 time= 8.40008\n",
      "Epoch: 0018 train_loss= 0.51269 train_acc= 0.76579 test_loss= 0.65961 test_acc= 0.66102 time= 8.57355\n",
      "Epoch: 0019 train_loss= 0.51266 train_acc= 0.76579 test_loss= 0.65964 test_acc= 0.66102 time= 8.92423\n",
      "Epoch: 0020 train_loss= 0.51262 train_acc= 0.76579 test_loss= 0.65967 test_acc= 0.66102 time= 8.64128\n",
      "Epoch: 0021 train_loss= 0.51259 train_acc= 0.76579 test_loss= 0.65970 test_acc= 0.66102 time= 8.64954\n",
      "Epoch: 0022 train_loss= 0.51255 train_acc= 0.76439 test_loss= 0.65972 test_acc= 0.66102 time= 8.66855\n",
      "Epoch: 0023 train_loss= 0.51250 train_acc= 0.76439 test_loss= 0.65974 test_acc= 0.66102 time= 8.76957\n",
      "Epoch: 0024 train_loss= 0.51243 train_acc= 0.76439 test_loss= 0.65976 test_acc= 0.66102 time= 9.02162\n",
      "Epoch: 0025 train_loss= 0.51235 train_acc= 0.76439 test_loss= 0.65979 test_acc= 0.66102 time= 9.06772\n",
      "Epoch: 0026 train_loss= 0.51224 train_acc= 0.76439 test_loss= 0.65983 test_acc= 0.66102 time= 8.87397\n",
      "Epoch: 0027 train_loss= 0.51212 train_acc= 0.76439 test_loss= 0.65987 test_acc= 0.66102 time= 8.97791\n",
      "Epoch: 0028 train_loss= 0.51196 train_acc= 0.76439 test_loss= 0.65991 test_acc= 0.66102 time= 8.64721\n",
      "Epoch: 0029 train_loss= 0.51179 train_acc= 0.76439 test_loss= 0.65994 test_acc= 0.66102 time= 8.90510\n",
      "Epoch: 0030 train_loss= 0.51159 train_acc= 0.76720 test_loss= 0.65997 test_acc= 0.66102 time= 9.26467\n",
      "Epoch: 0031 train_loss= 0.51136 train_acc= 0.76720 test_loss= 0.65999 test_acc= 0.66102 time= 8.83631\n",
      "Epoch: 0032 train_loss= 0.51111 train_acc= 0.76720 test_loss= 0.66000 test_acc= 0.66102 time= 9.09628\n",
      "Epoch: 0033 train_loss= 0.51083 train_acc= 0.76720 test_loss= 0.66001 test_acc= 0.66102 time= 8.96783\n",
      "Epoch: 0034 train_loss= 0.51052 train_acc= 0.76720 test_loss= 0.66001 test_acc= 0.66102 time= 9.00006\n",
      "Epoch: 0035 train_loss= 0.51020 train_acc= 0.76720 test_loss= 0.66000 test_acc= 0.66102 time= 9.17774\n",
      "Epoch: 0036 train_loss= 0.50986 train_acc= 0.76720 test_loss= 0.65997 test_acc= 0.66102 time= 8.90728\n",
      "Epoch: 0037 train_loss= 0.50951 train_acc= 0.76720 test_loss= 0.65993 test_acc= 0.66102 time= 8.95351\n",
      "Epoch: 0038 train_loss= 0.50915 train_acc= 0.76720 test_loss= 0.65989 test_acc= 0.66102 time= 9.11304\n",
      "Epoch: 0039 train_loss= 0.50879 train_acc= 0.76720 test_loss= 0.65983 test_acc= 0.66102 time= 9.18087\n",
      "Epoch: 0040 train_loss= 0.50842 train_acc= 0.76579 test_loss= 0.65977 test_acc= 0.66102 time= 9.00526\n",
      "Epoch: 0041 train_loss= 0.50805 train_acc= 0.76579 test_loss= 0.65970 test_acc= 0.65855 time= 9.02843\n",
      "Epoch: 0042 train_loss= 0.50768 train_acc= 0.76720 test_loss= 0.65964 test_acc= 0.65537 time= 9.07551\n",
      "Epoch: 0043 train_loss= 0.50731 train_acc= 0.76861 test_loss= 0.65957 test_acc= 0.65537 time= 9.33702\n",
      "Epoch: 0044 train_loss= 0.50694 train_acc= 0.77002 test_loss= 0.65950 test_acc= 0.65537 time= 9.20131\n",
      "Epoch: 0045 train_loss= 0.50658 train_acc= 0.77002 test_loss= 0.65944 test_acc= 0.65537 time= 9.07210\n",
      "Epoch: 0046 train_loss= 0.50621 train_acc= 0.77143 test_loss= 0.65939 test_acc= 0.65537 time= 9.22326\n",
      "Epoch: 0047 train_loss= 0.50585 train_acc= 0.77143 test_loss= 0.65934 test_acc= 0.65537 time= 9.31796\n",
      "Epoch: 0048 train_loss= 0.50549 train_acc= 0.77143 test_loss= 0.65929 test_acc= 0.65537 time= 9.07249\n",
      "Epoch: 0049 train_loss= 0.50514 train_acc= 0.77284 test_loss= 0.65925 test_acc= 0.65537 time= 9.13956\n",
      "Epoch: 0050 train_loss= 0.50479 train_acc= 0.77284 test_loss= 0.65922 test_acc= 0.65537 time= 9.45978\n",
      "Epoch: 0051 train_loss= 0.50444 train_acc= 0.77425 test_loss= 0.65919 test_acc= 0.65537 time= 9.42632\n",
      "Epoch: 0052 train_loss= 0.50409 train_acc= 0.77425 test_loss= 0.65916 test_acc= 0.65537 time= 9.24511\n",
      "Epoch: 0053 train_loss= 0.50374 train_acc= 0.77565 test_loss= 0.65914 test_acc= 0.65537 time= 9.30554\n",
      "Epoch: 0054 train_loss= 0.50339 train_acc= 0.77706 test_loss= 0.65911 test_acc= 0.65537 time= 9.27047\n",
      "Epoch: 0055 train_loss= 0.50303 train_acc= 0.77706 test_loss= 0.65909 test_acc= 0.65537 time= 9.18135\n",
      "Epoch: 0056 train_loss= 0.50268 train_acc= 0.77706 test_loss= 0.65906 test_acc= 0.65537 time= 9.31060\n",
      "Epoch: 0057 train_loss= 0.50232 train_acc= 0.77706 test_loss= 0.65904 test_acc= 0.65537 time= 9.30200\n",
      "Epoch: 0058 train_loss= 0.50197 train_acc= 0.77706 test_loss= 0.65901 test_acc= 0.65998 time= 9.34750\n",
      "Epoch: 0059 train_loss= 0.50162 train_acc= 0.77847 test_loss= 0.65898 test_acc= 0.66102 time= 9.59898\n",
      "Epoch: 0060 train_loss= 0.50127 train_acc= 0.77847 test_loss= 0.65895 test_acc= 0.66102 time= 9.36697\n",
      "Epoch: 0061 train_loss= 0.50093 train_acc= 0.77847 test_loss= 0.65892 test_acc= 0.66102 time= 9.47874\n",
      "Epoch: 0062 train_loss= 0.50059 train_acc= 0.77847 test_loss= 0.65890 test_acc= 0.66102 time= 9.43060\n",
      "Epoch: 0063 train_loss= 0.50025 train_acc= 0.77847 test_loss= 0.65888 test_acc= 0.65640 time= 9.69741\n",
      "Epoch: 0064 train_loss= 0.49992 train_acc= 0.78129 test_loss= 0.65887 test_acc= 0.65537 time= 9.49689\n",
      "Epoch: 0065 train_loss= 0.49960 train_acc= 0.78129 test_loss= 0.65886 test_acc= 0.65537 time= 9.44219\n",
      "Epoch: 0066 train_loss= 0.49928 train_acc= 0.78129 test_loss= 0.65885 test_acc= 0.65537 time= 9.41752\n",
      "Epoch: 0067 train_loss= 0.49896 train_acc= 0.78129 test_loss= 0.65885 test_acc= 0.65537 time= 9.44408\n",
      "Epoch: 0068 train_loss= 0.49865 train_acc= 0.78129 test_loss= 0.65884 test_acc= 0.65537 time= 9.63243\n",
      "Epoch: 0069 train_loss= 0.49834 train_acc= 0.78129 test_loss= 0.65884 test_acc= 0.65537 time= 9.51150\n",
      "Epoch: 0070 train_loss= 0.49804 train_acc= 0.78129 test_loss= 0.65884 test_acc= 0.65712 time= 9.54403\n",
      "Epoch: 0071 train_loss= 0.49774 train_acc= 0.78270 test_loss= 0.65884 test_acc= 0.66102 time= 10.28645\n",
      "Epoch: 0072 train_loss= 0.49744 train_acc= 0.78270 test_loss= 0.65883 test_acc= 0.66102 time= 9.70836\n",
      "Epoch: 0073 train_loss= 0.49715 train_acc= 0.78270 test_loss= 0.65883 test_acc= 0.66102 time= 9.60560\n",
      "Epoch: 0074 train_loss= 0.49686 train_acc= 0.78270 test_loss= 0.65882 test_acc= 0.66102 time= 9.62568\n",
      "Epoch: 0075 train_loss= 0.49657 train_acc= 0.78270 test_loss= 0.65882 test_acc= 0.66102 time= 9.66450\n",
      "Epoch: 0076 train_loss= 0.49628 train_acc= 0.78129 test_loss= 0.65881 test_acc= 0.66563 time= 9.37829\n",
      "Epoch: 0077 train_loss= 0.49600 train_acc= 0.78129 test_loss= 0.65881 test_acc= 0.66667 time= 9.31917\n",
      "Epoch: 0078 train_loss= 0.49572 train_acc= 0.78129 test_loss= 0.65881 test_acc= 0.66667 time= 9.47768\n",
      "Epoch: 0079 train_loss= 0.49544 train_acc= 0.78410 test_loss= 0.65880 test_acc= 0.66667 time= 9.31610\n",
      "Epoch: 0080 train_loss= 0.49517 train_acc= 0.78551 test_loss= 0.65880 test_acc= 0.66667 time= 9.32606\n",
      "Epoch: 0081 train_loss= 0.49490 train_acc= 0.78692 test_loss= 0.65880 test_acc= 0.66667 time= 9.45304\n",
      "Epoch: 0082 train_loss= 0.49463 train_acc= 0.78833 test_loss= 0.65880 test_acc= 0.66667 time= 9.98377\n",
      "Epoch: 0083 train_loss= 0.49436 train_acc= 0.78974 test_loss= 0.65881 test_acc= 0.66667 time= 9.56099\n",
      "Epoch: 0084 train_loss= 0.49409 train_acc= 0.79115 test_loss= 0.65881 test_acc= 0.66667 time= 9.87875\n",
      "Epoch: 0085 train_loss= 0.49383 train_acc= 0.79115 test_loss= 0.65881 test_acc= 0.66667 time= 9.70748\n",
      "Epoch: 0086 train_loss= 0.49357 train_acc= 0.79396 test_loss= 0.65882 test_acc= 0.66667 time= 9.73133\n",
      "Epoch: 0087 train_loss= 0.49331 train_acc= 0.79396 test_loss= 0.65882 test_acc= 0.66667 time= 10.04395\n",
      "Epoch: 0088 train_loss= 0.49305 train_acc= 0.79396 test_loss= 0.65883 test_acc= 0.66667 time= 10.05547\n",
      "Epoch: 0089 train_loss= 0.49279 train_acc= 0.79396 test_loss= 0.65884 test_acc= 0.66667 time= 9.79638\n",
      "Epoch: 0090 train_loss= 0.49253 train_acc= 0.79396 test_loss= 0.65885 test_acc= 0.66667 time= 9.56729\n",
      "Epoch: 0091 train_loss= 0.49228 train_acc= 0.79396 test_loss= 0.65886 test_acc= 0.66667 time= 9.75317\n",
      "Epoch: 0092 train_loss= 0.49203 train_acc= 0.79396 test_loss= 0.65887 test_acc= 0.66667 time= 9.75630\n",
      "Epoch: 0093 train_loss= 0.49179 train_acc= 0.79537 test_loss= 0.65889 test_acc= 0.66667 time= 9.97345\n",
      "Epoch: 0094 train_loss= 0.49155 train_acc= 0.79678 test_loss= 0.65890 test_acc= 0.66667 time= 9.77641\n",
      "Epoch: 0095 train_loss= 0.49131 train_acc= 0.79819 test_loss= 0.65891 test_acc= 0.66667 time= 9.90222\n",
      "Epoch: 0096 train_loss= 0.49108 train_acc= 0.79819 test_loss= 0.65893 test_acc= 0.66667 time= 10.19806\n",
      "Epoch: 0097 train_loss= 0.49085 train_acc= 0.79819 test_loss= 0.65894 test_acc= 0.66667 time= 9.84776\n",
      "Epoch: 0098 train_loss= 0.49062 train_acc= 0.79819 test_loss= 0.65896 test_acc= 0.66667 time= 9.74074\n",
      "Epoch: 0099 train_loss= 0.49040 train_acc= 0.79819 test_loss= 0.65897 test_acc= 0.66667 time= 9.45605\n",
      "Epoch: 0100 train_loss= 0.49018 train_acc= 0.79819 test_loss= 0.65899 test_acc= 0.66667 time= 9.45818\n",
      "Epoch: 0101 train_loss= 0.48996 train_acc= 0.79819 test_loss= 0.65900 test_acc= 0.66667 time= 9.82956\n",
      "Epoch: 0102 train_loss= 0.48974 train_acc= 0.79819 test_loss= 0.65902 test_acc= 0.66667 time= 9.62139\n",
      "Epoch: 0103 train_loss= 0.48953 train_acc= 0.79819 test_loss= 0.65903 test_acc= 0.66667 time= 9.87287\n",
      "Epoch: 0104 train_loss= 0.48932 train_acc= 0.79819 test_loss= 0.65904 test_acc= 0.66667 time= 9.48324\n",
      "Epoch: 0105 train_loss= 0.48911 train_acc= 0.79819 test_loss= 0.65905 test_acc= 0.66667 time= 9.52079\n",
      "Epoch: 0106 train_loss= 0.48890 train_acc= 0.79819 test_loss= 0.65906 test_acc= 0.66667 time= 9.44492\n",
      "Epoch: 0107 train_loss= 0.48869 train_acc= 0.79819 test_loss= 0.65907 test_acc= 0.66667 time= 9.47036\n",
      "Epoch: 0108 train_loss= 0.48849 train_acc= 0.79819 test_loss= 0.65908 test_acc= 0.66667 time= 9.45248\n",
      "Epoch: 0109 train_loss= 0.48829 train_acc= 0.79819 test_loss= 0.65908 test_acc= 0.66667 time= 9.40413\n",
      "Epoch: 0110 train_loss= 0.48809 train_acc= 0.79819 test_loss= 0.65909 test_acc= 0.66667 time= 9.37264\n",
      "Epoch: 0111 train_loss= 0.48789 train_acc= 0.79819 test_loss= 0.65910 test_acc= 0.66667 time= 9.51719\n",
      "Epoch: 0112 train_loss= 0.48770 train_acc= 0.79819 test_loss= 0.65910 test_acc= 0.66667 time= 9.45432\n",
      "Epoch: 0113 train_loss= 0.48750 train_acc= 0.79819 test_loss= 0.65911 test_acc= 0.66667 time= 9.40248\n",
      "Epoch: 0114 train_loss= 0.48731 train_acc= 0.79819 test_loss= 0.65911 test_acc= 0.66667 time= 9.40163\n",
      "Epoch: 0115 train_loss= 0.48712 train_acc= 0.79819 test_loss= 0.65912 test_acc= 0.66667 time= 9.61079\n",
      "Epoch: 0116 train_loss= 0.48694 train_acc= 0.79819 test_loss= 0.65913 test_acc= 0.66667 time= 9.79451\n",
      "Epoch: 0117 train_loss= 0.48675 train_acc= 0.79960 test_loss= 0.65914 test_acc= 0.66667 time= 9.52804\n",
      "Epoch: 0118 train_loss= 0.48657 train_acc= 0.79960 test_loss= 0.65914 test_acc= 0.66667 time= 9.56029\n",
      "Epoch: 0119 train_loss= 0.48638 train_acc= 0.79960 test_loss= 0.65915 test_acc= 0.66842 time= 9.46665\n",
      "Epoch: 0120 train_loss= 0.48620 train_acc= 0.79960 test_loss= 0.65916 test_acc= 0.67216 time= 9.51941\n",
      "Epoch: 0121 train_loss= 0.48603 train_acc= 0.79960 test_loss= 0.65917 test_acc= 0.67232 time= 9.63924\n",
      "Epoch: 0122 train_loss= 0.48585 train_acc= 0.79960 test_loss= 0.65917 test_acc= 0.67232 time= 9.49889\n",
      "Epoch: 0123 train_loss= 0.48567 train_acc= 0.80101 test_loss= 0.65918 test_acc= 0.67232 time= 9.62113\n",
      "Epoch: 0124 train_loss= 0.48550 train_acc= 0.80101 test_loss= 0.65919 test_acc= 0.67232 time= 9.44002\n",
      "Epoch: 0125 train_loss= 0.48533 train_acc= 0.80101 test_loss= 0.65920 test_acc= 0.67232 time= 9.74898\n",
      "Epoch: 0126 train_loss= 0.48516 train_acc= 0.80101 test_loss= 0.65921 test_acc= 0.67232 time= 9.70239\n",
      "Epoch: 0127 train_loss= 0.48500 train_acc= 0.80101 test_loss= 0.65922 test_acc= 0.67232 time= 9.44570\n",
      "Epoch: 0128 train_loss= 0.48483 train_acc= 0.80101 test_loss= 0.65923 test_acc= 0.67232 time= 9.48117\n",
      "Epoch: 0129 train_loss= 0.48467 train_acc= 0.80101 test_loss= 0.65924 test_acc= 0.67232 time= 9.23191\n",
      "Epoch: 0130 train_loss= 0.48451 train_acc= 0.80241 test_loss= 0.65925 test_acc= 0.67232 time= 9.40932\n",
      "Epoch: 0131 train_loss= 0.48435 train_acc= 0.80241 test_loss= 0.65926 test_acc= 0.67232 time= 9.58880\n",
      "Epoch: 0132 train_loss= 0.48420 train_acc= 0.80241 test_loss= 0.65927 test_acc= 0.67232 time= 9.41149\n",
      "Epoch: 0133 train_loss= 0.48404 train_acc= 0.80241 test_loss= 0.65928 test_acc= 0.67232 time= 9.46151\n",
      "Epoch: 0134 train_loss= 0.48389 train_acc= 0.80241 test_loss= 0.65929 test_acc= 0.67232 time= 9.43413\n",
      "Epoch: 0135 train_loss= 0.48374 train_acc= 0.80241 test_loss= 0.65929 test_acc= 0.67232 time= 9.36665\n",
      "Epoch: 0136 train_loss= 0.48359 train_acc= 0.80241 test_loss= 0.65930 test_acc= 0.67232 time= 9.49556\n",
      "Epoch: 0137 train_loss= 0.48344 train_acc= 0.80241 test_loss= 0.65930 test_acc= 0.67232 time= 9.42530\n",
      "Epoch: 0138 train_loss= 0.48329 train_acc= 0.80241 test_loss= 0.65930 test_acc= 0.67232 time= 9.38250\n",
      "Epoch: 0139 train_loss= 0.48314 train_acc= 0.80241 test_loss= 0.65930 test_acc= 0.67232 time= 9.69508\n",
      "Epoch: 0140 train_loss= 0.48299 train_acc= 0.80241 test_loss= 0.65930 test_acc= 0.67232 time= 9.61100\n",
      "Epoch: 0141 train_loss= 0.48285 train_acc= 0.80241 test_loss= 0.65930 test_acc= 0.67232 time= 9.58359\n",
      "Epoch: 0142 train_loss= 0.48270 train_acc= 0.80241 test_loss= 0.65930 test_acc= 0.67232 time= 9.52962\n",
      "Epoch: 0143 train_loss= 0.48256 train_acc= 0.80241 test_loss= 0.65929 test_acc= 0.67232 time= 9.63543\n",
      "Epoch: 0144 train_loss= 0.48242 train_acc= 0.80241 test_loss= 0.65929 test_acc= 0.67232 time= 9.57987\n",
      "Epoch: 0145 train_loss= 0.48228 train_acc= 0.80101 test_loss= 0.65928 test_acc= 0.67232 time= 9.45943\n",
      "Epoch: 0146 train_loss= 0.48214 train_acc= 0.80101 test_loss= 0.65927 test_acc= 0.67232 time= 9.32069\n",
      "Epoch: 0147 train_loss= 0.48200 train_acc= 0.80241 test_loss= 0.65926 test_acc= 0.67232 time= 9.36984\n",
      "Epoch: 0148 train_loss= 0.48186 train_acc= 0.80241 test_loss= 0.65925 test_acc= 0.67232 time= 9.30907\n",
      "Epoch: 0149 train_loss= 0.48173 train_acc= 0.80241 test_loss= 0.65923 test_acc= 0.67232 time= 9.68269\n",
      "Epoch: 0150 train_loss= 0.48159 train_acc= 0.80241 test_loss= 0.65922 test_acc= 0.67232 time= 9.64839\n",
      "Epoch: 0151 train_loss= 0.48146 train_acc= 0.80241 test_loss= 0.65921 test_acc= 0.67232 time= 9.51938\n",
      "Epoch: 0152 train_loss= 0.48133 train_acc= 0.80382 test_loss= 0.65919 test_acc= 0.67232 time= 9.39531\n",
      "Epoch: 0153 train_loss= 0.48120 train_acc= 0.80382 test_loss= 0.65917 test_acc= 0.67232 time= 9.40326\n",
      "Epoch: 0154 train_loss= 0.48107 train_acc= 0.80382 test_loss= 0.65916 test_acc= 0.67232 time= 9.37570\n",
      "Epoch: 0155 train_loss= 0.48094 train_acc= 0.80382 test_loss= 0.65914 test_acc= 0.67232 time= 9.44325\n",
      "Epoch: 0156 train_loss= 0.48081 train_acc= 0.80382 test_loss= 0.65912 test_acc= 0.67232 time= 9.48196\n",
      "Epoch: 0157 train_loss= 0.48069 train_acc= 0.80382 test_loss= 0.65910 test_acc= 0.67232 time= 9.59822\n",
      "Epoch: 0158 train_loss= 0.48056 train_acc= 0.80382 test_loss= 0.65908 test_acc= 0.67216 time= 9.69307\n",
      "Epoch: 0159 train_loss= 0.48044 train_acc= 0.80523 test_loss= 0.65906 test_acc= 0.67080 time= 9.62775\n",
      "Epoch: 0160 train_loss= 0.48032 train_acc= 0.80382 test_loss= 0.65903 test_acc= 0.66786 time= 9.58821\n",
      "Epoch: 0161 train_loss= 0.48020 train_acc= 0.80523 test_loss= 0.65901 test_acc= 0.66810 time= 9.39001\n",
      "Epoch: 0162 train_loss= 0.48008 train_acc= 0.80664 test_loss= 0.65899 test_acc= 0.67080 time= 9.70335\n",
      "Epoch: 0163 train_loss= 0.47996 train_acc= 0.80664 test_loss= 0.65897 test_acc= 0.67168 time= 9.42762\n",
      "Epoch: 0164 train_loss= 0.47984 train_acc= 0.80664 test_loss= 0.65895 test_acc= 0.67232 time= 9.37517\n",
      "Epoch: 0165 train_loss= 0.47972 train_acc= 0.80664 test_loss= 0.65893 test_acc= 0.67232 time= 9.57258\n",
      "Epoch: 0166 train_loss= 0.47961 train_acc= 0.80664 test_loss= 0.65890 test_acc= 0.67232 time= 9.40595\n",
      "Epoch: 0167 train_loss= 0.47949 train_acc= 0.80664 test_loss= 0.65888 test_acc= 0.67232 time= 9.64465\n",
      "Epoch: 0168 train_loss= 0.47938 train_acc= 0.80805 test_loss= 0.65886 test_acc= 0.67232 time= 9.69048\n",
      "Epoch: 0169 train_loss= 0.47926 train_acc= 0.80946 test_loss= 0.65884 test_acc= 0.67232 time= 9.47293\n",
      "Epoch: 0170 train_loss= 0.47915 train_acc= 0.80946 test_loss= 0.65881 test_acc= 0.67232 time= 9.67626\n",
      "Epoch: 0171 train_loss= 0.47904 train_acc= 0.80946 test_loss= 0.65879 test_acc= 0.67232 time= 9.50596\n",
      "Epoch: 0172 train_loss= 0.47893 train_acc= 0.80946 test_loss= 0.65877 test_acc= 0.67232 time= 9.55032\n",
      "Epoch: 0173 train_loss= 0.47882 train_acc= 0.80946 test_loss= 0.65874 test_acc= 0.67232 time= 9.57458\n",
      "Epoch: 0174 train_loss= 0.47871 train_acc= 0.80946 test_loss= 0.65871 test_acc= 0.67232 time= 9.46978\n",
      "Epoch: 0175 train_loss= 0.47860 train_acc= 0.80946 test_loss= 0.65869 test_acc= 0.67232 time= 9.48857\n",
      "Epoch: 0176 train_loss= 0.47849 train_acc= 0.80946 test_loss= 0.65866 test_acc= 0.67232 time= 9.86101\n",
      "Epoch: 0177 train_loss= 0.47838 train_acc= 0.80946 test_loss= 0.65863 test_acc= 0.67232 time= 9.52303\n",
      "Epoch: 0178 train_loss= 0.47828 train_acc= 0.81087 test_loss= 0.65860 test_acc= 0.67232 time= 9.58515\n",
      "Epoch: 0179 train_loss= 0.47817 train_acc= 0.81087 test_loss= 0.65857 test_acc= 0.67232 time= 9.58045\n",
      "Epoch: 0180 train_loss= 0.47807 train_acc= 0.81087 test_loss= 0.65854 test_acc= 0.67232 time= 9.51560\n",
      "Epoch: 0181 train_loss= 0.47796 train_acc= 0.81087 test_loss= 0.65851 test_acc= 0.67232 time= 9.60144\n",
      "Epoch: 0182 train_loss= 0.47786 train_acc= 0.81087 test_loss= 0.65848 test_acc= 0.67232 time= 9.65266\n",
      "Epoch: 0183 train_loss= 0.47776 train_acc= 0.81227 test_loss= 0.65845 test_acc= 0.67232 time= 9.68945\n",
      "Epoch: 0184 train_loss= 0.47766 train_acc= 0.81227 test_loss= 0.65843 test_acc= 0.67232 time= 9.78623\n",
      "Epoch: 0185 train_loss= 0.47756 train_acc= 0.81227 test_loss= 0.65840 test_acc= 0.67232 time= 9.82657\n",
      "Epoch: 0186 train_loss= 0.47747 train_acc= 0.81227 test_loss= 0.65837 test_acc= 0.67232 time= 9.72039\n",
      "Epoch: 0187 train_loss= 0.47737 train_acc= 0.81227 test_loss= 0.65834 test_acc= 0.67232 time= 9.54900\n",
      "Epoch: 0188 train_loss= 0.47727 train_acc= 0.81227 test_loss= 0.65831 test_acc= 0.67232 time= 9.52273\n",
      "Epoch: 0189 train_loss= 0.47718 train_acc= 0.81227 test_loss= 0.65828 test_acc= 0.67232 time= 9.84825\n",
      "Epoch: 0190 train_loss= 0.47709 train_acc= 0.81227 test_loss= 0.65825 test_acc= 0.67232 time= 9.82078\n",
      "Epoch: 0191 train_loss= 0.47699 train_acc= 0.81227 test_loss= 0.65822 test_acc= 0.67232 time= 9.62120\n",
      "Epoch: 0192 train_loss= 0.47690 train_acc= 0.81227 test_loss= 0.65819 test_acc= 0.67232 time= 9.60188\n",
      "Epoch: 0193 train_loss= 0.47681 train_acc= 0.81227 test_loss= 0.65816 test_acc= 0.67232 time= 9.71095\n",
      "Epoch: 0194 train_loss= 0.47672 train_acc= 0.81227 test_loss= 0.65814 test_acc= 0.67232 time= 9.69638\n",
      "Epoch: 0195 train_loss= 0.47663 train_acc= 0.81368 test_loss= 0.65811 test_acc= 0.67232 time= 9.61278\n",
      "Epoch: 0196 train_loss= 0.47654 train_acc= 0.81368 test_loss= 0.65808 test_acc= 0.67232 time= 9.62216\n",
      "Epoch: 0197 train_loss= 0.47645 train_acc= 0.81368 test_loss= 0.65805 test_acc= 0.67232 time= 9.75730\n",
      "Epoch: 0198 train_loss= 0.47636 train_acc= 0.81509 test_loss= 0.65803 test_acc= 0.67232 time= 9.76119\n",
      "Epoch: 0199 train_loss= 0.47628 train_acc= 0.81650 test_loss= 0.65800 test_acc= 0.67232 time= 9.69229\n",
      "Epoch: 0200 train_loss= 0.47619 train_acc= 0.81650 test_loss= 0.65798 test_acc= 0.67232 time= 9.62258\n",
      "Epoch: 0201 train_loss= 0.47611 train_acc= 0.81650 test_loss= 0.65795 test_acc= 0.67232 time= 9.60116\n",
      "Epoch: 0202 train_loss= 0.47602 train_acc= 0.81650 test_loss= 0.65792 test_acc= 0.67232 time= 9.72825\n",
      "Epoch: 0203 train_loss= 0.47594 train_acc= 0.81650 test_loss= 0.65790 test_acc= 0.67232 time= 9.66038\n",
      "Epoch: 0204 train_loss= 0.47585 train_acc= 0.81791 test_loss= 0.65787 test_acc= 0.67232 time= 9.82631\n",
      "Epoch: 0205 train_loss= 0.47577 train_acc= 0.81791 test_loss= 0.65785 test_acc= 0.67232 time= 9.69104\n",
      "Epoch: 0206 train_loss= 0.47569 train_acc= 0.81791 test_loss= 0.65782 test_acc= 0.67232 time= 9.64879\n",
      "Epoch: 0207 train_loss= 0.47561 train_acc= 0.81791 test_loss= 0.65780 test_acc= 0.67232 time= 9.64319\n",
      "Epoch: 0208 train_loss= 0.47553 train_acc= 0.81932 test_loss= 0.65777 test_acc= 0.67232 time= 9.70780\n",
      "Epoch: 0209 train_loss= 0.47545 train_acc= 0.81932 test_loss= 0.65774 test_acc= 0.67232 time= 9.63174\n",
      "Epoch: 0210 train_loss= 0.47537 train_acc= 0.81932 test_loss= 0.65772 test_acc= 0.67232 time= 9.72258\n",
      "Epoch: 0211 train_loss= 0.47529 train_acc= 0.81932 test_loss= 0.65769 test_acc= 0.67232 time= 9.69204\n",
      "Epoch: 0212 train_loss= 0.47522 train_acc= 0.81791 test_loss= 0.65767 test_acc= 0.67232 time= 9.60695\n",
      "Epoch: 0213 train_loss= 0.47514 train_acc= 0.81791 test_loss= 0.65764 test_acc= 0.67232 time= 9.62174\n",
      "Epoch: 0214 train_loss= 0.47506 train_acc= 0.81932 test_loss= 0.65762 test_acc= 0.67232 time= 9.81633\n",
      "Epoch: 0215 train_loss= 0.47499 train_acc= 0.81932 test_loss= 0.65759 test_acc= 0.67232 time= 9.86811\n",
      "Epoch: 0216 train_loss= 0.47492 train_acc= 0.81932 test_loss= 0.65757 test_acc= 0.67232 time= 9.78187\n",
      "Epoch: 0217 train_loss= 0.47484 train_acc= 0.81932 test_loss= 0.65754 test_acc= 0.67232 time= 9.51281\n",
      "Epoch: 0218 train_loss= 0.47477 train_acc= 0.81791 test_loss= 0.65752 test_acc= 0.67232 time= 9.64755\n",
      "Epoch: 0219 train_loss= 0.47470 train_acc= 0.81791 test_loss= 0.65749 test_acc= 0.67232 time= 9.60472\n",
      "Epoch: 0220 train_loss= 0.47463 train_acc= 0.81791 test_loss= 0.65746 test_acc= 0.67232 time= 9.55298\n",
      "Epoch: 0221 train_loss= 0.47455 train_acc= 0.81932 test_loss= 0.65744 test_acc= 0.67232 time= 9.61778\n",
      "Epoch: 0222 train_loss= 0.47448 train_acc= 0.81932 test_loss= 0.65741 test_acc= 0.67232 time= 9.64380\n",
      "Epoch: 0223 train_loss= 0.47441 train_acc= 0.81932 test_loss= 0.65739 test_acc= 0.67232 time= 9.76619\n",
      "Epoch: 0224 train_loss= 0.47434 train_acc= 0.81932 test_loss= 0.65736 test_acc= 0.67232 time= 9.86061\n",
      "Epoch: 0225 train_loss= 0.47427 train_acc= 0.81932 test_loss= 0.65734 test_acc= 0.67232 time= 9.70771\n",
      "Epoch: 0226 train_loss= 0.47421 train_acc= 0.81791 test_loss= 0.65731 test_acc= 0.67232 time= 9.61345\n",
      "Epoch: 0227 train_loss= 0.47414 train_acc= 0.81791 test_loss= 0.65729 test_acc= 0.67232 time= 9.70331\n",
      "Epoch: 0228 train_loss= 0.47407 train_acc= 0.81791 test_loss= 0.65727 test_acc= 0.67232 time= 9.73677\n",
      "Epoch: 0229 train_loss= 0.47400 train_acc= 0.81791 test_loss= 0.65724 test_acc= 0.67232 time= 9.66384\n",
      "Epoch: 0230 train_loss= 0.47394 train_acc= 0.81791 test_loss= 0.65722 test_acc= 0.67232 time= 9.70250\n",
      "Epoch: 0231 train_loss= 0.47387 train_acc= 0.81791 test_loss= 0.65720 test_acc= 0.67232 time= 9.70232\n",
      "Epoch: 0232 train_loss= 0.47381 train_acc= 0.81932 test_loss= 0.65717 test_acc= 0.67232 time= 9.69387\n",
      "Epoch: 0233 train_loss= 0.47374 train_acc= 0.81932 test_loss= 0.65715 test_acc= 0.67232 time= 9.71004\n",
      "Epoch: 0234 train_loss= 0.47368 train_acc= 0.82072 test_loss= 0.65713 test_acc= 0.67232 time= 9.76337\n",
      "Epoch: 0235 train_loss= 0.47361 train_acc= 0.82072 test_loss= 0.65711 test_acc= 0.67232 time= 9.72631\n",
      "Epoch: 0236 train_loss= 0.47355 train_acc= 0.82213 test_loss= 0.65709 test_acc= 0.67232 time= 9.78799\n",
      "Epoch: 0237 train_loss= 0.47348 train_acc= 0.82213 test_loss= 0.65707 test_acc= 0.67232 time= 9.54408\n",
      "Epoch: 0238 train_loss= 0.47342 train_acc= 0.82213 test_loss= 0.65705 test_acc= 0.67232 time= 9.68099\n",
      "Epoch: 0239 train_loss= 0.47336 train_acc= 0.82213 test_loss= 0.65703 test_acc= 0.67232 time= 9.54553\n",
      "Epoch: 0240 train_loss= 0.47329 train_acc= 0.82354 test_loss= 0.65701 test_acc= 0.67232 time= 9.43335\n",
      "Epoch: 0241 train_loss= 0.47323 train_acc= 0.82354 test_loss= 0.65699 test_acc= 0.67232 time= 9.49642\n",
      "Epoch: 0242 train_loss= 0.47317 train_acc= 0.82354 test_loss= 0.65697 test_acc= 0.67232 time= 9.61676\n",
      "Epoch: 0243 train_loss= 0.47311 train_acc= 0.82636 test_loss= 0.65695 test_acc= 0.67232 time= 10.08046\n",
      "Epoch: 0244 train_loss= 0.47305 train_acc= 0.82777 test_loss= 0.65694 test_acc= 0.67232 time= 9.64068\n",
      "Epoch: 0245 train_loss= 0.47299 train_acc= 0.82777 test_loss= 0.65692 test_acc= 0.67232 time= 9.60931\n",
      "Epoch: 0246 train_loss= 0.47293 train_acc= 0.82777 test_loss= 0.65690 test_acc= 0.67232 time= 9.72626\n",
      "Epoch: 0247 train_loss= 0.47287 train_acc= 0.82918 test_loss= 0.65689 test_acc= 0.67232 time= 9.74109\n",
      "Epoch: 0248 train_loss= 0.47281 train_acc= 0.82918 test_loss= 0.65687 test_acc= 0.67232 time= 9.81401\n",
      "Epoch: 0249 train_loss= 0.47275 train_acc= 0.82918 test_loss= 0.65686 test_acc= 0.67232 time= 9.72537\n",
      "Epoch: 0250 train_loss= 0.47269 train_acc= 0.82918 test_loss= 0.65684 test_acc= 0.67232 time= 9.58104\n",
      "Epoch: 0251 train_loss= 0.47263 train_acc= 0.82918 test_loss= 0.65683 test_acc= 0.67232 time= 9.53353\n",
      "Epoch: 0252 train_loss= 0.47257 train_acc= 0.82777 test_loss= 0.65682 test_acc= 0.67232 time= 9.75063\n",
      "Epoch: 0253 train_loss= 0.47251 train_acc= 0.82777 test_loss= 0.65681 test_acc= 0.67232 time= 9.64606\n",
      "Epoch: 0254 train_loss= 0.47246 train_acc= 0.82777 test_loss= 0.65679 test_acc= 0.67232 time= 9.54789\n",
      "Epoch: 0255 train_loss= 0.47240 train_acc= 0.82777 test_loss= 0.65678 test_acc= 0.67232 time= 9.67797\n",
      "Epoch: 0256 train_loss= 0.47234 train_acc= 0.82777 test_loss= 0.65677 test_acc= 0.67232 time= 9.90969\n",
      "Epoch: 0257 train_loss= 0.47229 train_acc= 0.82777 test_loss= 0.65676 test_acc= 0.67232 time= 9.67252\n",
      "Epoch: 0258 train_loss= 0.47223 train_acc= 0.82777 test_loss= 0.65675 test_acc= 0.67232 time= 9.68959\n",
      "Epoch: 0259 train_loss= 0.47217 train_acc= 0.82777 test_loss= 0.65675 test_acc= 0.67232 time= 9.81853\n",
      "Epoch: 0260 train_loss= 0.47212 train_acc= 0.82918 test_loss= 0.65674 test_acc= 0.67232 time= 9.64341\n",
      "Epoch: 0261 train_loss= 0.47206 train_acc= 0.82918 test_loss= 0.65673 test_acc= 0.67232 time= 9.63327\n",
      "Epoch: 0262 train_loss= 0.47201 train_acc= 0.82918 test_loss= 0.65673 test_acc= 0.67232 time= 9.68976\n",
      "Epoch: 0263 train_loss= 0.47195 train_acc= 0.82918 test_loss= 0.65672 test_acc= 0.67232 time= 9.71926\n",
      "Epoch: 0264 train_loss= 0.47190 train_acc= 0.82918 test_loss= 0.65672 test_acc= 0.67232 time= 9.69855\n",
      "Epoch: 0265 train_loss= 0.47185 train_acc= 0.82918 test_loss= 0.65672 test_acc= 0.67232 time= 9.76788\n",
      "Epoch: 0266 train_loss= 0.47179 train_acc= 0.82918 test_loss= 0.65671 test_acc= 0.67232 time= 9.78421\n",
      "Epoch: 0267 train_loss= 0.47174 train_acc= 0.82918 test_loss= 0.65671 test_acc= 0.67232 time= 9.59691\n",
      "Epoch: 0268 train_loss= 0.47169 train_acc= 0.82918 test_loss= 0.65671 test_acc= 0.67232 time= 9.80586\n",
      "Epoch: 0269 train_loss= 0.47163 train_acc= 0.82777 test_loss= 0.65671 test_acc= 0.67232 time= 9.75534\n",
      "Epoch: 0270 train_loss= 0.47158 train_acc= 0.82777 test_loss= 0.65671 test_acc= 0.67232 time= 9.66012\n",
      "Epoch: 0271 train_loss= 0.47153 train_acc= 0.82777 test_loss= 0.65670 test_acc= 0.67232 time= 9.84059\n",
      "Epoch: 0272 train_loss= 0.47148 train_acc= 0.82777 test_loss= 0.65670 test_acc= 0.67232 time= 9.80585\n",
      "Epoch: 0273 train_loss= 0.47143 train_acc= 0.82777 test_loss= 0.65670 test_acc= 0.67232 time= 9.75575\n",
      "Epoch: 0274 train_loss= 0.47138 train_acc= 0.82777 test_loss= 0.65670 test_acc= 0.67232 time= 9.81256\n",
      "Epoch: 0275 train_loss= 0.47133 train_acc= 0.82777 test_loss= 0.65670 test_acc= 0.67232 time= 9.56627\n",
      "Epoch: 0276 train_loss= 0.47128 train_acc= 0.82777 test_loss= 0.65670 test_acc= 0.67232 time= 9.81637\n",
      "Epoch: 0277 train_loss= 0.47123 train_acc= 0.82777 test_loss= 0.65670 test_acc= 0.67232 time= 9.85595\n",
      "Epoch: 0278 train_loss= 0.47118 train_acc= 0.82777 test_loss= 0.65669 test_acc= 0.67232 time= 9.62023\n",
      "Epoch: 0279 train_loss= 0.47113 train_acc= 0.82777 test_loss= 0.65669 test_acc= 0.67232 time= 9.77029\n",
      "Epoch: 0280 train_loss= 0.47108 train_acc= 0.82777 test_loss= 0.65669 test_acc= 0.67232 time= 10.03473\n",
      "Epoch: 0281 train_loss= 0.47103 train_acc= 0.82777 test_loss= 0.65669 test_acc= 0.67232 time= 10.34592\n",
      "Epoch: 0282 train_loss= 0.47098 train_acc= 0.82777 test_loss= 0.65669 test_acc= 0.67232 time= 9.97288\n",
      "Epoch: 0283 train_loss= 0.47093 train_acc= 0.82777 test_loss= 0.65669 test_acc= 0.67232 time= 9.84354\n",
      "Epoch: 0284 train_loss= 0.47089 train_acc= 0.82777 test_loss= 0.65669 test_acc= 0.67232 time= 9.77214\n",
      "Epoch: 0285 train_loss= 0.47084 train_acc= 0.82777 test_loss= 0.65669 test_acc= 0.67232 time= 9.58428\n",
      "Epoch: 0286 train_loss= 0.47079 train_acc= 0.82777 test_loss= 0.65668 test_acc= 0.67232 time= 9.68371\n",
      "Epoch: 0287 train_loss= 0.47074 train_acc= 0.82777 test_loss= 0.65668 test_acc= 0.67232 time= 9.84206\n",
      "Epoch: 0288 train_loss= 0.47069 train_acc= 0.82777 test_loss= 0.65668 test_acc= 0.67232 time= 9.61780\n",
      "Epoch: 0289 train_loss= 0.47065 train_acc= 0.82777 test_loss= 0.65668 test_acc= 0.67232 time= 9.70159\n",
      "Epoch: 0290 train_loss= 0.47060 train_acc= 0.82777 test_loss= 0.65668 test_acc= 0.67232 time= 10.29406\n",
      "Epoch: 0291 train_loss= 0.47055 train_acc= 0.82636 test_loss= 0.65668 test_acc= 0.67232 time= 10.32130\n",
      "Epoch: 0292 train_loss= 0.47051 train_acc= 0.82636 test_loss= 0.65668 test_acc= 0.67232 time= 9.96899\n",
      "Epoch: 0293 train_loss= 0.47046 train_acc= 0.82636 test_loss= 0.65668 test_acc= 0.67232 time= 10.21465\n",
      "Epoch: 0294 train_loss= 0.47041 train_acc= 0.82636 test_loss= 0.65668 test_acc= 0.67232 time= 10.06160\n",
      "Epoch: 0295 train_loss= 0.47037 train_acc= 0.82636 test_loss= 0.65668 test_acc= 0.67232 time= 10.06607\n",
      "Epoch: 0296 train_loss= 0.47032 train_acc= 0.82636 test_loss= 0.65668 test_acc= 0.67232 time= 10.18126\n",
      "Epoch: 0297 train_loss= 0.47028 train_acc= 0.82636 test_loss= 0.65668 test_acc= 0.67232 time= 10.07712\n",
      "Epoch: 0298 train_loss= 0.47023 train_acc= 0.82636 test_loss= 0.65668 test_acc= 0.67232 time= 10.28829\n",
      "Epoch: 0299 train_loss= 0.47018 train_acc= 0.82636 test_loss= 0.65668 test_acc= 0.67232 time= 10.14989\n",
      "Epoch: 0300 train_loss= 0.47014 train_acc= 0.82636 test_loss= 0.65668 test_acc= 0.67232 time= 9.96750\n",
      "Epoch: 0301 train_loss= 0.47009 train_acc= 0.82636 test_loss= 0.65668 test_acc= 0.67232 time= 9.81227\n",
      "Epoch: 0302 train_loss= 0.47005 train_acc= 0.82636 test_loss= 0.65668 test_acc= 0.67232 time= 9.87839\n",
      "Epoch: 0303 train_loss= 0.47001 train_acc= 0.82636 test_loss= 0.65668 test_acc= 0.67232 time= 10.13199\n",
      "Epoch: 0304 train_loss= 0.46996 train_acc= 0.82636 test_loss= 0.65669 test_acc= 0.67232 time= 9.64894\n",
      "Epoch: 0305 train_loss= 0.46992 train_acc= 0.82636 test_loss= 0.65669 test_acc= 0.67232 time= 9.87525\n",
      "Epoch: 0306 train_loss= 0.46987 train_acc= 0.82636 test_loss= 0.65669 test_acc= 0.67232 time= 9.93254\n",
      "Epoch: 0307 train_loss= 0.46983 train_acc= 0.82636 test_loss= 0.65669 test_acc= 0.67232 time= 10.14459\n",
      "Epoch: 0308 train_loss= 0.46979 train_acc= 0.82636 test_loss= 0.65669 test_acc= 0.67232 time= 9.64712\n",
      "Epoch: 0309 train_loss= 0.46974 train_acc= 0.82636 test_loss= 0.65670 test_acc= 0.67232 time= 9.81064\n",
      "Epoch: 0310 train_loss= 0.46970 train_acc= 0.82636 test_loss= 0.65670 test_acc= 0.67232 time= 9.72429\n",
      "Epoch: 0311 train_loss= 0.46966 train_acc= 0.82495 test_loss= 0.65670 test_acc= 0.67232 time= 9.91227\n",
      "Epoch: 0312 train_loss= 0.46961 train_acc= 0.82495 test_loss= 0.65670 test_acc= 0.67232 time= 10.23345\n",
      "Epoch: 0313 train_loss= 0.46957 train_acc= 0.82495 test_loss= 0.65670 test_acc= 0.67232 time= 9.83117\n",
      "Epoch: 0314 train_loss= 0.46953 train_acc= 0.82495 test_loss= 0.65671 test_acc= 0.67232 time= 9.97071\n",
      "Epoch: 0315 train_loss= 0.46949 train_acc= 0.82495 test_loss= 0.65671 test_acc= 0.67232 time= 9.79994\n",
      "Epoch: 0316 train_loss= 0.46945 train_acc= 0.82495 test_loss= 0.65671 test_acc= 0.67232 time= 9.81268\n",
      "Epoch: 0317 train_loss= 0.46941 train_acc= 0.82495 test_loss= 0.65671 test_acc= 0.67287 time= 9.92040\n",
      "Epoch: 0318 train_loss= 0.46937 train_acc= 0.82495 test_loss= 0.65671 test_acc= 0.67367 time= 9.89175\n",
      "Epoch: 0319 train_loss= 0.46933 train_acc= 0.82495 test_loss= 0.65671 test_acc= 0.67637 time= 9.90665\n",
      "Epoch: 0320 train_loss= 0.46929 train_acc= 0.82495 test_loss= 0.65671 test_acc= 0.67797 time= 9.78447\n",
      "Epoch: 0321 train_loss= 0.46925 train_acc= 0.82495 test_loss= 0.65671 test_acc= 0.67797 time= 9.81255\n",
      "Epoch: 0322 train_loss= 0.46921 train_acc= 0.82495 test_loss= 0.65671 test_acc= 0.67797 time= 9.79683\n",
      "Epoch: 0323 train_loss= 0.46917 train_acc= 0.82495 test_loss= 0.65671 test_acc= 0.67797 time= 10.07634\n",
      "Epoch: 0324 train_loss= 0.46913 train_acc= 0.82495 test_loss= 0.65671 test_acc= 0.67797 time= 10.00667\n",
      "Epoch: 0325 train_loss= 0.46910 train_acc= 0.82495 test_loss= 0.65671 test_acc= 0.67797 time= 9.67460\n",
      "Epoch: 0326 train_loss= 0.46906 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.67797 time= 9.80800\n",
      "Epoch: 0327 train_loss= 0.46902 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.67797 time= 9.65158\n",
      "Epoch: 0328 train_loss= 0.46898 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.67797 time= 9.86536\n",
      "Epoch: 0329 train_loss= 0.46895 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.67797 time= 9.65262\n",
      "Epoch: 0330 train_loss= 0.46891 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.67797 time= 9.67607\n",
      "Epoch: 0331 train_loss= 0.46887 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.67797 time= 9.71016\n",
      "Epoch: 0332 train_loss= 0.46884 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.67797 time= 9.89107\n",
      "Epoch: 0333 train_loss= 0.46880 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.67797 time= 10.37238\n",
      "Epoch: 0334 train_loss= 0.46877 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.67797 time= 9.99884\n",
      "Epoch: 0335 train_loss= 0.46873 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.67797 time= 9.94382\n",
      "Epoch: 0336 train_loss= 0.46870 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.67797 time= 10.24720\n",
      "Epoch: 0337 train_loss= 0.46866 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.67908 time= 9.76389\n",
      "Epoch: 0338 train_loss= 0.46863 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.67988 time= 9.91361\n",
      "Epoch: 0339 train_loss= 0.46859 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.68139 time= 9.92830\n",
      "Epoch: 0340 train_loss= 0.46856 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.68218 time= 9.67761\n",
      "Epoch: 0341 train_loss= 0.46852 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.68266 time= 9.96422\n",
      "Epoch: 0342 train_loss= 0.46849 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.68346 time= 9.88059\n",
      "Epoch: 0343 train_loss= 0.46846 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.68362 time= 9.76533\n",
      "Epoch: 0344 train_loss= 0.46842 train_acc= 0.82636 test_loss= 0.65671 test_acc= 0.68362 time= 9.77915\n",
      "Epoch: 0345 train_loss= 0.46839 train_acc= 0.82636 test_loss= 0.65672 test_acc= 0.68362 time= 9.60517\n",
      "Epoch: 0346 train_loss= 0.46836 train_acc= 0.82636 test_loss= 0.65672 test_acc= 0.68362 time= 9.61437\n",
      "Epoch: 0347 train_loss= 0.46833 train_acc= 0.82636 test_loss= 0.65672 test_acc= 0.68362 time= 9.58296\n",
      "Epoch: 0348 train_loss= 0.46829 train_acc= 0.82636 test_loss= 0.65672 test_acc= 0.68362 time= 9.62089\n",
      "Epoch: 0349 train_loss= 0.46826 train_acc= 0.82636 test_loss= 0.65672 test_acc= 0.68362 time= 9.72060\n",
      "Epoch: 0350 train_loss= 0.46823 train_acc= 0.82636 test_loss= 0.65672 test_acc= 0.68362 time= 9.86564\n",
      "Epoch: 0351 train_loss= 0.46820 train_acc= 0.82636 test_loss= 0.65672 test_acc= 0.68362 time= 9.88831\n",
      "Epoch: 0352 train_loss= 0.46817 train_acc= 0.82636 test_loss= 0.65672 test_acc= 0.68362 time= 9.55564\n",
      "Epoch: 0353 train_loss= 0.46813 train_acc= 0.82636 test_loss= 0.65673 test_acc= 0.68362 time= 9.46066\n",
      "Epoch: 0354 train_loss= 0.46810 train_acc= 0.82636 test_loss= 0.65673 test_acc= 0.68322 time= 9.52524\n",
      "Epoch: 0355 train_loss= 0.46807 train_acc= 0.82777 test_loss= 0.65673 test_acc= 0.68194 time= 9.44520\n",
      "Epoch: 0356 train_loss= 0.46804 train_acc= 0.82777 test_loss= 0.65673 test_acc= 0.67988 time= 9.52178\n",
      "Epoch: 0357 train_loss= 0.46801 train_acc= 0.82777 test_loss= 0.65673 test_acc= 0.67868 time= 9.44160\n",
      "Epoch: 0358 train_loss= 0.46798 train_acc= 0.82777 test_loss= 0.65674 test_acc= 0.67797 time= 9.43954\n",
      "Epoch: 0359 train_loss= 0.46795 train_acc= 0.82777 test_loss= 0.65674 test_acc= 0.67797 time= 9.50035\n",
      "Epoch: 0360 train_loss= 0.46792 train_acc= 0.82777 test_loss= 0.65674 test_acc= 0.67797 time= 9.59479\n",
      "Epoch: 0361 train_loss= 0.46789 train_acc= 0.82777 test_loss= 0.65674 test_acc= 0.67797 time= 9.57026\n",
      "Epoch: 0362 train_loss= 0.46786 train_acc= 0.82978 test_loss= 0.65675 test_acc= 0.67797 time= 9.78992\n",
      "Epoch: 0363 train_loss= 0.46783 train_acc= 0.82978 test_loss= 0.65675 test_acc= 0.67797 time= 9.77438\n",
      "Epoch: 0364 train_loss= 0.46780 train_acc= 0.82978 test_loss= 0.65675 test_acc= 0.67797 time= 9.71283\n",
      "Epoch: 0365 train_loss= 0.46777 train_acc= 0.82978 test_loss= 0.65675 test_acc= 0.67797 time= 9.54403\n",
      "Epoch: 0366 train_loss= 0.46774 train_acc= 0.82978 test_loss= 0.65676 test_acc= 0.67797 time= 9.55047\n",
      "Epoch: 0367 train_loss= 0.46771 train_acc= 0.82978 test_loss= 0.65676 test_acc= 0.67797 time= 9.52216\n",
      "Epoch: 0368 train_loss= 0.46768 train_acc= 0.82978 test_loss= 0.65676 test_acc= 0.67797 time= 9.55743\n",
      "Epoch: 0369 train_loss= 0.46765 train_acc= 0.82978 test_loss= 0.65677 test_acc= 0.67797 time= 9.65416\n",
      "Epoch: 0370 train_loss= 0.46762 train_acc= 0.82978 test_loss= 0.65677 test_acc= 0.67797 time= 9.67316\n",
      "Epoch: 0371 train_loss= 0.46759 train_acc= 0.82978 test_loss= 0.65677 test_acc= 0.67797 time= 9.69012\n",
      "Epoch: 0372 train_loss= 0.46757 train_acc= 0.82978 test_loss= 0.65677 test_acc= 0.67797 time= 9.67021\n",
      "Epoch: 0373 train_loss= 0.46754 train_acc= 0.82978 test_loss= 0.65678 test_acc= 0.67797 time= 9.64796\n",
      "Epoch: 0374 train_loss= 0.46751 train_acc= 0.82978 test_loss= 0.65678 test_acc= 0.67797 time= 9.52139\n",
      "Epoch: 0375 train_loss= 0.46748 train_acc= 0.82978 test_loss= 0.65678 test_acc= 0.67797 time= 9.64937\n",
      "Epoch: 0376 train_loss= 0.46745 train_acc= 0.82978 test_loss= 0.65678 test_acc= 0.67797 time= 9.63822\n",
      "Epoch: 0377 train_loss= 0.46743 train_acc= 0.82978 test_loss= 0.65679 test_acc= 0.67797 time= 9.67572\n",
      "Epoch: 0378 train_loss= 0.46740 train_acc= 0.82978 test_loss= 0.65679 test_acc= 0.67797 time= 9.40874\n",
      "Epoch: 0379 train_loss= 0.46737 train_acc= 0.82978 test_loss= 0.65679 test_acc= 0.67797 time= 9.43164\n",
      "Epoch: 0380 train_loss= 0.46734 train_acc= 0.82978 test_loss= 0.65679 test_acc= 0.67797 time= 9.43517\n",
      "Epoch: 0381 train_loss= 0.46732 train_acc= 0.82978 test_loss= 0.65679 test_acc= 0.67797 time= 9.47514\n",
      "Epoch: 0382 train_loss= 0.46729 train_acc= 0.82978 test_loss= 0.65680 test_acc= 0.67797 time= 9.47156\n",
      "Epoch: 0383 train_loss= 0.46726 train_acc= 0.82978 test_loss= 0.65680 test_acc= 0.67797 time= 9.46831\n",
      "Epoch: 0384 train_loss= 0.46724 train_acc= 0.82978 test_loss= 0.65680 test_acc= 0.67797 time= 9.63296\n",
      "Epoch: 0385 train_loss= 0.46721 train_acc= 0.83119 test_loss= 0.65680 test_acc= 0.67797 time= 9.71794\n",
      "Epoch: 0386 train_loss= 0.46718 train_acc= 0.83119 test_loss= 0.65680 test_acc= 0.67797 time= 9.74741\n",
      "Epoch: 0387 train_loss= 0.46716 train_acc= 0.83119 test_loss= 0.65681 test_acc= 0.67797 time= 9.74879\n",
      "Epoch: 0388 train_loss= 0.46713 train_acc= 0.83119 test_loss= 0.65681 test_acc= 0.67797 time= 9.72164\n",
      "Epoch: 0389 train_loss= 0.46711 train_acc= 0.83119 test_loss= 0.65681 test_acc= 0.67797 time= 9.71424\n",
      "Epoch: 0390 train_loss= 0.46708 train_acc= 0.83119 test_loss= 0.65681 test_acc= 0.67797 time= 9.47654\n",
      "Epoch: 0391 train_loss= 0.46705 train_acc= 0.83119 test_loss= 0.65682 test_acc= 0.67797 time= 9.35367\n",
      "Epoch: 0392 train_loss= 0.46703 train_acc= 0.83119 test_loss= 0.65682 test_acc= 0.67797 time= 9.42934\n",
      "Epoch: 0393 train_loss= 0.46700 train_acc= 0.83119 test_loss= 0.65682 test_acc= 0.67797 time= 9.51055\n",
      "Epoch: 0394 train_loss= 0.46698 train_acc= 0.82978 test_loss= 0.65682 test_acc= 0.67797 time= 9.60929\n",
      "Epoch: 0395 train_loss= 0.46695 train_acc= 0.82978 test_loss= 0.65683 test_acc= 0.67797 time= 9.58595\n",
      "Epoch: 0396 train_loss= 0.46693 train_acc= 0.82978 test_loss= 0.65683 test_acc= 0.67797 time= 9.52018\n",
      "Epoch: 0397 train_loss= 0.46690 train_acc= 0.82978 test_loss= 0.65683 test_acc= 0.67797 time= 9.46505\n",
      "Epoch: 0398 train_loss= 0.46688 train_acc= 0.82978 test_loss= 0.65683 test_acc= 0.67797 time= 9.52345\n",
      "Epoch: 0399 train_loss= 0.46685 train_acc= 0.82978 test_loss= 0.65684 test_acc= 0.67797 time= 9.66208\n",
      "Epoch: 0400 train_loss= 0.46683 train_acc= 0.82978 test_loss= 0.65684 test_acc= 0.67797 time= 9.60367\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "cost_val = []\n",
    "for epoch in range(400):  #FLAGS.epochs\n",
    "    batches = range(round(train_num / batch_num))\n",
    "\n",
    "    costs = np.zeros((len(batches), 2))\n",
    "    accs = np.zeros((len(batches), 2))\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    for ib in batches:\n",
    "        from_i = ib * batch_num\n",
    "        to_i = (ib+1) * batch_num\n",
    "\n",
    "        features_train_batch = features_train[from_i:to_i]\n",
    "        support_train_batch = support_train[from_i:to_i]\n",
    "        y_train_batch = y_train[from_i:to_i]\n",
    "        feed_dict = construct_feed_dict(features_train_batch, support_train_batch, y_train_batch, placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "        feed_dict.update({placeholders['in_drop']: FLAGS.in_drop})\n",
    "\n",
    "        outs = sess.run([model.opt_op_explainer, model.loss_explainer, model.accuracy],feed_dict=feed_dict)\n",
    "\n",
    "        cost,acc,duration = evaluate(features_test, support_test, y_test, placeholders)\n",
    "        costs[ib] = [outs[1], cost]\n",
    "        accs[ib] = [outs[2], acc]\n",
    "\n",
    "    costs = costs.mean(axis=0)\n",
    "    cost_train, cost_test = costs\n",
    "    cost_val.append(cost_test)\n",
    "\n",
    "    accs = accs.mean(axis=0)\n",
    "    acc_train, acc_test = accs\n",
    "\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cost_train),\n",
    "        \"train_acc=\", \"{:.5f}\".format(acc_train), \"test_loss=\", \"{:.5f}\".format(cost_test),\n",
    "        \"test_acc=\", \"{:.5f}\".format(acc_test), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "m_saver.save(sess, bestModelSavePath1)\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b542681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 0.65684 accuracy= 0.67797 time= 0.11780\n"
     ]
    }
   ],
   "source": [
    "test_cost, test_acc, test_duration = evaluate(features_test, support_test, y_test, placeholders)\n",
    "\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "    \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "l2.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2712d426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6723164]\n",
      "[0.6779661]\n",
      "[0.6610169491525424, 0.5802469135802469, 0.7291666666666666, 0.6103896103896104, 0.715406378600823]\n"
     ]
    }
   ],
   "source": [
    "M = model.M\n",
    "s_m = tf.sigmoid(M)\n",
    "M = M.eval(session=sess)\n",
    "s_m = s_m.eval(session=sess)\n",
    "\n",
    "# Custom save directory\n",
    "save_dir = \"/home/celery/Documents/Research/NWPU-GAT/GAT-Li-Revisited/weights/\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Format file path using Python 3.5 compatible style\n",
    "save_path = os.path.join(save_dir, \"fold{}_mask.pkl\".format(k))\n",
    "\n",
    "# Save the sigmoid mask\n",
    "with open(save_path, 'wb+') as f:\n",
    "    pkl.dump(s_m, f)\n",
    "\n",
    "sess.close()\n",
    "\n",
    "\n",
    "print(l1)\n",
    "print(l2)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

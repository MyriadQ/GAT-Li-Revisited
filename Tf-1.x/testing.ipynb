{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c126c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2e3b87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/celery/.pyenv/versions/GAT/lib/python3.5/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import ABIDE_Parser as Reader\n",
    "import keras\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "import pickle as pkl\n",
    "import time\n",
    "import copy\n",
    "import scipy.spatial.distance\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1b3b2",
   "metadata": {},
   "source": [
    "Glorot initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0bdda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot(shape, name=None):\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    var = tf.Variable(initial, name=name)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98cc17a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: (5, 5)\n",
      "Weight sample:\n",
      " [[-0.71648014 -0.04834729  0.3869903  -0.33620343 -0.5158652 ]\n",
      " [ 0.2996589  -0.43660522  0.7125673  -0.39135537  0.0455662 ]\n",
      " [ 0.32520568 -0.21057141  0.5897924  -0.06281775  0.76459396]\n",
      " [ 0.02825612  0.46784544 -0.13363522  0.5326885  -0.609742  ]\n",
      " [-0.6846892  -0.6376404  -0.4884361   0.31057727 -0.25517952]]\n"
     ]
    }
   ],
   "source": [
    "# Define shape (e.g. 128 inputs, 64 outputs)\n",
    "shape = (5, 5)\n",
    "\n",
    "# Create the variable\n",
    "weight_var = glorot(shape, name=\"test_weights\")\n",
    "\n",
    "# Start a session to evaluate the variable\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    weights = sess.run(weight_var)\n",
    "    print(\"Weight shape:\", weights.shape)\n",
    "    print(\"Weight sample:\\n\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab5d22c",
   "metadata": {},
   "source": [
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3ed48f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cab0fe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "shape = (5,5)\n",
    "a = zeros(shape, name = 'testing')\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  print(sess.run(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7f333",
   "metadata": {},
   "source": [
    "these are easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc5783fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(x, y, sparse=False):\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x,y)\n",
    "    else:\n",
    "        res = tf.matmul(x,y)\n",
    "    return res\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    correct_prediction = tf.equal(tf.round(preds), labels)\n",
    "    accuracy = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy)\n",
    "\n",
    "def tens(shape, name=None):\n",
    "    initial = tf.constant(10, tf.float32, shape)\n",
    "    return tf.Variable(initial, name=name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed258d34",
   "metadata": {},
   "source": [
    "Gat_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ef474",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gat_layer(object):\n",
    "    def __init__(self, input_dim,F_, placeholders,attn_heads=1,attn_heads_reduction='concat',\n",
    "                 activation=tf.nn.relu, use_bias=True,name_=''):\n",
    "        self.dropout_rate = placeholders['dropout']\n",
    "        self.in_drop = placeholders['in_drop']\n",
    "        self.name = 'gat_layer'+name_\n",
    "        self.vars = {}\n",
    "        self.act = activation\n",
    "        self.attn_heads = attn_heads  # Number of attention heads (K in the paper)\n",
    "        self.attn_heads_reduction = attn_heads_reduction  #\n",
    "        self.bias = use_bias\n",
    "        self.A = placeholders[\"adj\"]\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        with tf.variable_scope(self.name+'_vars'):\n",
    "            for i in range(self.attn_heads):\n",
    "                self.vars['weights_'+str(i)] = glorot([input_dim, F_], name='weights_' + str(i))\n",
    "                self.vars[\"attn_self_weights_\"+str(i)] = glorot([F_, 1], name='attn_self_weights_' + str(i))\n",
    "                self.vars[\"attn_neighs_weights_\"+str(i)] = glorot([F_, 1], name='attn_neighs_weights_' + str(i))\n",
    "        if self.bias:\n",
    "            self.vars['bias'] = zeros([F_],name='bias')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        X = inputs\n",
    "        if self.in_drop != 0.0:\n",
    "            X = tf.nn.dropout(X, 1-self.in_drop)\n",
    "        outputs = []\n",
    "        dense_mask = []\n",
    "\n",
    "        for head in range(self.attn_heads):\n",
    "            # Compute inputs to attention network\n",
    "            kernel = self.vars['weights_'+str(head)]\n",
    "            features = tf.tensordot(X, kernel, axes=1)  # (N x F')\n",
    "\n",
    "            # Compute feature combinations\n",
    "            attention_self_kernel = self.vars[\"attn_self_weights_\"+str(head)]\n",
    "            attention_neighs_kernel = self.vars[\"attn_neighs_weights_\"+str(head)]\n",
    "            attn_for_self = tf.tensordot(features, attention_self_kernel, axes=1)\n",
    "            attn_for_neighs = tf.tensordot(features, attention_neighs_kernel, axes=1)\n",
    "\n",
    "            # Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]\n",
    "            dense = attn_for_self + tf.transpose(attn_for_neighs, [0,2,1])  # (N x N) via broadcasting\n",
    "\n",
    "            print(\"plus:\", dense.shape)\n",
    "\n",
    "            # Add nonlinearty\n",
    "            dense = tf.nn.leaky_relu(dense,alpha=0.2)\n",
    "\n",
    "            zero_vec = -9e15*tf.ones_like(dense)\n",
    "            dense = tf.where(self.A > 0.0, dense, zero_vec)\n",
    "            dense_mask.append(dense)\n",
    "\n",
    "            # Apply softmax to get attention coefficients\n",
    "            dense = tf.nn.softmax(dense)  # (N x N)\n",
    "\n",
    "            # Apply dropout to features and attention coefficients\n",
    "            dropout_attn = tf.nn.dropout(dense, 1-self.dropout_rate) # (N x N)\n",
    "            dropout_feat = tf.nn.dropout(features, 1-self.dropout_rate)  # (N x F')\n",
    "\n",
    "            # Linear combination with neighbors' features\n",
    "            node_features = tf.matmul(dropout_attn, dropout_feat)  # (N x F')\n",
    "\n",
    "            if self.bias:\n",
    "                node_features += self.vars[\"bias\"]\n",
    "\n",
    "            # Add output of attention head to final output\n",
    "            if self.attn_heads_reduction == 'concat':\n",
    "                outputs.append(self.act(node_features))\n",
    "            else:\n",
    "                outputs.append(node_features)\n",
    "\n",
    "        # Aggregate the heads' output according to the reduction method\n",
    "        if self.attn_heads_reduction == 'concat':\n",
    "            output = tf.concat(outputs, axis=-1)  # (N x KF')\n",
    "        else:\n",
    "            output = tf.add_n(outputs) / self.attn_heads  # N x F')\n",
    "            output = self.act(output)\n",
    "\n",
    "        return output, dense_mask\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

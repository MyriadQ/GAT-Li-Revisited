{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62dade44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ca12bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import ABIDE_Parser as Reader\n",
    "import keras\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "import pickle as pkl \n",
    "import time\n",
    "import copy\n",
    "import scipy.spatial.distance\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec38117",
   "metadata": {},
   "source": [
    "Some functions in Tensorflow 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ea0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot(shape, name=None):\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    var = tf.Variable(initial, name=name)\n",
    "    return var\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x,y)\n",
    "    else:\n",
    "        res = tf.matmul(x,y)\n",
    "    return res\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    correct_prediction = tf.equal(tf.round(preds), labels)\n",
    "    accuracy = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy)\n",
    "\n",
    "def tens(shape, name=None):\n",
    "    initial = tf.constant(10, tf.float32, shape)\n",
    "    return tf.Variable(initial, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39a5fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gat_layer(object):\n",
    "    def __init__(self, input_dim,F_, placeholders,attn_heads=1,attn_heads_reduction='concat',\n",
    "                 activation=tf.nn.relu, use_bias=True,name_=''):\n",
    "        self.dropout_rate = placeholders['dropout']\n",
    "        self.in_drop = placeholders['in_drop']\n",
    "        self.name = 'gat_layer'+name_\n",
    "        self.vars = {}\n",
    "        self.act = activation\n",
    "        self.attn_heads = attn_heads  # Number of attention heads (K in the paper)\n",
    "        self.attn_heads_reduction = attn_heads_reduction  #\n",
    "        self.bias = use_bias\n",
    "        self.A = placeholders[\"adj\"]\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        with tf.variable_scope(self.name+'_vars'):\n",
    "            for i in range(self.attn_heads):\n",
    "                self.vars['weights_'+str(i)] = glorot([input_dim, F_], name='weights_' + str(i))\n",
    "                self.vars[\"attn_self_weights_\"+str(i)] = glorot([F_, 1], name='attn_self_weights_' + str(i))\n",
    "                self.vars[\"attn_neighs_weights_\"+str(i)] = glorot([F_, 1], name='attn_neighs_weights_' + str(i))\n",
    "        if self.bias:\n",
    "            self.vars['bias'] = zeros([F_],name='bias')\n",
    "            \n",
    "    def __call__(self, inputs):\n",
    "        X = inputs\n",
    "        if self.in_drop != 0.0:\n",
    "            X = tf.nn.dropout(X, 1-self.in_drop)\n",
    "        outputs = []\n",
    "        dense_mask = []\n",
    "        \n",
    "        for head in range(self.attn_heads):\n",
    "            # Compute inputs to attention network\n",
    "            kernel = self.vars['weights_'+str(head)]\n",
    "            features = tf.tensordot(X, kernel, axes=1)  # (N x F')\n",
    "\n",
    "            # Compute feature combinations\n",
    "            attention_self_kernel = self.vars[\"attn_self_weights_\"+str(head)]\n",
    "            attention_neighs_kernel = self.vars[\"attn_neighs_weights_\"+str(head)]\n",
    "            attn_for_self = tf.tensordot(features, attention_self_kernel, axes=1)    \n",
    "            attn_for_neighs = tf.tensordot(features, attention_neighs_kernel, axes=1)  \n",
    "\n",
    "            # Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]\n",
    "            dense = attn_for_self + tf.transpose(attn_for_neighs, [0,2,1])  # (N x N) via broadcasting\n",
    "            \n",
    "            #print(\"plus:\", dense.shape)\n",
    "\n",
    "            # Add nonlinearty\n",
    "            dense = tf.nn.leaky_relu(dense,alpha=0.2)\n",
    "\n",
    "            zero_vec = -9e15*tf.ones_like(dense)\n",
    "            dense = tf.where(self.A > 0.0, dense, zero_vec)\n",
    "            dense_mask.append(dense)\n",
    "\n",
    "            # Apply softmax to get attention coefficients\n",
    "            dense = tf.nn.softmax(dense)  # (N x N)\n",
    "\n",
    "            # Apply dropout to features and attention coefficients\n",
    "            dropout_attn = tf.nn.dropout(dense, 1-self.dropout_rate) # (N x N)\n",
    "            dropout_feat = tf.nn.dropout(features, 1-self.dropout_rate)  # (N x F')\n",
    "\n",
    "            # Linear combination with neighbors' features\n",
    "            node_features = tf.matmul(dropout_attn, dropout_feat)  # (N x F')\n",
    "\n",
    "            if self.bias:\n",
    "                node_features += self.vars[\"bias\"]\n",
    "\n",
    "            # Add output of attention head to final output\n",
    "            if self.attn_heads_reduction == 'concat':\n",
    "                outputs.append(self.act(node_features))\n",
    "            else:\n",
    "                outputs.append(node_features)\n",
    "\n",
    "        # Aggregate the heads' output according to the reduction method\n",
    "        if self.attn_heads_reduction == 'concat':\n",
    "            output = tf.concat(outputs, axis=-1)  # (N x KF')\n",
    "        else:\n",
    "            output = tf.add_n(outputs) / self.attn_heads  # N x F')\n",
    "            output = self.act(output)\n",
    "\n",
    "        return output, dense_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaa51ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_layer(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,sparse_input=False, act=tf.nn.relu, bias=False, featureless=False,name_=''):\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.name = 'fc_layer'+name_\n",
    "        self.vars = {}\n",
    "        self.act = act\n",
    "\n",
    "        self.sparse_input = sparse_input\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        with tf.variable_scope(self.name+'_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim], name='weights')\n",
    "        if self.bias:\n",
    "            self.vars['bias'] = zeros([output_dim],name='bias')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = tf.nn.dropout(x,1-self.dropout)\n",
    "\n",
    "        output = tf.tensordot(x, self.vars['weights'], axes=1)\n",
    "\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "        return self.act(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d03e894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x7f559df79a90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer('node_num', 110, 'Number of Graph nodes')\n",
    "\n",
    "flags.DEFINE_integer('output_dim', 1, 'Number of output_dim')\n",
    "flags.DEFINE_float('learning_rate', 0.0001, 'Initial learning rate') #0.0005，0.0001，0.00005，0.00001，0.00003\n",
    "flags.DEFINE_integer('batch_num', 10, 'Number of epochs to train')\n",
    "flags.DEFINE_integer('epochs', 1000, 'Number of epochs to train')\n",
    "flags.DEFINE_integer('attn_heads', 5, 'Number of attention head')\n",
    "\n",
    "flags.DEFINE_integer('hidden1_gat', 24, 'Number of units in hidden layer 1 of gcn')\n",
    "flags.DEFINE_integer('output_gat', 3, 'Number of units in output layer 1 of gcn')\n",
    "\n",
    "flags.DEFINE_float('dropout', 0, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('in_drop', 0, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 15, 'Tolerance for early stopping (# of epochs).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "883bb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def __init__(self, placeholders, input_dim):\n",
    "        self.placeholders = placeholders\n",
    "        self.input_dim = input_dim\n",
    "        self.name = 'gat_mil'\n",
    "\n",
    "        self.gat_layers = []\n",
    "        self.fc_layers = []\n",
    "        self.gcn_activations = []\n",
    "        self.fc_activatinos = []\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        self.outputs = None\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        \n",
    "        self.node_prob = None\n",
    "        self.dense_mask = []\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "        self.opt_op = None\n",
    "        \n",
    "        self.loss_explainer = 0\n",
    "        self.optimizer_explainer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01)\n",
    "        self.opt_op_explainer = None\n",
    "        self.M = tens((FLAGS.node_num, FLAGS.node_num), name='mask')\n",
    "        \n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        sigmoid_M = tf.sigmoid(self.M)\n",
    "        self.inputs = tf.multiply(self.inputs, sigmoid_M)\n",
    "\n",
    "        self.gcn_activations.append(self.inputs)\n",
    "\n",
    "        for layer in self.gat_layers:\n",
    "            hidden, dense_mask = layer(self.gcn_activations[-1])\n",
    "            self.gcn_activations.append(hidden) \n",
    "            self.dense_mask.append(dense_mask)\n",
    "\n",
    "\n",
    "        p_layer = self.fc_layers[0]\n",
    "        node_prob = p_layer(self.gcn_activations[-1])\n",
    "        \n",
    "        tensor = tf.reshape(node_prob, shape=(-1, FLAGS.node_num))\n",
    "        layer = self.fc_layers[1]\n",
    "        attention_prob = layer(tensor)\n",
    "\n",
    "\n",
    "        attention_mul = tf.multiply(tensor, attention_prob)\n",
    "        self.outputs = tf.reduce_sum(attention_mul, 1, keep_dims=True)\n",
    "        print(self.outputs.shape)\n",
    "\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "\n",
    "        var_list = tf.trainable_variables()\n",
    "        var_list1 = []\n",
    "        for var in var_list:\n",
    "            if var != self.M:\n",
    "                var_list1.append(var)\n",
    "            elif var == self.M:\n",
    "                #stop = input(\"M exit!!!!!!!\")\n",
    "                pass\n",
    "        \n",
    "        self.opt_op = self.optimizer.minimize(self.loss, var_list = [var_list1])\n",
    "        self.loss_explainer += tf.reduce_mean(tf.losses.log_loss(labels=self.placeholders['labels'], predictions=self.outputs))\n",
    "        self.opt_op_explainer = self.optimizer_explainer.minimize(self.loss_explainer, var_list=[self.M])\n",
    "\n",
    "    def _build(self):\n",
    "        self.gat_layers.append(gat_layer(input_dim=self.input_dim,F_=FLAGS.hidden1_gat, placeholders=self.placeholders,\n",
    "                                         attn_heads=FLAGS.attn_heads,attn_heads_reduction='concat',\n",
    "                                         activation=tf.nn.leaky_relu, use_bias=True,name_='1'))\n",
    "\n",
    "        self.gat_layers.append(gat_layer(input_dim=FLAGS.hidden1_gat*FLAGS.attn_heads,F_=FLAGS.output_gat, placeholders=self.placeholders,\n",
    "                                         attn_heads=3,attn_heads_reduction='average',\n",
    "                                         activation=tf.nn.leaky_relu, use_bias=True,name_='2'))\n",
    "\n",
    "        self.fc_layers.append(fc_layer(input_dim=FLAGS.output_gat, output_dim=FLAGS.output_dim, placeholders=self.placeholders, \n",
    "                                       act=tf.nn.sigmoid, dropout=True, name_='1'))\n",
    "        \n",
    "        self.fc_layers.append(fc_layer(input_dim=FLAGS.node_num, output_dim=FLAGS.node_num, placeholders=self.placeholders, \n",
    "                                       act=tf.nn.softmax, dropout=True, name_='2'))\n",
    "\n",
    "    def _loss(self):\n",
    "        for var in self.gat_layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay*tf.nn.l2_loss(var)\n",
    "\n",
    "            \n",
    "        self.loss += tf.reduce_mean(tf.losses.log_loss(labels=self.placeholders['labels'], predictions=self.outputs))\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = accuracy(self.outputs, self.placeholders['labels'])\n",
    "\n",
    "    def predict(self):\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d1f2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

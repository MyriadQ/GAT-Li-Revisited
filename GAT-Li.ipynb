{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62dade44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ca12bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/celery/.pyenv/versions/3.5.10/envs/GAT/lib/python3.5/site-packages/nilearn/__init__.py:68: FutureWarning: Python 3.5 support is deprecated and will be removed in a future release. Consider switching to Python 3.6 or 3.7\n",
      "  _python_deprecation_warnings()\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import ABIDE_Parser as Reader\n",
    "import keras\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "import pickle as pkl \n",
    "import time\n",
    "import copy\n",
    "import scipy.spatial.distance\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec38117",
   "metadata": {},
   "source": [
    "Some functions in Tensorflow 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ea0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot(shape, name=None):\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    var = tf.Variable(initial, name=name)\n",
    "    return var\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x,y)\n",
    "    else:\n",
    "        res = tf.matmul(x,y)\n",
    "    return res\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    correct_prediction = tf.equal(tf.round(preds), labels)\n",
    "    accuracy = tf.cast(correct_prediction, tf.float32)\n",
    "    return tf.reduce_mean(accuracy)\n",
    "\n",
    "def tens(shape, name=None):\n",
    "    initial = tf.constant(10, tf.float32, shape)\n",
    "    return tf.Variable(initial, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a5fd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gat_layer(object):\n",
    "    def __init__(self, input_dim,F_, placeholders,attn_heads=1,attn_heads_reduction='concat',\n",
    "                 activation=tf.nn.relu, use_bias=True,name_=''):\n",
    "        self.dropout_rate = placeholders['dropout']\n",
    "        self.in_drop = placeholders['in_drop']\n",
    "        self.name = 'gat_layer'+name_\n",
    "        self.vars = {}\n",
    "        self.act = activation\n",
    "        self.attn_heads = attn_heads  # Number of attention heads (K in the paper)\n",
    "        self.attn_heads_reduction = attn_heads_reduction  #\n",
    "        self.bias = use_bias\n",
    "        self.A = placeholders[\"adj\"]\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        with tf.variable_scope(self.name+'_vars'):\n",
    "            for i in range(self.attn_heads):\n",
    "                self.vars['weights_'+str(i)] = glorot([input_dim, F_], name='weights_' + str(i))\n",
    "                self.vars[\"attn_self_weights_\"+str(i)] = glorot([F_, 1], name='attn_self_weights_' + str(i))\n",
    "                self.vars[\"attn_neighs_weights_\"+str(i)] = glorot([F_, 1], name='attn_neighs_weights_' + str(i))\n",
    "        if self.bias:\n",
    "            self.vars['bias'] = zeros([F_],name='bias')\n",
    "            \n",
    "    def __call__(self, inputs):\n",
    "        X = inputs\n",
    "        if self.in_drop != 0.0:\n",
    "            X = tf.nn.dropout(X, 1-self.in_drop)\n",
    "        outputs = []\n",
    "        dense_mask = []\n",
    "        \n",
    "        for head in range(self.attn_heads):\n",
    "            # Compute inputs to attention network\n",
    "            kernel = self.vars['weights_'+str(head)]\n",
    "            features = tf.tensordot(X, kernel, axes=1)  # (N x F')\n",
    "\n",
    "            # Compute feature combinations\n",
    "            attention_self_kernel = self.vars[\"attn_self_weights_\"+str(head)]\n",
    "            attention_neighs_kernel = self.vars[\"attn_neighs_weights_\"+str(head)]\n",
    "            attn_for_self = tf.tensordot(features, attention_self_kernel, axes=1)    \n",
    "            attn_for_neighs = tf.tensordot(features, attention_neighs_kernel, axes=1)  \n",
    "\n",
    "            # Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]\n",
    "            dense = attn_for_self + tf.transpose(attn_for_neighs, [0,2,1])  # (N x N) via broadcasting\n",
    "            \n",
    "            #print(\"plus:\", dense.shape)\n",
    "\n",
    "            # Add nonlinearty\n",
    "            dense = tf.nn.leaky_relu(dense,alpha=0.2)\n",
    "\n",
    "            zero_vec = -9e15*tf.ones_like(dense)\n",
    "            dense = tf.where(self.A > 0.0, dense, zero_vec)\n",
    "            dense_mask.append(dense)\n",
    "\n",
    "            # Apply softmax to get attention coefficients\n",
    "            dense = tf.nn.softmax(dense)  # (N x N)\n",
    "\n",
    "            # Apply dropout to features and attention coefficients\n",
    "            dropout_attn = tf.nn.dropout(dense, 1-self.dropout_rate) # (N x N)\n",
    "            dropout_feat = tf.nn.dropout(features, 1-self.dropout_rate)  # (N x F')\n",
    "\n",
    "            # Linear combination with neighbors' features\n",
    "            node_features = tf.matmul(dropout_attn, dropout_feat)  # (N x F')\n",
    "\n",
    "            if self.bias:\n",
    "                node_features += self.vars[\"bias\"]\n",
    "\n",
    "            # Add output of attention head to final output\n",
    "            if self.attn_heads_reduction == 'concat':\n",
    "                outputs.append(self.act(node_features))\n",
    "            else:\n",
    "                outputs.append(node_features)\n",
    "\n",
    "        # Aggregate the heads' output according to the reduction method\n",
    "        if self.attn_heads_reduction == 'concat':\n",
    "            output = tf.concat(outputs, axis=-1)  # (N x KF')\n",
    "        else:\n",
    "            output = tf.add_n(outputs) / self.attn_heads  # N x F')\n",
    "            output = self.act(output)\n",
    "\n",
    "        return output, dense_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa51ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_layer(object):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,sparse_input=False, act=tf.nn.relu, bias=False, featureless=False,name_=''):\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.name = 'fc_layer'+name_\n",
    "        self.vars = {}\n",
    "        self.act = act\n",
    "\n",
    "        self.sparse_input = sparse_input\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        with tf.variable_scope(self.name+'_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim], name='weights')\n",
    "        if self.bias:\n",
    "            self.vars['bias'] = zeros([output_dim],name='bias')\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        x = tf.nn.dropout(x,1-self.dropout)\n",
    "\n",
    "        output = tf.tensordot(x, self.vars['weights'], axes=1)\n",
    "\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "        return self.act(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d03e894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x7fbc54cfb0d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "flags.DEFINE_integer('node_num', 110, 'Number of Graph nodes')\n",
    "\n",
    "flags.DEFINE_integer('output_dim', 1, 'Number of output_dim')\n",
    "flags.DEFINE_float('learning_rate', 0.0001, 'Initial learning rate') #0.0005，0.0001，0.00005，0.00001，0.00003\n",
    "flags.DEFINE_integer('batch_num', 10, 'Number of epochs to train')\n",
    "flags.DEFINE_integer('epochs', 1000, 'Number of epochs to train')\n",
    "flags.DEFINE_integer('attn_heads', 5, 'Number of attention head')\n",
    "\n",
    "flags.DEFINE_integer('hidden1_gat', 24, 'Number of units in hidden layer 1 of gcn')\n",
    "flags.DEFINE_integer('output_gat', 3, 'Number of units in output layer 1 of gcn')\n",
    "\n",
    "flags.DEFINE_float('dropout', 0, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('in_drop', 0, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 15, 'Tolerance for early stopping (# of epochs).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "883bb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def __init__(self, placeholders, input_dim):\n",
    "        self.placeholders = placeholders\n",
    "        self.input_dim = input_dim\n",
    "        self.name = 'gat_mil'\n",
    "\n",
    "        self.gat_layers = []\n",
    "        self.fc_layers = []\n",
    "        self.gcn_activations = []\n",
    "        self.fc_activatinos = []\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        self.outputs = None\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        \n",
    "        self.node_prob = None\n",
    "        self.dense_mask = []\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "        self.opt_op = None\n",
    "        \n",
    "        self.loss_explainer = 0\n",
    "        #self.optimizer_explainer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.01) since im already using tf 1.x so the below code is used\n",
    "        self.optimizer_explainer = tf.train.AdamOptimizer(learning_rate=0.01) #replaced the above line\n",
    "        self.opt_op_explainer = None\n",
    "        self.M = tens((FLAGS.node_num, FLAGS.node_num), name='mask')\n",
    "        \n",
    "        self.build()\n",
    "\n",
    "    def build(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        sigmoid_M = tf.sigmoid(self.M)\n",
    "        self.inputs = tf.multiply(self.inputs, sigmoid_M)\n",
    "\n",
    "        self.gcn_activations.append(self.inputs)\n",
    "\n",
    "        for layer in self.gat_layers:\n",
    "            hidden, dense_mask = layer(self.gcn_activations[-1])\n",
    "            self.gcn_activations.append(hidden) \n",
    "            self.dense_mask.append(dense_mask)\n",
    "\n",
    "\n",
    "        p_layer = self.fc_layers[0]\n",
    "        node_prob = p_layer(self.gcn_activations[-1])\n",
    "        \n",
    "        tensor = tf.reshape(node_prob, shape=(-1, FLAGS.node_num))\n",
    "        layer = self.fc_layers[1]\n",
    "        attention_prob = layer(tensor)\n",
    "\n",
    "\n",
    "        attention_mul = tf.multiply(tensor, attention_prob)\n",
    "        self.outputs = tf.reduce_sum(attention_mul, 1, keep_dims=True)\n",
    "        print(self.outputs.shape)\n",
    "\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "\n",
    "        var_list = tf.trainable_variables()\n",
    "        var_list1 = []\n",
    "        for var in var_list:\n",
    "            if var != self.M:\n",
    "                var_list1.append(var)\n",
    "            elif var == self.M:\n",
    "                #stop = input(\"M exit!!!!!!!\")\n",
    "                pass\n",
    "        \n",
    "        self.opt_op = self.optimizer.minimize(self.loss, var_list = [var_list1])\n",
    "        self.loss_explainer += tf.reduce_mean(tf.losses.log_loss(labels=self.placeholders['labels'], predictions=self.outputs))\n",
    "        self.opt_op_explainer = self.optimizer_explainer.minimize(self.loss_explainer, var_list=[self.M])\n",
    "\n",
    "    def _build(self):\n",
    "        self.gat_layers.append(gat_layer(input_dim=self.input_dim,F_=FLAGS.hidden1_gat, placeholders=self.placeholders,\n",
    "                                         attn_heads=FLAGS.attn_heads,attn_heads_reduction='concat',\n",
    "                                         activation=tf.nn.leaky_relu, use_bias=True,name_='1'))\n",
    "\n",
    "        self.gat_layers.append(gat_layer(input_dim=FLAGS.hidden1_gat*FLAGS.attn_heads,F_=FLAGS.output_gat, placeholders=self.placeholders,\n",
    "                                         attn_heads=3,attn_heads_reduction='average',\n",
    "                                         activation=tf.nn.leaky_relu, use_bias=True,name_='2'))\n",
    "\n",
    "        self.fc_layers.append(fc_layer(input_dim=FLAGS.output_gat, output_dim=FLAGS.output_dim, placeholders=self.placeholders, \n",
    "                                       act=tf.nn.sigmoid, dropout=True, name_='1'))\n",
    "        \n",
    "        self.fc_layers.append(fc_layer(input_dim=FLAGS.node_num, output_dim=FLAGS.node_num, placeholders=self.placeholders, \n",
    "                                       act=tf.nn.softmax, dropout=True, name_='2'))\n",
    "\n",
    "    def _loss(self):\n",
    "        for var in self.gat_layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay*tf.nn.l2_loss(var)\n",
    "\n",
    "            \n",
    "        self.loss += tf.reduce_mean(tf.losses.log_loss(labels=self.placeholders['labels'], predictions=self.outputs))\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = accuracy(self.outputs, self.placeholders['labels'])\n",
    "\n",
    "    def predict(self):\n",
    "        return self.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d1f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = '/home/celery/Documents/Research/dataset/Outputs/'\n",
    "data_folder = os.path.join(root_folder, 'cpac/filt_noglobal')\n",
    "subject_IDs = np.genfromtxt('/home/celery/Documents/Research/dataset/valid_subject_ids.txt', dtype=str)\n",
    "subject_IDs = subject_IDs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d92db3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get label\n",
    "label_dict = Reader.get_label(subject_IDs)\n",
    "label_list = np.array([int(label_dict[x]) for x in subject_IDs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13ae19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/celery/Documents/Research/dataset/Outputs/cpac/filt_global/mat/'\n",
    "def load_connectivity(subject_list, kind, atlas_name = 'ho'):\n",
    "    all_networks = []\n",
    "    for subject in subject_list:\n",
    "        fl = os.path.join(data_folder,\n",
    "                          subject + \"_\" + atlas_name + \"_\" + kind + \".mat\")\n",
    "        matrix = sio.loadmat(fl)['connectivity']\n",
    "        if atlas_name == 'ho':\n",
    "            matrix = np.delete(matrix, 82, axis=0)\n",
    "            matrix = np.delete(matrix, 82, axis=1)\n",
    "        all_networks.append(matrix)\n",
    "    all_networks=np.array(all_networks)\n",
    "    return all_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0339601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getconn_vector(subject_name0, kind, atlas):\n",
    "    subject_name = np.array(subject_name0)\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    conn_array = load_connectivity(subject_name, kind, atlas)\n",
    "    data_x = np.array(conn_array)\n",
    "    \n",
    "    for subname in subject_name:\n",
    "        data_y.append([int(label_dict[subname])])\n",
    "    \n",
    "    data_y = np.array(data_y)\n",
    "    return data_x, data_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edeaf7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = getconn_vector(subject_IDs, \"correlation\", \"ho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26ecd223",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_x = map(abs, X)\n",
    "adjs = np.array(list(abs_x))\n",
    "features = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "632c093e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x7fbc54cb3d10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b014108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'adj': tf.placeholder(tf.float32, shape=(None,FLAGS.node_num,FLAGS.node_num)),\n",
    "    'features': tf.placeholder(tf.float32, shape=(None,FLAGS.node_num,FLAGS.node_num)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, 1)),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'in_drop': tf.placeholder_with_default(0., shape=()),\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5426dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_feed_dict(features, support, labels, placeholders):\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['adj']: support})\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91df014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "217cf587",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(adjs, features, y):\n",
    "    shuffle_ix = np.random.permutation(np.arange(len(y)))\n",
    "    adjs = adjs[shuffle_ix]\n",
    "    features = features[shuffle_ix]\n",
    "    y = y[shuffle_ix]\n",
    "    return adjs, features, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af1ff071",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_index = []\n",
    "all_train_index = []\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
    "for train_index, test_index in skf.split(subject_IDs, label_list):\n",
    "    all_train_index.append(train_index)\n",
    "    all_test_index.append(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33243a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11bec599",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "l1, l2 = [], []\n",
    "\n",
    "k = 4\n",
    "train_index = all_train_index[k]\n",
    "test_index = all_test_index[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f57e21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestModelSavePath0 = '/home/celery/Documents/Research/GNN-Research/fold_e_mask%s/gat_e%s_weights_best.ckpt' % (str(k),str(k))\n",
    "bestModelSavePath1 = '/home/celery/Documents/Research/GNN-Research/fold_e_mask%s/gat_e%s_weights_best.ckpt' % (str(k),str(k))\n",
    "features_train, features_test = features[train_index], features[test_index]\n",
    "support_train, support_test = adjs[train_index], adjs[test_index]\n",
    "y_train, y_test = Y[train_index], Y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2a9b2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test shape: (176, 110, 110) support shape: (708, 110, 110)\n",
      "y_test [[1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "support_train, features_train, y_train = shuffle(support_train, features_train, y_train)\n",
    "support_test, features_test, y_test = shuffle(support_test, features_test, y_test)\n",
    "print(\"test shape:\",features_test.shape, \"support shape:\", support_train.shape)\n",
    "print(\"y_test\", y_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9e6daea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-aaf1058e953e>:59: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "model = Model(placeholders, input_dim=features.shape[2])\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "'''设置模型保存器'''\n",
    "m_saver = tf.train.Saver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ab78972",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_val = []\n",
    "batch_num = FLAGS.batch_num\n",
    "train_num = features_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7a7724d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 0.74590 train_acc= 0.52570 test_loss= 0.74533 test_acc= 0.53881 time= 8.73298\n",
      "Epoch: 0002 train_loss= 0.74287 train_acc= 0.53697 test_loss= 0.74250 test_acc= 0.53873 time= 8.19808\n",
      "Epoch: 0003 train_loss= 0.74040 train_acc= 0.53979 test_loss= 0.74030 test_acc= 0.54105 time= 8.22167\n",
      "Epoch: 0004 train_loss= 0.73801 train_acc= 0.53838 test_loss= 0.73830 test_acc= 0.54209 time= 8.33492\n",
      "Epoch: 0005 train_loss= 0.73551 train_acc= 0.53979 test_loss= 0.73634 test_acc= 0.54409 time= 8.39938\n",
      "Epoch: 0006 train_loss= 0.73286 train_acc= 0.54120 test_loss= 0.73428 test_acc= 0.54545 time= 8.65560\n",
      "Epoch: 0007 train_loss= 0.72995 train_acc= 0.54401 test_loss= 0.73215 test_acc= 0.54537 time= 8.92956\n",
      "Epoch: 0008 train_loss= 0.72680 train_acc= 0.54683 test_loss= 0.72991 test_acc= 0.54321 time= 8.84277\n",
      "Epoch: 0009 train_loss= 0.72330 train_acc= 0.55528 test_loss= 0.72749 test_acc= 0.54834 time= 9.03406\n",
      "Epoch: 0010 train_loss= 0.71922 train_acc= 0.57817 test_loss= 0.72486 test_acc= 0.56466 time= 9.02395\n",
      "Epoch: 0011 train_loss= 0.71463 train_acc= 0.58803 test_loss= 0.72212 test_acc= 0.58587 time= 9.12400\n",
      "Epoch: 0012 train_loss= 0.70963 train_acc= 0.61197 test_loss= 0.71931 test_acc= 0.58459 time= 9.23178\n",
      "Epoch: 0013 train_loss= 0.70426 train_acc= 0.62746 test_loss= 0.71658 test_acc= 0.59635 time= 9.32093\n",
      "Epoch: 0014 train_loss= 0.69866 train_acc= 0.64859 test_loss= 0.71397 test_acc= 0.60003 time= 9.60045\n",
      "Epoch: 0015 train_loss= 0.69294 train_acc= 0.66268 test_loss= 0.71152 test_acc= 0.60835 time= 9.65354\n",
      "Epoch: 0016 train_loss= 0.68720 train_acc= 0.68063 test_loss= 0.70937 test_acc= 0.60411 time= 9.52250\n",
      "Epoch: 0017 train_loss= 0.68162 train_acc= 0.68627 test_loss= 0.70759 test_acc= 0.60427 time= 8.95354\n",
      "Epoch: 0018 train_loss= 0.67624 train_acc= 0.68768 test_loss= 0.70617 test_acc= 0.60651 time= 8.90303\n",
      "Epoch: 0019 train_loss= 0.67107 train_acc= 0.68908 test_loss= 0.70508 test_acc= 0.60940 time= 8.40065\n",
      "Epoch: 0020 train_loss= 0.66612 train_acc= 0.69613 test_loss= 0.70420 test_acc= 0.61100 time= 8.21735\n",
      "Epoch: 0021 train_loss= 0.66136 train_acc= 0.70176 test_loss= 0.70350 test_acc= 0.61556 time= 8.37543\n",
      "Epoch: 0022 train_loss= 0.65681 train_acc= 0.70317 test_loss= 0.70291 test_acc= 0.62044 time= 8.42774\n",
      "Epoch: 0023 train_loss= 0.65238 train_acc= 0.70599 test_loss= 0.70234 test_acc= 0.62700 time= 8.34875\n",
      "Epoch: 0024 train_loss= 0.64808 train_acc= 0.70880 test_loss= 0.70179 test_acc= 0.62932 time= 8.27698\n",
      "Epoch: 0025 train_loss= 0.64390 train_acc= 0.71021 test_loss= 0.70126 test_acc= 0.62844 time= 8.30408\n",
      "Epoch: 0026 train_loss= 0.63984 train_acc= 0.71162 test_loss= 0.70073 test_acc= 0.62636 time= 8.27837\n",
      "Epoch: 0027 train_loss= 0.63585 train_acc= 0.71585 test_loss= 0.70021 test_acc= 0.62564 time= 8.34325\n",
      "Epoch: 0028 train_loss= 0.63193 train_acc= 0.71444 test_loss= 0.69966 test_acc= 0.62748 time= 8.38109\n",
      "Epoch: 0029 train_loss= 0.62806 train_acc= 0.71585 test_loss= 0.69911 test_acc= 0.63220 time= 8.25405\n",
      "Epoch: 0030 train_loss= 0.62423 train_acc= 0.71866 test_loss= 0.69857 test_acc= 0.63636 time= 8.31981\n",
      "Epoch: 0031 train_loss= 0.62046 train_acc= 0.72289 test_loss= 0.69802 test_acc= 0.64069 time= 8.37302\n",
      "Epoch: 0032 train_loss= 0.61677 train_acc= 0.71585 test_loss= 0.69751 test_acc= 0.64733 time= 8.33720\n",
      "Epoch: 0033 train_loss= 0.61312 train_acc= 0.72289 test_loss= 0.69701 test_acc= 0.65013 time= 8.35035\n",
      "Epoch: 0034 train_loss= 0.60954 train_acc= 0.72289 test_loss= 0.69652 test_acc= 0.65253 time= 8.56518\n",
      "Epoch: 0035 train_loss= 0.60598 train_acc= 0.72113 test_loss= 0.69600 test_acc= 0.65589 time= 8.38119\n",
      "Epoch: 0036 train_loss= 0.60245 train_acc= 0.71972 test_loss= 0.69546 test_acc= 0.65589 time= 8.30446\n",
      "Epoch: 0037 train_loss= 0.59900 train_acc= 0.72676 test_loss= 0.69491 test_acc= 0.65749 time= 8.32203\n",
      "Epoch: 0038 train_loss= 0.59557 train_acc= 0.73803 test_loss= 0.69434 test_acc= 0.65925 time= 8.41066\n",
      "Epoch: 0039 train_loss= 0.59213 train_acc= 0.73944 test_loss= 0.69376 test_acc= 0.65973 time= 8.55590\n",
      "Epoch: 0040 train_loss= 0.58875 train_acc= 0.74085 test_loss= 0.69318 test_acc= 0.66013 time= 8.60702\n",
      "Epoch: 0041 train_loss= 0.58540 train_acc= 0.74507 test_loss= 0.69262 test_acc= 0.65925 time= 8.46359\n",
      "Epoch: 0042 train_loss= 0.58206 train_acc= 0.74366 test_loss= 0.69208 test_acc= 0.65893 time= 8.44009\n",
      "Epoch: 0043 train_loss= 0.57876 train_acc= 0.74507 test_loss= 0.69158 test_acc= 0.65861 time= 8.49633\n",
      "Epoch: 0044 train_loss= 0.57549 train_acc= 0.74930 test_loss= 0.69109 test_acc= 0.65869 time= 8.52083\n",
      "Epoch: 0045 train_loss= 0.57227 train_acc= 0.75106 test_loss= 0.69058 test_acc= 0.65909 time= 8.45096\n",
      "Epoch: 0046 train_loss= 0.56901 train_acc= 0.75282 test_loss= 0.69004 test_acc= 0.65885 time= 8.66324\n",
      "Epoch: 0047 train_loss= 0.56577 train_acc= 0.75563 test_loss= 0.68953 test_acc= 0.65901 time= 8.62782\n",
      "Epoch: 0048 train_loss= 0.56256 train_acc= 0.75282 test_loss= 0.68900 test_acc= 0.65901 time= 8.43911\n",
      "Epoch: 0049 train_loss= 0.55933 train_acc= 0.75704 test_loss= 0.68851 test_acc= 0.65885 time= 8.59245\n",
      "Epoch: 0050 train_loss= 0.55615 train_acc= 0.75986 test_loss= 0.68803 test_acc= 0.65893 time= 8.51916\n",
      "Epoch: 0051 train_loss= 0.55294 train_acc= 0.76408 test_loss= 0.68759 test_acc= 0.65901 time= 8.42204\n",
      "Epoch: 0052 train_loss= 0.54975 train_acc= 0.77113 test_loss= 0.68714 test_acc= 0.65933 time= 8.59054\n",
      "Epoch: 0053 train_loss= 0.54659 train_acc= 0.77113 test_loss= 0.68677 test_acc= 0.66045 time= 8.51955\n",
      "Epoch: 0054 train_loss= 0.54345 train_acc= 0.77535 test_loss= 0.68643 test_acc= 0.66085 time= 8.56752\n",
      "Epoch: 0055 train_loss= 0.54031 train_acc= 0.77817 test_loss= 0.68614 test_acc= 0.66173 time= 8.86294\n",
      "Epoch: 0056 train_loss= 0.53719 train_acc= 0.77817 test_loss= 0.68584 test_acc= 0.66125 time= 8.91148\n",
      "Epoch: 0057 train_loss= 0.53406 train_acc= 0.77817 test_loss= 0.68557 test_acc= 0.66157 time= 8.76668\n",
      "Epoch: 0058 train_loss= 0.53093 train_acc= 0.78380 test_loss= 0.68533 test_acc= 0.66205 time= 8.63323\n",
      "Epoch: 0059 train_loss= 0.52779 train_acc= 0.78239 test_loss= 0.68516 test_acc= 0.66237 time= 8.78954\n",
      "Epoch: 0060 train_loss= 0.52468 train_acc= 0.78521 test_loss= 0.68505 test_acc= 0.66205 time= 8.70403\n",
      "Epoch: 0061 train_loss= 0.52159 train_acc= 0.78521 test_loss= 0.68503 test_acc= 0.66149 time= 8.71888\n",
      "Epoch: 0062 train_loss= 0.51850 train_acc= 0.78521 test_loss= 0.68509 test_acc= 0.66141 time= 9.20063\n",
      "Epoch: 0063 train_loss= 0.51543 train_acc= 0.78521 test_loss= 0.68520 test_acc= 0.66133 time= 9.39173\n",
      "Epoch: 0064 train_loss= 0.51238 train_acc= 0.78662 test_loss= 0.68539 test_acc= 0.66061 time= 9.07928\n",
      "Epoch: 0065 train_loss= 0.50934 train_acc= 0.79085 test_loss= 0.68561 test_acc= 0.66413 time= 8.84582\n",
      "Epoch: 0066 train_loss= 0.50633 train_acc= 0.79366 test_loss= 0.68588 test_acc= 0.66981 time= 8.78380\n",
      "early_stopping...\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(FLAGS.epochs):  #FLAGS.epochs\n",
    "    batches = range(round(train_num / batch_num))\n",
    "        \n",
    "    costs = np.zeros((len(batches), 2))\n",
    "    accs = np.zeros((len(batches), 2))\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    for ib in batches:\n",
    "        from_i = ib * batch_num\n",
    "        to_i = (ib+1) * batch_num\n",
    "            \n",
    "        features_train_batch = features_train[from_i:to_i]\n",
    "        support_train_batch = support_train[from_i:to_i]\n",
    "        y_train_batch = y_train[from_i:to_i]\n",
    "        feed_dict = construct_feed_dict(features_train_batch, support_train_batch, y_train_batch, placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "        feed_dict.update({placeholders['in_drop']: FLAGS.in_drop})\n",
    "\n",
    "        outs = sess.run([model.opt_op, model.loss, model.accuracy],feed_dict=feed_dict)\n",
    "\n",
    "        cost,acc,duration = evaluate(features_test, support_test, y_test, placeholders)\n",
    "        costs[ib] = [outs[1], cost]\n",
    "        accs[ib] = [outs[2], acc]\n",
    "            \n",
    "    costs = costs.mean(axis=0)\n",
    "    cost_train, cost_test = costs\n",
    "    cost_val.append(cost_test)\n",
    "        \n",
    "    accs = accs.mean(axis=0)\n",
    "    acc_train, acc_test = accs\n",
    "        \n",
    "        \n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cost_train),\n",
    "        \"train_acc=\", \"{:.5f}\".format(acc_train), \"test_loss=\", \"{:.5f}\".format(cost_test),\n",
    "        \"test_acc=\", \"{:.5f}\".format(acc_test), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "        \n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"early_stopping...\")\n",
    "        break\n",
    "\n",
    "m_saver.save(sess, bestModelSavePath0)\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "261caa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cost, test_acc, test_duration = evaluate(features_test, support_test, y_test, placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ef32695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 0.68672 accuracy= 0.67614 time= 0.08801\n"
     ]
    }
   ],
   "source": [
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "    \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "l1.append(test_acc)\n",
    "test_pre = model.predict()\n",
    "feed_dict = construct_feed_dict(features_test, support_test, y_test, placeholders)\n",
    "test_pre = sess.run([test_pre],feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68d9c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for p in test_pre[0]:\n",
    "    y_pred.append(round(p[0]))\n",
    "y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0e674d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy, sensivity, specificity, fscore, auc: 0.6761363636363636 0.6790123456790124 0.6736842105263158 0.6586826347305389 0.7230669265756985\n"
     ]
    }
   ],
   "source": [
    "[[TN, FP], [FN, TP]] = confusion_matrix(y_test, y_pred, labels=[0, 1]).astype(float)\n",
    "acc = (TP+TN)/(TP+TN+FP+FN)\n",
    "specificity = TN/(FP+TN)\n",
    "sensitivity = recall = TP/(TP+FN)\n",
    "fscore = f1_score(y_test, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(y_test,test_pre[0])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"accuracy, sensivity, specificity, fscore, auc:\", acc, sensitivity, specificity, fscore, roc_auc)\n",
    "result = [acc, sensitivity, specificity, fscore, roc_auc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea0c7a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 0.45845 train_acc= 0.79648 test_loss= 0.68654 test_acc= 0.67614 time= 7.41743\n",
      "Epoch: 0002 train_loss= 0.45828 train_acc= 0.79648 test_loss= 0.68643 test_acc= 0.67614 time= 7.51150\n",
      "Epoch: 0003 train_loss= 0.45804 train_acc= 0.79648 test_loss= 0.68627 test_acc= 0.67614 time= 7.90611\n",
      "Epoch: 0004 train_loss= 0.45775 train_acc= 0.79648 test_loss= 0.68606 test_acc= 0.67614 time= 7.51589\n",
      "Epoch: 0005 train_loss= 0.45743 train_acc= 0.79648 test_loss= 0.68581 test_acc= 0.67742 time= 7.82715\n",
      "Epoch: 0006 train_loss= 0.45708 train_acc= 0.79507 test_loss= 0.68553 test_acc= 0.68182 time= 7.83426\n",
      "Epoch: 0007 train_loss= 0.45671 train_acc= 0.79648 test_loss= 0.68521 test_acc= 0.68182 time= 8.09583\n",
      "Epoch: 0008 train_loss= 0.45631 train_acc= 0.79648 test_loss= 0.68487 test_acc= 0.68182 time= 8.11137\n",
      "Epoch: 0009 train_loss= 0.45590 train_acc= 0.79648 test_loss= 0.68449 test_acc= 0.68182 time= 8.03207\n",
      "Epoch: 0010 train_loss= 0.45548 train_acc= 0.79789 test_loss= 0.68409 test_acc= 0.68182 time= 7.96447\n",
      "Epoch: 0011 train_loss= 0.45504 train_acc= 0.79789 test_loss= 0.68368 test_acc= 0.68182 time= 8.26843\n",
      "Epoch: 0012 train_loss= 0.45460 train_acc= 0.79930 test_loss= 0.68326 test_acc= 0.67702 time= 8.48165\n",
      "Epoch: 0013 train_loss= 0.45415 train_acc= 0.79789 test_loss= 0.68285 test_acc= 0.67614 time= 8.08483\n",
      "Epoch: 0014 train_loss= 0.45368 train_acc= 0.80070 test_loss= 0.68243 test_acc= 0.67614 time= 8.23316\n",
      "Epoch: 0015 train_loss= 0.45318 train_acc= 0.80070 test_loss= 0.68202 test_acc= 0.67614 time= 8.34139\n",
      "Epoch: 0016 train_loss= 0.45267 train_acc= 0.80211 test_loss= 0.68160 test_acc= 0.67382 time= 8.30383\n",
      "Epoch: 0017 train_loss= 0.45213 train_acc= 0.80493 test_loss= 0.68120 test_acc= 0.67045 time= 8.21422\n",
      "Epoch: 0018 train_loss= 0.45159 train_acc= 0.80634 test_loss= 0.68080 test_acc= 0.67045 time= 8.14774\n",
      "Epoch: 0019 train_loss= 0.45103 train_acc= 0.80634 test_loss= 0.68041 test_acc= 0.67045 time= 8.02780\n",
      "Epoch: 0020 train_loss= 0.45047 train_acc= 0.80634 test_loss= 0.68003 test_acc= 0.67045 time= 8.06092\n",
      "Epoch: 0021 train_loss= 0.44989 train_acc= 0.80634 test_loss= 0.67965 test_acc= 0.67045 time= 8.07051\n",
      "Epoch: 0022 train_loss= 0.44930 train_acc= 0.80634 test_loss= 0.67929 test_acc= 0.67045 time= 8.08178\n",
      "Epoch: 0023 train_loss= 0.44871 train_acc= 0.80493 test_loss= 0.67893 test_acc= 0.67045 time= 7.87462\n",
      "Epoch: 0024 train_loss= 0.44811 train_acc= 0.80634 test_loss= 0.67858 test_acc= 0.67045 time= 7.70177\n",
      "Epoch: 0025 train_loss= 0.44751 train_acc= 0.80775 test_loss= 0.67824 test_acc= 0.67045 time= 7.70590\n",
      "Epoch: 0026 train_loss= 0.44691 train_acc= 0.81056 test_loss= 0.67790 test_acc= 0.67045 time= 7.96215\n",
      "Epoch: 0027 train_loss= 0.44631 train_acc= 0.81197 test_loss= 0.67757 test_acc= 0.67045 time= 8.20566\n",
      "Epoch: 0028 train_loss= 0.44571 train_acc= 0.81338 test_loss= 0.67725 test_acc= 0.67045 time= 8.52319\n",
      "Epoch: 0029 train_loss= 0.44512 train_acc= 0.81479 test_loss= 0.67693 test_acc= 0.67045 time= 8.93416\n",
      "Epoch: 0030 train_loss= 0.44452 train_acc= 0.81479 test_loss= 0.67661 test_acc= 0.67045 time= 9.19368\n",
      "Epoch: 0031 train_loss= 0.44392 train_acc= 0.81479 test_loss= 0.67630 test_acc= 0.67045 time= 9.12510\n",
      "Epoch: 0032 train_loss= 0.44331 train_acc= 0.82042 test_loss= 0.67599 test_acc= 0.67045 time= 9.11600\n",
      "Epoch: 0033 train_loss= 0.44270 train_acc= 0.82042 test_loss= 0.67568 test_acc= 0.67045 time= 9.40939\n",
      "Epoch: 0034 train_loss= 0.44210 train_acc= 0.82324 test_loss= 0.67537 test_acc= 0.67045 time= 9.29823\n",
      "Epoch: 0035 train_loss= 0.44149 train_acc= 0.82324 test_loss= 0.67508 test_acc= 0.67045 time= 9.22843\n",
      "Epoch: 0036 train_loss= 0.44089 train_acc= 0.82324 test_loss= 0.67480 test_acc= 0.67286 time= 9.04629\n",
      "Epoch: 0037 train_loss= 0.44030 train_acc= 0.82324 test_loss= 0.67453 test_acc= 0.67614 time= 8.93034\n",
      "Epoch: 0038 train_loss= 0.43971 train_acc= 0.82465 test_loss= 0.67427 test_acc= 0.67614 time= 9.00285\n",
      "Epoch: 0039 train_loss= 0.43913 train_acc= 0.82465 test_loss= 0.67402 test_acc= 0.67614 time= 8.95216\n",
      "Epoch: 0040 train_loss= 0.43855 train_acc= 0.82465 test_loss= 0.67378 test_acc= 0.67926 time= 9.39310\n",
      "Epoch: 0041 train_loss= 0.43799 train_acc= 0.82606 test_loss= 0.67355 test_acc= 0.68494 time= 9.26381\n",
      "Epoch: 0042 train_loss= 0.43743 train_acc= 0.82746 test_loss= 0.67334 test_acc= 0.68750 time= 9.03999\n",
      "Epoch: 0043 train_loss= 0.43689 train_acc= 0.82887 test_loss= 0.67314 test_acc= 0.68574 time= 8.92770\n",
      "Epoch: 0044 train_loss= 0.43635 train_acc= 0.83028 test_loss= 0.67296 test_acc= 0.68182 time= 9.01193\n",
      "Epoch: 0045 train_loss= 0.43583 train_acc= 0.83169 test_loss= 0.67278 test_acc= 0.68502 time= 9.32256\n",
      "Epoch: 0046 train_loss= 0.43531 train_acc= 0.83169 test_loss= 0.67261 test_acc= 0.69166 time= 9.29859\n",
      "Epoch: 0047 train_loss= 0.43481 train_acc= 0.83451 test_loss= 0.67246 test_acc= 0.69318 time= 9.08992\n",
      "Epoch: 0048 train_loss= 0.43432 train_acc= 0.83732 test_loss= 0.67231 test_acc= 0.69318 time= 9.18368\n",
      "Epoch: 0049 train_loss= 0.43383 train_acc= 0.83873 test_loss= 0.67217 test_acc= 0.69318 time= 9.00180\n",
      "Epoch: 0050 train_loss= 0.43336 train_acc= 0.83873 test_loss= 0.67203 test_acc= 0.69318 time= 9.09517\n",
      "Epoch: 0051 train_loss= 0.43290 train_acc= 0.83873 test_loss= 0.67190 test_acc= 0.69366 time= 9.23890\n",
      "Epoch: 0052 train_loss= 0.43245 train_acc= 0.83873 test_loss= 0.67177 test_acc= 0.69886 time= 9.24095\n",
      "Epoch: 0053 train_loss= 0.43201 train_acc= 0.84014 test_loss= 0.67164 test_acc= 0.69886 time= 9.31811\n",
      "Epoch: 0054 train_loss= 0.43158 train_acc= 0.84437 test_loss= 0.67152 test_acc= 0.69886 time= 9.28767\n",
      "Epoch: 0055 train_loss= 0.43116 train_acc= 0.84437 test_loss= 0.67139 test_acc= 0.69886 time= 9.29745\n",
      "Epoch: 0056 train_loss= 0.43074 train_acc= 0.84718 test_loss= 0.67128 test_acc= 0.69886 time= 9.30018\n",
      "Epoch: 0057 train_loss= 0.43034 train_acc= 0.84718 test_loss= 0.67117 test_acc= 0.69886 time= 9.30559\n",
      "Epoch: 0058 train_loss= 0.42994 train_acc= 0.84718 test_loss= 0.67106 test_acc= 0.69886 time= 9.22970\n",
      "Epoch: 0059 train_loss= 0.42955 train_acc= 0.84718 test_loss= 0.67096 test_acc= 0.69886 time= 9.19567\n",
      "Epoch: 0060 train_loss= 0.42917 train_acc= 0.84859 test_loss= 0.67087 test_acc= 0.69886 time= 9.33274\n",
      "Epoch: 0061 train_loss= 0.42879 train_acc= 0.84718 test_loss= 0.67079 test_acc= 0.69886 time= 9.09245\n",
      "Epoch: 0062 train_loss= 0.42843 train_acc= 0.84718 test_loss= 0.67071 test_acc= 0.69886 time= 9.15579\n",
      "Epoch: 0063 train_loss= 0.42807 train_acc= 0.84718 test_loss= 0.67064 test_acc= 0.69886 time= 9.38119\n",
      "Epoch: 0064 train_loss= 0.42771 train_acc= 0.84718 test_loss= 0.67058 test_acc= 0.69886 time= 9.65298\n",
      "Epoch: 0065 train_loss= 0.42736 train_acc= 0.84718 test_loss= 0.67052 test_acc= 0.69886 time= 9.61622\n",
      "Epoch: 0066 train_loss= 0.42702 train_acc= 0.84718 test_loss= 0.67048 test_acc= 0.69886 time= 9.74011\n",
      "Epoch: 0067 train_loss= 0.42668 train_acc= 0.84718 test_loss= 0.67044 test_acc= 0.69886 time= 9.60072\n",
      "Epoch: 0068 train_loss= 0.42634 train_acc= 0.84894 test_loss= 0.67041 test_acc= 0.69886 time= 9.69104\n",
      "Epoch: 0069 train_loss= 0.42601 train_acc= 0.84894 test_loss= 0.67039 test_acc= 0.69886 time= 9.33969\n",
      "Epoch: 0070 train_loss= 0.42569 train_acc= 0.84894 test_loss= 0.67038 test_acc= 0.69886 time= 9.38319\n",
      "Epoch: 0071 train_loss= 0.42536 train_acc= 0.85035 test_loss= 0.67038 test_acc= 0.69886 time= 9.22357\n",
      "Epoch: 0072 train_loss= 0.42505 train_acc= 0.85035 test_loss= 0.67038 test_acc= 0.69886 time= 9.25696\n",
      "Epoch: 0073 train_loss= 0.42473 train_acc= 0.85035 test_loss= 0.67039 test_acc= 0.69638 time= 9.22314\n",
      "Epoch: 0074 train_loss= 0.42443 train_acc= 0.85035 test_loss= 0.67040 test_acc= 0.69318 time= 9.20628\n",
      "Epoch: 0075 train_loss= 0.42412 train_acc= 0.85035 test_loss= 0.67041 test_acc= 0.69318 time= 9.24219\n",
      "Epoch: 0076 train_loss= 0.42383 train_acc= 0.85176 test_loss= 0.67042 test_acc= 0.69318 time= 9.26860\n",
      "Epoch: 0077 train_loss= 0.42353 train_acc= 0.85176 test_loss= 0.67044 test_acc= 0.69318 time= 9.24113\n",
      "Epoch: 0078 train_loss= 0.42324 train_acc= 0.85176 test_loss= 0.67046 test_acc= 0.69318 time= 9.31506\n",
      "Epoch: 0079 train_loss= 0.42296 train_acc= 0.85176 test_loss= 0.67048 test_acc= 0.69318 time= 9.29763\n",
      "Epoch: 0080 train_loss= 0.42268 train_acc= 0.85176 test_loss= 0.67049 test_acc= 0.69318 time= 9.26829\n",
      "Epoch: 0081 train_loss= 0.42240 train_acc= 0.85176 test_loss= 0.67051 test_acc= 0.69318 time= 9.32624\n",
      "Epoch: 0082 train_loss= 0.42213 train_acc= 0.85176 test_loss= 0.67052 test_acc= 0.69318 time= 9.25421\n",
      "Epoch: 0083 train_loss= 0.42187 train_acc= 0.85317 test_loss= 0.67053 test_acc= 0.69318 time= 9.32829\n",
      "Epoch: 0084 train_loss= 0.42160 train_acc= 0.85317 test_loss= 0.67054 test_acc= 0.69406 time= 9.31234\n",
      "Epoch: 0085 train_loss= 0.42135 train_acc= 0.85317 test_loss= 0.67055 test_acc= 0.69630 time= 9.20878\n",
      "Epoch: 0086 train_loss= 0.42109 train_acc= 0.85317 test_loss= 0.67055 test_acc= 0.69878 time= 9.31725\n",
      "Epoch: 0087 train_loss= 0.42084 train_acc= 0.85317 test_loss= 0.67056 test_acc= 0.69630 time= 9.34016\n",
      "Epoch: 0088 train_loss= 0.42059 train_acc= 0.85317 test_loss= 0.67056 test_acc= 0.69318 time= 9.30485\n",
      "Epoch: 0089 train_loss= 0.42035 train_acc= 0.85317 test_loss= 0.67056 test_acc= 0.69318 time= 9.32235\n",
      "Epoch: 0090 train_loss= 0.42011 train_acc= 0.85317 test_loss= 0.67056 test_acc= 0.69318 time= 9.26471\n",
      "Epoch: 0091 train_loss= 0.41986 train_acc= 0.85317 test_loss= 0.67056 test_acc= 0.69318 time= 9.27183\n",
      "Epoch: 0092 train_loss= 0.41963 train_acc= 0.85317 test_loss= 0.67056 test_acc= 0.69318 time= 9.24431\n",
      "Epoch: 0093 train_loss= 0.41939 train_acc= 0.85317 test_loss= 0.67055 test_acc= 0.69318 time= 9.30065\n",
      "Epoch: 0094 train_loss= 0.41916 train_acc= 0.85317 test_loss= 0.67055 test_acc= 0.69318 time= 9.34957\n",
      "Epoch: 0095 train_loss= 0.41893 train_acc= 0.85458 test_loss= 0.67055 test_acc= 0.69254 time= 9.42652\n",
      "Epoch: 0096 train_loss= 0.41870 train_acc= 0.85599 test_loss= 0.67054 test_acc= 0.69142 time= 9.62384\n",
      "Epoch: 0097 train_loss= 0.41847 train_acc= 0.85599 test_loss= 0.67054 test_acc= 0.68750 time= 9.15140\n",
      "Epoch: 0098 train_loss= 0.41824 train_acc= 0.85599 test_loss= 0.67053 test_acc= 0.68750 time= 9.19185\n",
      "Epoch: 0099 train_loss= 0.41802 train_acc= 0.85599 test_loss= 0.67053 test_acc= 0.68750 time= 9.65210\n",
      "Epoch: 0100 train_loss= 0.41780 train_acc= 0.85599 test_loss= 0.67053 test_acc= 0.68702 time= 9.76558\n",
      "Epoch: 0101 train_loss= 0.41758 train_acc= 0.85599 test_loss= 0.67053 test_acc= 0.68406 time= 9.33115\n",
      "Epoch: 0102 train_loss= 0.41736 train_acc= 0.85599 test_loss= 0.67053 test_acc= 0.68278 time= 9.26103\n",
      "Epoch: 0103 train_loss= 0.41715 train_acc= 0.85599 test_loss= 0.67052 test_acc= 0.68614 time= 9.15419\n",
      "Epoch: 0104 train_loss= 0.41694 train_acc= 0.85599 test_loss= 0.67052 test_acc= 0.68750 time= 9.48636\n",
      "Epoch: 0105 train_loss= 0.41673 train_acc= 0.85599 test_loss= 0.67052 test_acc= 0.68750 time= 9.43705\n",
      "Epoch: 0106 train_loss= 0.41652 train_acc= 0.85599 test_loss= 0.67052 test_acc= 0.68750 time= 9.79351\n",
      "Epoch: 0107 train_loss= 0.41631 train_acc= 0.85599 test_loss= 0.67052 test_acc= 0.68750 time= 9.17926\n",
      "Epoch: 0108 train_loss= 0.41611 train_acc= 0.85739 test_loss= 0.67052 test_acc= 0.68750 time= 9.09601\n",
      "Epoch: 0109 train_loss= 0.41591 train_acc= 0.85739 test_loss= 0.67052 test_acc= 0.68750 time= 9.58630\n",
      "Epoch: 0110 train_loss= 0.41571 train_acc= 0.85880 test_loss= 0.67051 test_acc= 0.68974 time= 9.58733\n",
      "Epoch: 0111 train_loss= 0.41551 train_acc= 0.85880 test_loss= 0.67051 test_acc= 0.69134 time= 9.24697\n",
      "Epoch: 0112 train_loss= 0.41532 train_acc= 0.85880 test_loss= 0.67050 test_acc= 0.69222 time= 9.17935\n",
      "Epoch: 0113 train_loss= 0.41513 train_acc= 0.85880 test_loss= 0.67050 test_acc= 0.69318 time= 9.31911\n",
      "Epoch: 0114 train_loss= 0.41494 train_acc= 0.85880 test_loss= 0.67049 test_acc= 0.69318 time= 9.29088\n",
      "Epoch: 0115 train_loss= 0.41476 train_acc= 0.85880 test_loss= 0.67048 test_acc= 0.69318 time= 9.43279\n",
      "Epoch: 0116 train_loss= 0.41457 train_acc= 0.85880 test_loss= 0.67048 test_acc= 0.69318 time= 9.61433\n",
      "Epoch: 0117 train_loss= 0.41439 train_acc= 0.85880 test_loss= 0.67047 test_acc= 0.69318 time= 9.31959\n",
      "Epoch: 0118 train_loss= 0.41422 train_acc= 0.85880 test_loss= 0.67046 test_acc= 0.69318 time= 9.39051\n",
      "Epoch: 0119 train_loss= 0.41404 train_acc= 0.85880 test_loss= 0.67045 test_acc= 0.69318 time= 9.19800\n",
      "Epoch: 0120 train_loss= 0.41387 train_acc= 0.85880 test_loss= 0.67044 test_acc= 0.69318 time= 9.21520\n",
      "Epoch: 0121 train_loss= 0.41370 train_acc= 0.85880 test_loss= 0.67044 test_acc= 0.69318 time= 9.29934\n",
      "Epoch: 0122 train_loss= 0.41353 train_acc= 0.85880 test_loss= 0.67043 test_acc= 0.69318 time= 9.32972\n",
      "Epoch: 0123 train_loss= 0.41337 train_acc= 0.85880 test_loss= 0.67042 test_acc= 0.69318 time= 9.31489\n",
      "Epoch: 0124 train_loss= 0.41320 train_acc= 0.85880 test_loss= 0.67041 test_acc= 0.69318 time= 9.26095\n",
      "Epoch: 0125 train_loss= 0.41304 train_acc= 0.85880 test_loss= 0.67041 test_acc= 0.69318 time= 9.40960\n",
      "Epoch: 0126 train_loss= 0.41288 train_acc= 0.85880 test_loss= 0.67040 test_acc= 0.69318 time= 9.33955\n",
      "Epoch: 0127 train_loss= 0.41273 train_acc= 0.86021 test_loss= 0.67039 test_acc= 0.69318 time= 9.29848\n",
      "Epoch: 0128 train_loss= 0.41257 train_acc= 0.86162 test_loss= 0.67038 test_acc= 0.69318 time= 9.50041\n",
      "Epoch: 0129 train_loss= 0.41242 train_acc= 0.86162 test_loss= 0.67037 test_acc= 0.69318 time= 9.77295\n",
      "Epoch: 0130 train_loss= 0.41227 train_acc= 0.86162 test_loss= 0.67036 test_acc= 0.69318 time= 9.72088\n",
      "Epoch: 0131 train_loss= 0.41212 train_acc= 0.86162 test_loss= 0.67035 test_acc= 0.69318 time= 9.84638\n",
      "Epoch: 0132 train_loss= 0.41198 train_acc= 0.86162 test_loss= 0.67034 test_acc= 0.69318 time= 9.68334\n",
      "Epoch: 0133 train_loss= 0.41183 train_acc= 0.86303 test_loss= 0.67032 test_acc= 0.69318 time= 9.42920\n",
      "Epoch: 0134 train_loss= 0.41169 train_acc= 0.86303 test_loss= 0.67030 test_acc= 0.69318 time= 9.27816\n",
      "Epoch: 0135 train_loss= 0.41155 train_acc= 0.86303 test_loss= 0.67029 test_acc= 0.69318 time= 9.69541\n",
      "Epoch: 0136 train_loss= 0.41141 train_acc= 0.86303 test_loss= 0.67027 test_acc= 0.69318 time= 9.34548\n",
      "Epoch: 0137 train_loss= 0.41127 train_acc= 0.86303 test_loss= 0.67025 test_acc= 0.69318 time= 9.56857\n",
      "Epoch: 0138 train_loss= 0.41114 train_acc= 0.86303 test_loss= 0.67023 test_acc= 0.69318 time= 9.47906\n",
      "Epoch: 0139 train_loss= 0.41100 train_acc= 0.86303 test_loss= 0.67020 test_acc= 0.69318 time= 9.70888\n",
      "Epoch: 0140 train_loss= 0.41087 train_acc= 0.86303 test_loss= 0.67018 test_acc= 0.69318 time= 9.31628\n",
      "Epoch: 0141 train_loss= 0.41074 train_acc= 0.86303 test_loss= 0.67016 test_acc= 0.69318 time= 9.29061\n",
      "Epoch: 0142 train_loss= 0.41061 train_acc= 0.86303 test_loss= 0.67014 test_acc= 0.69318 time= 9.26390\n",
      "Epoch: 0143 train_loss= 0.41049 train_acc= 0.86444 test_loss= 0.67011 test_acc= 0.69318 time= 9.73424\n",
      "Epoch: 0144 train_loss= 0.41036 train_acc= 0.86444 test_loss= 0.67009 test_acc= 0.69318 time= 9.58265\n",
      "Epoch: 0145 train_loss= 0.41024 train_acc= 0.86444 test_loss= 0.67006 test_acc= 0.69318 time= 9.15138\n",
      "Epoch: 0146 train_loss= 0.41012 train_acc= 0.86444 test_loss= 0.67004 test_acc= 0.69318 time= 9.29330\n",
      "Epoch: 0147 train_loss= 0.41000 train_acc= 0.86585 test_loss= 0.67001 test_acc= 0.69318 time= 9.25357\n",
      "Epoch: 0148 train_loss= 0.40988 train_acc= 0.86585 test_loss= 0.66999 test_acc= 0.69318 time= 9.33524\n",
      "Epoch: 0149 train_loss= 0.40976 train_acc= 0.86585 test_loss= 0.66996 test_acc= 0.69318 time= 9.23047\n",
      "Epoch: 0150 train_loss= 0.40965 train_acc= 0.86585 test_loss= 0.66993 test_acc= 0.69318 time= 9.52872\n",
      "Epoch: 0151 train_loss= 0.40953 train_acc= 0.86585 test_loss= 0.66991 test_acc= 0.69318 time= 9.76331\n",
      "Epoch: 0152 train_loss= 0.40942 train_acc= 0.86585 test_loss= 0.66988 test_acc= 0.69318 time= 9.33107\n",
      "Epoch: 0153 train_loss= 0.40931 train_acc= 0.86585 test_loss= 0.66985 test_acc= 0.69318 time= 9.20093\n",
      "Epoch: 0154 train_loss= 0.40920 train_acc= 0.86585 test_loss= 0.66982 test_acc= 0.69318 time= 9.26917\n",
      "Epoch: 0155 train_loss= 0.40909 train_acc= 0.86585 test_loss= 0.66979 test_acc= 0.69318 time= 9.35844\n",
      "Epoch: 0156 train_loss= 0.40899 train_acc= 0.86585 test_loss= 0.66976 test_acc= 0.69318 time= 9.32710\n",
      "Epoch: 0157 train_loss= 0.40888 train_acc= 0.86585 test_loss= 0.66973 test_acc= 0.69318 time= 9.67148\n",
      "Epoch: 0158 train_loss= 0.40877 train_acc= 0.86725 test_loss= 0.66970 test_acc= 0.69318 time= 9.65087\n",
      "Epoch: 0159 train_loss= 0.40867 train_acc= 0.86725 test_loss= 0.66967 test_acc= 0.69318 time= 9.23252\n",
      "Epoch: 0160 train_loss= 0.40856 train_acc= 0.86725 test_loss= 0.66964 test_acc= 0.69318 time= 9.54482\n",
      "Epoch: 0161 train_loss= 0.40846 train_acc= 0.86866 test_loss= 0.66960 test_acc= 0.69318 time= 9.39618\n",
      "Epoch: 0162 train_loss= 0.40836 train_acc= 0.86866 test_loss= 0.66957 test_acc= 0.69318 time= 9.43626\n",
      "Epoch: 0163 train_loss= 0.40826 train_acc= 0.86866 test_loss= 0.66953 test_acc= 0.69318 time= 9.24218\n",
      "Epoch: 0164 train_loss= 0.40816 train_acc= 0.86866 test_loss= 0.66950 test_acc= 0.69318 time= 9.13755\n",
      "Epoch: 0165 train_loss= 0.40806 train_acc= 0.86866 test_loss= 0.66946 test_acc= 0.69318 time= 9.63271\n",
      "Epoch: 0166 train_loss= 0.40796 train_acc= 0.86866 test_loss= 0.66942 test_acc= 0.69318 time= 9.49785\n",
      "Epoch: 0167 train_loss= 0.40786 train_acc= 0.86866 test_loss= 0.66938 test_acc= 0.69318 time= 9.55081\n",
      "Epoch: 0168 train_loss= 0.40776 train_acc= 0.86866 test_loss= 0.66934 test_acc= 0.69318 time= 9.40090\n",
      "Epoch: 0169 train_loss= 0.40767 train_acc= 0.86866 test_loss= 0.66930 test_acc= 0.69318 time= 9.50146\n",
      "Epoch: 0170 train_loss= 0.40757 train_acc= 0.86866 test_loss= 0.66926 test_acc= 0.69318 time= 9.55410\n",
      "Epoch: 0171 train_loss= 0.40748 train_acc= 0.86866 test_loss= 0.66922 test_acc= 0.69318 time= 9.43834\n",
      "Epoch: 0172 train_loss= 0.40738 train_acc= 0.86866 test_loss= 0.66918 test_acc= 0.69318 time= 9.63083\n",
      "Epoch: 0173 train_loss= 0.40729 train_acc= 0.87007 test_loss= 0.66914 test_acc= 0.69318 time= 9.56230\n",
      "Epoch: 0174 train_loss= 0.40719 train_acc= 0.87148 test_loss= 0.66910 test_acc= 0.69318 time= 9.48942\n",
      "Epoch: 0175 train_loss= 0.40710 train_acc= 0.87148 test_loss= 0.66905 test_acc= 0.69318 time= 9.47207\n",
      "Epoch: 0176 train_loss= 0.40701 train_acc= 0.87148 test_loss= 0.66901 test_acc= 0.69086 time= 9.50368\n",
      "Epoch: 0177 train_loss= 0.40692 train_acc= 0.87148 test_loss= 0.66897 test_acc= 0.68846 time= 9.69527\n",
      "Epoch: 0178 train_loss= 0.40682 train_acc= 0.87289 test_loss= 0.66893 test_acc= 0.68750 time= 9.67974\n",
      "Epoch: 0179 train_loss= 0.40673 train_acc= 0.87289 test_loss= 0.66888 test_acc= 0.68750 time= 9.24597\n",
      "Epoch: 0180 train_loss= 0.40664 train_acc= 0.87289 test_loss= 0.66884 test_acc= 0.68750 time= 9.58917\n",
      "Epoch: 0181 train_loss= 0.40655 train_acc= 0.87289 test_loss= 0.66880 test_acc= 0.68750 time= 9.58635\n",
      "Epoch: 0182 train_loss= 0.40646 train_acc= 0.87289 test_loss= 0.66875 test_acc= 0.68750 time= 9.38426\n",
      "Epoch: 0183 train_loss= 0.40638 train_acc= 0.87430 test_loss= 0.66871 test_acc= 0.68750 time= 9.75009\n",
      "Epoch: 0184 train_loss= 0.40629 train_acc= 0.87570 test_loss= 0.66867 test_acc= 0.68750 time= 9.51011\n",
      "Epoch: 0185 train_loss= 0.40620 train_acc= 0.87711 test_loss= 0.66862 test_acc= 0.68750 time= 9.36791\n",
      "Epoch: 0186 train_loss= 0.40612 train_acc= 0.87711 test_loss= 0.66858 test_acc= 0.68750 time= 9.35751\n",
      "Epoch: 0187 train_loss= 0.40603 train_acc= 0.87711 test_loss= 0.66854 test_acc= 0.68750 time= 9.24401\n",
      "Epoch: 0188 train_loss= 0.40595 train_acc= 0.87711 test_loss= 0.66850 test_acc= 0.68750 time= 9.24598\n",
      "Epoch: 0189 train_loss= 0.40586 train_acc= 0.87711 test_loss= 0.66846 test_acc= 0.68750 time= 9.30206\n",
      "Epoch: 0190 train_loss= 0.40578 train_acc= 0.87711 test_loss= 0.66842 test_acc= 0.68750 time= 9.46653\n",
      "Epoch: 0191 train_loss= 0.40570 train_acc= 0.87711 test_loss= 0.66838 test_acc= 0.68750 time= 9.31570\n",
      "Epoch: 0192 train_loss= 0.40562 train_acc= 0.87711 test_loss= 0.66834 test_acc= 0.68750 time= 9.47122\n",
      "Epoch: 0193 train_loss= 0.40554 train_acc= 0.87711 test_loss= 0.66831 test_acc= 0.68750 time= 9.37459\n",
      "Epoch: 0194 train_loss= 0.40546 train_acc= 0.87711 test_loss= 0.66827 test_acc= 0.68750 time= 9.39034\n",
      "Epoch: 0195 train_loss= 0.40538 train_acc= 0.87711 test_loss= 0.66824 test_acc= 0.68750 time= 9.78872\n",
      "Epoch: 0196 train_loss= 0.40530 train_acc= 0.87711 test_loss= 0.66820 test_acc= 0.68750 time= 10.01809\n",
      "Epoch: 0197 train_loss= 0.40523 train_acc= 0.87711 test_loss= 0.66817 test_acc= 0.68750 time= 9.92539\n",
      "Epoch: 0198 train_loss= 0.40515 train_acc= 0.87711 test_loss= 0.66813 test_acc= 0.68750 time= 9.55276\n",
      "Epoch: 0199 train_loss= 0.40507 train_acc= 0.87711 test_loss= 0.66810 test_acc= 0.68750 time= 9.69976\n",
      "Epoch: 0200 train_loss= 0.40500 train_acc= 0.87711 test_loss= 0.66806 test_acc= 0.68750 time= 9.48438\n",
      "Epoch: 0201 train_loss= 0.40492 train_acc= 0.87711 test_loss= 0.66803 test_acc= 0.68750 time= 9.48230\n",
      "Epoch: 0202 train_loss= 0.40485 train_acc= 0.87711 test_loss= 0.66799 test_acc= 0.68750 time= 9.40262\n",
      "Epoch: 0203 train_loss= 0.40478 train_acc= 0.87711 test_loss= 0.66796 test_acc= 0.68750 time= 9.57773\n",
      "Epoch: 0204 train_loss= 0.40471 train_acc= 0.87711 test_loss= 0.66792 test_acc= 0.68750 time= 9.67144\n",
      "Epoch: 0205 train_loss= 0.40464 train_acc= 0.87711 test_loss= 0.66789 test_acc= 0.68750 time= 9.65972\n",
      "Epoch: 0206 train_loss= 0.40457 train_acc= 0.87711 test_loss= 0.66786 test_acc= 0.68750 time= 9.55581\n",
      "Epoch: 0207 train_loss= 0.40450 train_acc= 0.87711 test_loss= 0.66783 test_acc= 0.68750 time= 9.76108\n",
      "Epoch: 0208 train_loss= 0.40443 train_acc= 0.87711 test_loss= 0.66780 test_acc= 0.68750 time= 9.88115\n",
      "Epoch: 0209 train_loss= 0.40436 train_acc= 0.87852 test_loss= 0.66777 test_acc= 0.68750 time= 9.48085\n",
      "Epoch: 0210 train_loss= 0.40429 train_acc= 0.87852 test_loss= 0.66774 test_acc= 0.68750 time= 9.65752\n",
      "Epoch: 0211 train_loss= 0.40423 train_acc= 0.87852 test_loss= 0.66771 test_acc= 0.68750 time= 9.66707\n",
      "Epoch: 0212 train_loss= 0.40416 train_acc= 0.87993 test_loss= 0.66768 test_acc= 0.68750 time= 9.76152\n",
      "Epoch: 0213 train_loss= 0.40409 train_acc= 0.87993 test_loss= 0.66766 test_acc= 0.68750 time= 9.87053\n",
      "Epoch: 0214 train_loss= 0.40403 train_acc= 0.87993 test_loss= 0.66763 test_acc= 0.68750 time= 9.53230\n",
      "Epoch: 0215 train_loss= 0.40396 train_acc= 0.87993 test_loss= 0.66760 test_acc= 0.68750 time= 9.57874\n",
      "Epoch: 0216 train_loss= 0.40390 train_acc= 0.87852 test_loss= 0.66758 test_acc= 0.68750 time= 9.39325\n",
      "Epoch: 0217 train_loss= 0.40384 train_acc= 0.87852 test_loss= 0.66756 test_acc= 0.68750 time= 9.47437\n",
      "Epoch: 0218 train_loss= 0.40378 train_acc= 0.87852 test_loss= 0.66753 test_acc= 0.68750 time= 9.48161\n",
      "Epoch: 0219 train_loss= 0.40371 train_acc= 0.87852 test_loss= 0.66751 test_acc= 0.68750 time= 9.58898\n",
      "Epoch: 0220 train_loss= 0.40365 train_acc= 0.87852 test_loss= 0.66749 test_acc= 0.68750 time= 9.43850\n",
      "Epoch: 0221 train_loss= 0.40359 train_acc= 0.87852 test_loss= 0.66747 test_acc= 0.68750 time= 9.52898\n",
      "Epoch: 0222 train_loss= 0.40353 train_acc= 0.87852 test_loss= 0.66745 test_acc= 0.68750 time= 9.54231\n",
      "Epoch: 0223 train_loss= 0.40347 train_acc= 0.87852 test_loss= 0.66743 test_acc= 0.68750 time= 9.57724\n",
      "Epoch: 0224 train_loss= 0.40341 train_acc= 0.87852 test_loss= 0.66741 test_acc= 0.68750 time= 9.49305\n",
      "Epoch: 0225 train_loss= 0.40336 train_acc= 0.87852 test_loss= 0.66739 test_acc= 0.68750 time= 9.66751\n",
      "Epoch: 0226 train_loss= 0.40330 train_acc= 0.87852 test_loss= 0.66737 test_acc= 0.68750 time= 9.55186\n",
      "Epoch: 0227 train_loss= 0.40324 train_acc= 0.87852 test_loss= 0.66735 test_acc= 0.68750 time= 9.53083\n",
      "Epoch: 0228 train_loss= 0.40318 train_acc= 0.87852 test_loss= 0.66733 test_acc= 0.68750 time= 9.73341\n",
      "Epoch: 0229 train_loss= 0.40312 train_acc= 0.87852 test_loss= 0.66731 test_acc= 0.68750 time= 9.47010\n",
      "Epoch: 0230 train_loss= 0.40307 train_acc= 0.87852 test_loss= 0.66730 test_acc= 0.68750 time= 9.57140\n",
      "Epoch: 0231 train_loss= 0.40301 train_acc= 0.87852 test_loss= 0.66728 test_acc= 0.68750 time= 9.67038\n",
      "Epoch: 0232 train_loss= 0.40296 train_acc= 0.87852 test_loss= 0.66726 test_acc= 0.68750 time= 9.59251\n",
      "Epoch: 0233 train_loss= 0.40290 train_acc= 0.87852 test_loss= 0.66725 test_acc= 0.68750 time= 9.48491\n",
      "Epoch: 0234 train_loss= 0.40284 train_acc= 0.87852 test_loss= 0.66723 test_acc= 0.68750 time= 9.48573\n",
      "Epoch: 0235 train_loss= 0.40279 train_acc= 0.87852 test_loss= 0.66722 test_acc= 0.68750 time= 9.42973\n",
      "Epoch: 0236 train_loss= 0.40273 train_acc= 0.87852 test_loss= 0.66720 test_acc= 0.68750 time= 9.66980\n",
      "Epoch: 0237 train_loss= 0.40268 train_acc= 0.87852 test_loss= 0.66718 test_acc= 0.68750 time= 9.52453\n",
      "Epoch: 0238 train_loss= 0.40262 train_acc= 0.87852 test_loss= 0.66717 test_acc= 0.68750 time= 9.39379\n",
      "Epoch: 0239 train_loss= 0.40257 train_acc= 0.87852 test_loss= 0.66716 test_acc= 0.68750 time= 9.64739\n",
      "Epoch: 0240 train_loss= 0.40252 train_acc= 0.87852 test_loss= 0.66714 test_acc= 0.68750 time= 9.59255\n",
      "Epoch: 0241 train_loss= 0.40246 train_acc= 0.87852 test_loss= 0.66713 test_acc= 0.68750 time= 9.47969\n",
      "Epoch: 0242 train_loss= 0.40241 train_acc= 0.87852 test_loss= 0.66712 test_acc= 0.68750 time= 9.48912\n",
      "Epoch: 0243 train_loss= 0.40236 train_acc= 0.87852 test_loss= 0.66710 test_acc= 0.68750 time= 9.64271\n",
      "Epoch: 0244 train_loss= 0.40231 train_acc= 0.87852 test_loss= 0.66709 test_acc= 0.68750 time= 9.91066\n",
      "Epoch: 0245 train_loss= 0.40226 train_acc= 0.87711 test_loss= 0.66708 test_acc= 0.68750 time= 9.71983\n",
      "Epoch: 0246 train_loss= 0.40221 train_acc= 0.87852 test_loss= 0.66707 test_acc= 0.68750 time= 9.69368\n",
      "Epoch: 0247 train_loss= 0.40216 train_acc= 0.87852 test_loss= 0.66705 test_acc= 0.68750 time= 9.51661\n",
      "Epoch: 0248 train_loss= 0.40211 train_acc= 0.87852 test_loss= 0.66704 test_acc= 0.68750 time= 9.46633\n",
      "Epoch: 0249 train_loss= 0.40206 train_acc= 0.87852 test_loss= 0.66703 test_acc= 0.68750 time= 9.53947\n",
      "Epoch: 0250 train_loss= 0.40201 train_acc= 0.87852 test_loss= 0.66702 test_acc= 0.68750 time= 9.58155\n",
      "Epoch: 0251 train_loss= 0.40196 train_acc= 0.87993 test_loss= 0.66700 test_acc= 0.68750 time= 9.47433\n",
      "Epoch: 0252 train_loss= 0.40192 train_acc= 0.87993 test_loss= 0.66699 test_acc= 0.68750 time= 9.65298\n",
      "Epoch: 0253 train_loss= 0.40187 train_acc= 0.87993 test_loss= 0.66698 test_acc= 0.68750 time= 9.67618\n",
      "Epoch: 0254 train_loss= 0.40182 train_acc= 0.87993 test_loss= 0.66696 test_acc= 0.68750 time= 9.52885\n",
      "Epoch: 0255 train_loss= 0.40178 train_acc= 0.87993 test_loss= 0.66695 test_acc= 0.68750 time= 9.65270\n",
      "Epoch: 0256 train_loss= 0.40173 train_acc= 0.87993 test_loss= 0.66694 test_acc= 0.68750 time= 9.78796\n",
      "Epoch: 0257 train_loss= 0.40169 train_acc= 0.87993 test_loss= 0.66693 test_acc= 0.68750 time= 9.75675\n",
      "Epoch: 0258 train_loss= 0.40164 train_acc= 0.87993 test_loss= 0.66691 test_acc= 0.68750 time= 9.74316\n",
      "Epoch: 0259 train_loss= 0.40160 train_acc= 0.87993 test_loss= 0.66690 test_acc= 0.68750 time= 9.85442\n",
      "Epoch: 0260 train_loss= 0.40155 train_acc= 0.87993 test_loss= 0.66689 test_acc= 0.68750 time= 9.62220\n",
      "Epoch: 0261 train_loss= 0.40151 train_acc= 0.87993 test_loss= 0.66687 test_acc= 0.68750 time= 9.70645\n",
      "Epoch: 0262 train_loss= 0.40147 train_acc= 0.87993 test_loss= 0.66686 test_acc= 0.68750 time= 9.91772\n",
      "Epoch: 0263 train_loss= 0.40142 train_acc= 0.87993 test_loss= 0.66684 test_acc= 0.68750 time= 9.83544\n",
      "Epoch: 0264 train_loss= 0.40138 train_acc= 0.87993 test_loss= 0.66683 test_acc= 0.68750 time= 9.69817\n",
      "Epoch: 0265 train_loss= 0.40134 train_acc= 0.87993 test_loss= 0.66681 test_acc= 0.68750 time= 9.66720\n",
      "Epoch: 0266 train_loss= 0.40130 train_acc= 0.87993 test_loss= 0.66679 test_acc= 0.68750 time= 9.58955\n",
      "Epoch: 0267 train_loss= 0.40126 train_acc= 0.87993 test_loss= 0.66677 test_acc= 0.68750 time= 9.77862\n",
      "Epoch: 0268 train_loss= 0.40122 train_acc= 0.87993 test_loss= 0.66675 test_acc= 0.68750 time= 9.76898\n",
      "Epoch: 0269 train_loss= 0.40118 train_acc= 0.87993 test_loss= 0.66673 test_acc= 0.68750 time= 9.67373\n",
      "Epoch: 0270 train_loss= 0.40114 train_acc= 0.87993 test_loss= 0.66671 test_acc= 0.68750 time= 9.81243\n",
      "Epoch: 0271 train_loss= 0.40110 train_acc= 0.87993 test_loss= 0.66669 test_acc= 0.68750 time= 9.67366\n",
      "Epoch: 0272 train_loss= 0.40106 train_acc= 0.87993 test_loss= 0.66667 test_acc= 0.68750 time= 9.57355\n",
      "Epoch: 0273 train_loss= 0.40102 train_acc= 0.87993 test_loss= 0.66665 test_acc= 0.68750 time= 9.77678\n",
      "Epoch: 0274 train_loss= 0.40098 train_acc= 0.87993 test_loss= 0.66663 test_acc= 0.68750 time= 9.77134\n",
      "Epoch: 0275 train_loss= 0.40094 train_acc= 0.87993 test_loss= 0.66660 test_acc= 0.68750 time= 9.76094\n",
      "Epoch: 0276 train_loss= 0.40090 train_acc= 0.87993 test_loss= 0.66658 test_acc= 0.68750 time= 9.71249\n",
      "Epoch: 0277 train_loss= 0.40086 train_acc= 0.87993 test_loss= 0.66655 test_acc= 0.68750 time= 9.82220\n",
      "Epoch: 0278 train_loss= 0.40082 train_acc= 0.87993 test_loss= 0.66653 test_acc= 0.68750 time= 9.78582\n",
      "Epoch: 0279 train_loss= 0.40079 train_acc= 0.87993 test_loss= 0.66650 test_acc= 0.68750 time= 9.80172\n",
      "Epoch: 0280 train_loss= 0.40075 train_acc= 0.88134 test_loss= 0.66648 test_acc= 0.68750 time= 9.80658\n",
      "Epoch: 0281 train_loss= 0.40071 train_acc= 0.88134 test_loss= 0.66645 test_acc= 0.68750 time= 9.92798\n",
      "Epoch: 0282 train_loss= 0.40067 train_acc= 0.88134 test_loss= 0.66642 test_acc= 0.68750 time= 9.69662\n",
      "Epoch: 0283 train_loss= 0.40064 train_acc= 0.88134 test_loss= 0.66640 test_acc= 0.68750 time= 9.73451\n",
      "Epoch: 0284 train_loss= 0.40060 train_acc= 0.88134 test_loss= 0.66637 test_acc= 0.68750 time= 9.76341\n",
      "Epoch: 0285 train_loss= 0.40056 train_acc= 0.88134 test_loss= 0.66634 test_acc= 0.68750 time= 9.69536\n",
      "Epoch: 0286 train_loss= 0.40053 train_acc= 0.88134 test_loss= 0.66632 test_acc= 0.68750 time= 9.68779\n",
      "Epoch: 0287 train_loss= 0.40049 train_acc= 0.88134 test_loss= 0.66629 test_acc= 0.68750 time= 9.79408\n",
      "Epoch: 0288 train_loss= 0.40046 train_acc= 0.88134 test_loss= 0.66627 test_acc= 0.68750 time= 9.80151\n",
      "Epoch: 0289 train_loss= 0.40042 train_acc= 0.88134 test_loss= 0.66624 test_acc= 0.68750 time= 9.74190\n",
      "Epoch: 0290 train_loss= 0.40038 train_acc= 0.88134 test_loss= 0.66622 test_acc= 0.68750 time= 9.70284\n",
      "Epoch: 0291 train_loss= 0.40035 train_acc= 0.88134 test_loss= 0.66619 test_acc= 0.68750 time= 9.87417\n",
      "Epoch: 0292 train_loss= 0.40032 train_acc= 0.88134 test_loss= 0.66617 test_acc= 0.68750 time= 9.64554\n",
      "Epoch: 0293 train_loss= 0.40028 train_acc= 0.88134 test_loss= 0.66615 test_acc= 0.68750 time= 9.73453\n",
      "Epoch: 0294 train_loss= 0.40025 train_acc= 0.88134 test_loss= 0.66612 test_acc= 0.68750 time= 9.76114\n",
      "Epoch: 0295 train_loss= 0.40021 train_acc= 0.88134 test_loss= 0.66610 test_acc= 0.68750 time= 9.88410\n",
      "Epoch: 0296 train_loss= 0.40018 train_acc= 0.88134 test_loss= 0.66608 test_acc= 0.68750 time= 9.73455\n",
      "Epoch: 0297 train_loss= 0.40015 train_acc= 0.88134 test_loss= 0.66606 test_acc= 0.68750 time= 9.82559\n",
      "Epoch: 0298 train_loss= 0.40011 train_acc= 0.88134 test_loss= 0.66604 test_acc= 0.68750 time= 9.79640\n",
      "Epoch: 0299 train_loss= 0.40008 train_acc= 0.88134 test_loss= 0.66602 test_acc= 0.68750 time= 9.75384\n",
      "Epoch: 0300 train_loss= 0.40005 train_acc= 0.88134 test_loss= 0.66600 test_acc= 0.68750 time= 9.69141\n",
      "Epoch: 0301 train_loss= 0.40001 train_acc= 0.88134 test_loss= 0.66598 test_acc= 0.68750 time= 9.68892\n",
      "Epoch: 0302 train_loss= 0.39998 train_acc= 0.88134 test_loss= 0.66596 test_acc= 0.68750 time= 9.72872\n",
      "Epoch: 0303 train_loss= 0.39995 train_acc= 0.88134 test_loss= 0.66594 test_acc= 0.68750 time= 9.72963\n",
      "Epoch: 0304 train_loss= 0.39992 train_acc= 0.88134 test_loss= 0.66592 test_acc= 0.68750 time= 9.64927\n",
      "Epoch: 0305 train_loss= 0.39989 train_acc= 0.88134 test_loss= 0.66590 test_acc= 0.68750 time= 9.62863\n",
      "Epoch: 0306 train_loss= 0.39986 train_acc= 0.88275 test_loss= 0.66589 test_acc= 0.68750 time= 9.65652\n",
      "Epoch: 0307 train_loss= 0.39982 train_acc= 0.88275 test_loss= 0.66587 test_acc= 0.68750 time= 9.60308\n",
      "Epoch: 0308 train_loss= 0.39979 train_acc= 0.88275 test_loss= 0.66585 test_acc= 0.68750 time= 9.74650\n",
      "Epoch: 0309 train_loss= 0.39976 train_acc= 0.88275 test_loss= 0.66584 test_acc= 0.68750 time= 9.81344\n",
      "Epoch: 0310 train_loss= 0.39973 train_acc= 0.88275 test_loss= 0.66582 test_acc= 0.68750 time= 9.60749\n",
      "Epoch: 0311 train_loss= 0.39970 train_acc= 0.88275 test_loss= 0.66581 test_acc= 0.68750 time= 9.69481\n",
      "Epoch: 0312 train_loss= 0.39967 train_acc= 0.88275 test_loss= 0.66580 test_acc= 0.68750 time= 9.78595\n",
      "Epoch: 0313 train_loss= 0.39964 train_acc= 0.88275 test_loss= 0.66578 test_acc= 0.68750 time= 9.70524\n",
      "Epoch: 0314 train_loss= 0.39961 train_acc= 0.88275 test_loss= 0.66577 test_acc= 0.68750 time= 9.73733\n",
      "Epoch: 0315 train_loss= 0.39958 train_acc= 0.88275 test_loss= 0.66576 test_acc= 0.68750 time= 9.81292\n",
      "Epoch: 0316 train_loss= 0.39955 train_acc= 0.88275 test_loss= 0.66575 test_acc= 0.68750 time= 9.67623\n",
      "Epoch: 0317 train_loss= 0.39952 train_acc= 0.88275 test_loss= 0.66574 test_acc= 0.68750 time= 9.71080\n",
      "Epoch: 0318 train_loss= 0.39949 train_acc= 0.88275 test_loss= 0.66573 test_acc= 0.68750 time= 9.94289\n",
      "Epoch: 0319 train_loss= 0.39946 train_acc= 0.88275 test_loss= 0.66572 test_acc= 0.68750 time= 9.62103\n",
      "Epoch: 0320 train_loss= 0.39943 train_acc= 0.88275 test_loss= 0.66571 test_acc= 0.68750 time= 9.79045\n",
      "Epoch: 0321 train_loss= 0.39940 train_acc= 0.88275 test_loss= 0.66571 test_acc= 0.68750 time= 10.01874\n",
      "Epoch: 0322 train_loss= 0.39937 train_acc= 0.88275 test_loss= 0.66570 test_acc= 0.68750 time= 9.72574\n",
      "Epoch: 0323 train_loss= 0.39934 train_acc= 0.88275 test_loss= 0.66570 test_acc= 0.68750 time= 9.84544\n",
      "Epoch: 0324 train_loss= 0.39931 train_acc= 0.88415 test_loss= 0.66569 test_acc= 0.68750 time= 9.81316\n",
      "Epoch: 0325 train_loss= 0.39928 train_acc= 0.88415 test_loss= 0.66569 test_acc= 0.68750 time= 9.70861\n",
      "Epoch: 0326 train_loss= 0.39925 train_acc= 0.88415 test_loss= 0.66568 test_acc= 0.68750 time= 9.78038\n",
      "Epoch: 0327 train_loss= 0.39922 train_acc= 0.88415 test_loss= 0.66568 test_acc= 0.68750 time= 9.94533\n",
      "Epoch: 0328 train_loss= 0.39919 train_acc= 0.88415 test_loss= 0.66568 test_acc= 0.68750 time= 9.70370\n",
      "Epoch: 0329 train_loss= 0.39916 train_acc= 0.88415 test_loss= 0.66568 test_acc= 0.68750 time= 9.77846\n",
      "Epoch: 0330 train_loss= 0.39914 train_acc= 0.88415 test_loss= 0.66568 test_acc= 0.68750 time= 9.96475\n",
      "Epoch: 0331 train_loss= 0.39911 train_acc= 0.88415 test_loss= 0.66569 test_acc= 0.68750 time= 9.75258\n",
      "Epoch: 0332 train_loss= 0.39908 train_acc= 0.88415 test_loss= 0.66569 test_acc= 0.68750 time= 9.68946\n",
      "Epoch: 0333 train_loss= 0.39905 train_acc= 0.88415 test_loss= 0.66569 test_acc= 0.68750 time= 9.78045\n",
      "Epoch: 0334 train_loss= 0.39902 train_acc= 0.88415 test_loss= 0.66570 test_acc= 0.68750 time= 9.88527\n",
      "Epoch: 0335 train_loss= 0.39899 train_acc= 0.88415 test_loss= 0.66570 test_acc= 0.68750 time= 9.69014\n",
      "Epoch: 0336 train_loss= 0.39896 train_acc= 0.88415 test_loss= 0.66571 test_acc= 0.68750 time= 9.84831\n",
      "Epoch: 0337 train_loss= 0.39893 train_acc= 0.88415 test_loss= 0.66571 test_acc= 0.68750 time= 9.83692\n",
      "Epoch: 0338 train_loss= 0.39891 train_acc= 0.88415 test_loss= 0.66572 test_acc= 0.68750 time= 9.76403\n",
      "Epoch: 0339 train_loss= 0.39888 train_acc= 0.88415 test_loss= 0.66573 test_acc= 0.68750 time= 9.83704\n",
      "Epoch: 0340 train_loss= 0.39885 train_acc= 0.88415 test_loss= 0.66574 test_acc= 0.68750 time= 9.89512\n",
      "Epoch: 0341 train_loss= 0.39882 train_acc= 0.88415 test_loss= 0.66574 test_acc= 0.68750 time= 9.79168\n",
      "Epoch: 0342 train_loss= 0.39879 train_acc= 0.88415 test_loss= 0.66575 test_acc= 0.68750 time= 9.92411\n",
      "Epoch: 0343 train_loss= 0.39877 train_acc= 0.88415 test_loss= 0.66576 test_acc= 0.68750 time= 9.86556\n",
      "Epoch: 0344 train_loss= 0.39874 train_acc= 0.88556 test_loss= 0.66577 test_acc= 0.68750 time= 9.78991\n",
      "Epoch: 0345 train_loss= 0.39871 train_acc= 0.88556 test_loss= 0.66578 test_acc= 0.68750 time= 9.92669\n",
      "Epoch: 0346 train_loss= 0.39869 train_acc= 0.88556 test_loss= 0.66579 test_acc= 0.68750 time= 9.95354\n",
      "Epoch: 0347 train_loss= 0.39866 train_acc= 0.88556 test_loss= 0.66580 test_acc= 0.68750 time= 9.80832\n",
      "Epoch: 0348 train_loss= 0.39863 train_acc= 0.88556 test_loss= 0.66581 test_acc= 0.68750 time= 9.92340\n",
      "Epoch: 0349 train_loss= 0.39861 train_acc= 0.88556 test_loss= 0.66582 test_acc= 0.68750 time= 9.74901\n",
      "Epoch: 0350 train_loss= 0.39858 train_acc= 0.88556 test_loss= 0.66583 test_acc= 0.68750 time= 9.83467\n",
      "Epoch: 0351 train_loss= 0.39856 train_acc= 0.88556 test_loss= 0.66584 test_acc= 0.68750 time= 10.08970\n",
      "Epoch: 0352 train_loss= 0.39853 train_acc= 0.88556 test_loss= 0.66585 test_acc= 0.68750 time= 8.81467\n",
      "Epoch: 0353 train_loss= 0.39850 train_acc= 0.88556 test_loss= 0.66586 test_acc= 0.68750 time= 8.73094\n",
      "Epoch: 0354 train_loss= 0.39848 train_acc= 0.88556 test_loss= 0.66587 test_acc= 0.68750 time= 8.81932\n",
      "Epoch: 0355 train_loss= 0.39845 train_acc= 0.88556 test_loss= 0.66588 test_acc= 0.68750 time= 8.77584\n",
      "Epoch: 0356 train_loss= 0.39843 train_acc= 0.88556 test_loss= 0.66589 test_acc= 0.68750 time= 8.86204\n",
      "Epoch: 0357 train_loss= 0.39840 train_acc= 0.88556 test_loss= 0.66590 test_acc= 0.68750 time= 8.75567\n",
      "Epoch: 0358 train_loss= 0.39838 train_acc= 0.88556 test_loss= 0.66591 test_acc= 0.68750 time= 8.93254\n",
      "Epoch: 0359 train_loss= 0.39836 train_acc= 0.88556 test_loss= 0.66593 test_acc= 0.68750 time= 9.01436\n",
      "Epoch: 0360 train_loss= 0.39833 train_acc= 0.88556 test_loss= 0.66594 test_acc= 0.68750 time= 8.89714\n",
      "Epoch: 0361 train_loss= 0.39831 train_acc= 0.88556 test_loss= 0.66595 test_acc= 0.68750 time= 8.75782\n",
      "Epoch: 0362 train_loss= 0.39829 train_acc= 0.88556 test_loss= 0.66596 test_acc= 0.68750 time= 8.80023\n",
      "Epoch: 0363 train_loss= 0.39826 train_acc= 0.88556 test_loss= 0.66597 test_acc= 0.68750 time= 8.69545\n",
      "Epoch: 0364 train_loss= 0.39824 train_acc= 0.88556 test_loss= 0.66598 test_acc= 0.68750 time= 8.69616\n",
      "Epoch: 0365 train_loss= 0.39822 train_acc= 0.88556 test_loss= 0.66599 test_acc= 0.68750 time= 8.67699\n",
      "Epoch: 0366 train_loss= 0.39819 train_acc= 0.88556 test_loss= 0.66600 test_acc= 0.68750 time= 8.67082\n",
      "Epoch: 0367 train_loss= 0.39817 train_acc= 0.88556 test_loss= 0.66601 test_acc= 0.68750 time= 8.63798\n",
      "Epoch: 0368 train_loss= 0.39815 train_acc= 0.88556 test_loss= 0.66602 test_acc= 0.68750 time= 8.66224\n",
      "Epoch: 0369 train_loss= 0.39813 train_acc= 0.88556 test_loss= 0.66603 test_acc= 0.68750 time= 8.70920\n",
      "Epoch: 0370 train_loss= 0.39811 train_acc= 0.88556 test_loss= 0.66604 test_acc= 0.68750 time= 8.76252\n",
      "Epoch: 0371 train_loss= 0.39808 train_acc= 0.88556 test_loss= 0.66605 test_acc= 0.68750 time= 8.75278\n",
      "Epoch: 0372 train_loss= 0.39806 train_acc= 0.88556 test_loss= 0.66606 test_acc= 0.68750 time= 8.64238\n",
      "Epoch: 0373 train_loss= 0.39804 train_acc= 0.88556 test_loss= 0.66607 test_acc= 0.68750 time= 8.77848\n",
      "Epoch: 0374 train_loss= 0.39802 train_acc= 0.88556 test_loss= 0.66608 test_acc= 0.68750 time= 8.69099\n",
      "Epoch: 0375 train_loss= 0.39800 train_acc= 0.88556 test_loss= 0.66609 test_acc= 0.68750 time= 8.79630\n",
      "Epoch: 0376 train_loss= 0.39798 train_acc= 0.88556 test_loss= 0.66610 test_acc= 0.68750 time= 8.67378\n",
      "Epoch: 0377 train_loss= 0.39796 train_acc= 0.88556 test_loss= 0.66611 test_acc= 0.68750 time= 8.71394\n",
      "Epoch: 0378 train_loss= 0.39794 train_acc= 0.88556 test_loss= 0.66611 test_acc= 0.68750 time= 8.62540\n",
      "Epoch: 0379 train_loss= 0.39792 train_acc= 0.88556 test_loss= 0.66612 test_acc= 0.68750 time= 8.69918\n",
      "Epoch: 0380 train_loss= 0.39790 train_acc= 0.88556 test_loss= 0.66613 test_acc= 0.68750 time= 8.66763\n",
      "Epoch: 0381 train_loss= 0.39788 train_acc= 0.88556 test_loss= 0.66614 test_acc= 0.68750 time= 8.69979\n",
      "Epoch: 0382 train_loss= 0.39786 train_acc= 0.88556 test_loss= 0.66614 test_acc= 0.68750 time= 8.63377\n",
      "Epoch: 0383 train_loss= 0.39784 train_acc= 0.88556 test_loss= 0.66615 test_acc= 0.68750 time= 8.65610\n",
      "Epoch: 0384 train_loss= 0.39782 train_acc= 0.88556 test_loss= 0.66616 test_acc= 0.68750 time= 8.71274\n",
      "Epoch: 0385 train_loss= 0.39780 train_acc= 0.88556 test_loss= 0.66616 test_acc= 0.68750 time= 8.74713\n",
      "Epoch: 0386 train_loss= 0.39778 train_acc= 0.88556 test_loss= 0.66617 test_acc= 0.68750 time= 8.71770\n",
      "Epoch: 0387 train_loss= 0.39776 train_acc= 0.88415 test_loss= 0.66617 test_acc= 0.68750 time= 8.65349\n",
      "Epoch: 0388 train_loss= 0.39775 train_acc= 0.88415 test_loss= 0.66618 test_acc= 0.68750 time= 8.74649\n",
      "Epoch: 0389 train_loss= 0.39773 train_acc= 0.88415 test_loss= 0.66619 test_acc= 0.68750 time= 8.66747\n",
      "Epoch: 0390 train_loss= 0.39771 train_acc= 0.88415 test_loss= 0.66619 test_acc= 0.68750 time= 8.65497\n",
      "Epoch: 0391 train_loss= 0.39769 train_acc= 0.88415 test_loss= 0.66619 test_acc= 0.68750 time= 8.70033\n",
      "Epoch: 0392 train_loss= 0.39767 train_acc= 0.88415 test_loss= 0.66620 test_acc= 0.68750 time= 8.62113\n",
      "Epoch: 0393 train_loss= 0.39765 train_acc= 0.88415 test_loss= 0.66620 test_acc= 0.68750 time= 8.76057\n",
      "Epoch: 0394 train_loss= 0.39764 train_acc= 0.88415 test_loss= 0.66620 test_acc= 0.68750 time= 8.66002\n",
      "Epoch: 0395 train_loss= 0.39762 train_acc= 0.88415 test_loss= 0.66621 test_acc= 0.68750 time= 8.57378\n",
      "Epoch: 0396 train_loss= 0.39760 train_acc= 0.88415 test_loss= 0.66621 test_acc= 0.68750 time= 8.68815\n",
      "Epoch: 0397 train_loss= 0.39758 train_acc= 0.88415 test_loss= 0.66621 test_acc= 0.68750 time= 8.66512\n",
      "Epoch: 0398 train_loss= 0.39756 train_acc= 0.88415 test_loss= 0.66622 test_acc= 0.68750 time= 8.65130\n",
      "Epoch: 0399 train_loss= 0.39755 train_acc= 0.88415 test_loss= 0.66622 test_acc= 0.68750 time= 8.54852\n",
      "Epoch: 0400 train_loss= 0.39753 train_acc= 0.88415 test_loss= 0.66622 test_acc= 0.68750 time= 8.78051\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "cost_val = []\n",
    "for epoch in range(400):  #FLAGS.epochs\n",
    "    batches = range(round(train_num / batch_num))\n",
    "        \n",
    "    costs = np.zeros((len(batches), 2))\n",
    "    accs = np.zeros((len(batches), 2))\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    for ib in batches:\n",
    "        from_i = ib * batch_num\n",
    "        to_i = (ib+1) * batch_num\n",
    "            \n",
    "        features_train_batch = features_train[from_i:to_i]\n",
    "        support_train_batch = support_train[from_i:to_i]\n",
    "        y_train_batch = y_train[from_i:to_i]\n",
    "        feed_dict = construct_feed_dict(features_train_batch, support_train_batch, y_train_batch, placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "        feed_dict.update({placeholders['in_drop']: FLAGS.in_drop})\n",
    "\n",
    "        outs = sess.run([model.opt_op_explainer, model.loss_explainer, model.accuracy],feed_dict=feed_dict)\n",
    "\n",
    "        cost,acc,duration = evaluate(features_test, support_test, y_test, placeholders)\n",
    "        costs[ib] = [outs[1], cost]\n",
    "        accs[ib] = [outs[2], acc]\n",
    "            \n",
    "    costs = costs.mean(axis=0)\n",
    "    cost_train, cost_test = costs\n",
    "    cost_val.append(cost_test)\n",
    "        \n",
    "    accs = accs.mean(axis=0)\n",
    "    acc_train, acc_test = accs\n",
    "        \n",
    "        \n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cost_train),\n",
    "        \"train_acc=\", \"{:.5f}\".format(acc_train), \"test_loss=\", \"{:.5f}\".format(cost_test),\n",
    "        \"test_acc=\", \"{:.5f}\".format(acc_test), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "m_saver.save(sess, bestModelSavePath1)\n",
    "print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b542681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: cost= 0.66623 accuracy= 0.68750 time= 0.09470\n"
     ]
    }
   ],
   "source": [
    "test_cost, test_acc, test_duration = evaluate(features_test, support_test, y_test, placeholders)\n",
    "\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "    \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n",
    "l2.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2712d426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6761364]\n",
      "[0.6875]\n",
      "[0.6761363636363636, 0.6790123456790124, 0.6736842105263158, 0.6586826347305389, 0.7230669265756985]\n"
     ]
    }
   ],
   "source": [
    "M = model.M\n",
    "s_m = tf.sigmoid(M)\n",
    "M = M.eval(session=sess)\n",
    "s_m = s_m.eval(session=sess)\n",
    "\n",
    "# Custom save directory\n",
    "save_dir = \"/home/celery/Documents/Research/GNN-Research/weights\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Format file path using Python 3.5 compatible style\n",
    "save_path = os.path.join(save_dir, \"fold{}_mask.pkl\".format(k))\n",
    "\n",
    "# Save the sigmoid mask\n",
    "with open(save_path, 'wb+') as f:\n",
    "    pkl.dump(s_m, f)\n",
    "    \n",
    "sess.close()\n",
    "\n",
    "\n",
    "print(l1)\n",
    "print(l2)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

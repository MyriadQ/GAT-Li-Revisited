{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c24850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix, f1_score, balanced_accuracy_score, roc_curve, auc\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras import layers, initializers, optimizers, callbacks\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import ABCD_Parser as Reader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9309b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73146b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = {\n",
    "    'node_num': 110,          # Number of Graph nodes\n",
    "    'output_dim': 1,           # Number of output dimensions\n",
    "    'learning_rate': 0.0001,   # Initial learning rate for model\n",
    "    'learning_rate_mask': 0.01,# Learning rate for mask optimization\n",
    "    'batch_num': 10,           # Batch size (original 'batch_num' = batch size)\n",
    "    'epochs': 1000,            # Epochs for model training\n",
    "    'epochs_mask': 400,        # Epochs for mask optimization\n",
    "    'attn_heads': 5,           # Number of attention heads\n",
    "    'hidden1_gat': 24,         # GAT hidden layer 1 units\n",
    "    'output_gat': 3,           # GAT output layer units\n",
    "    'dropout': 0.0,            # Dropout rate (1 - keep prob)\n",
    "    'in_drop': 0.0,            # Input dropout rate\n",
    "    'weight_decay': 5e-4,      # L2 weight decay\n",
    "    'early_stopping': 15,      # Early stopping tolerance\n",
    "    'fold': 4                  # Target fold to train\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97e1c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    \"\"\"Accuracy metric (matches TF1 logic).\"\"\"\n",
    "    correct_prediction = tf.equal(tf.round(preds), labels)\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d87b9e",
   "metadata": {},
   "source": [
    "Subclassing using Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5899fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "class gat_layer(tf.keras.layers.Layer):\n",
    "  def __init__(self,input_dim, F_, attn_heads=1, attn_heads_reduction=\"concat\",\n",
    "               activation = tf.nn.relu, use_bias = True, dropout_rate = 0.0, in_drop = 0.0, name=''):\n",
    "\n",
    "    super().__init__(name = f'gat_layer{name}')\n",
    "\n",
    "    self.input_dim = input_dim\n",
    "    self.F_ = F_ #Output feature per node\n",
    "    self.attn_heads = attn_heads\n",
    "    self.attn_heads_reduction = attn_heads_reduction\n",
    "    self.activation = activation\n",
    "    self.use_bias = use_bias\n",
    "    self.dropout_rate = dropout_rate\n",
    "    self.in_drop = in_drop\n",
    "\n",
    "  def build(self, input_shape): #input_dim is the features\n",
    "    glorot_init = initializers.GlorotUniform(seed=seed)\n",
    "    zero_init = initializers.Zeros()\n",
    "\n",
    "    self.weights_list = [] # input_dim x F_\n",
    "    self.attn_self_weights = [] # F_ x 1\n",
    "    self.attn_neigh_weight = [] # F_ x 1\n",
    "\n",
    "    for head in range(self.attn_heads):\n",
    "      #Linear transformation\n",
    "\n",
    "      w = self.add_weight(\n",
    "        shape =(self.input_dim, self.F_),\n",
    "        initializer= glorot_init,\n",
    "        dtype = tf.float32,\n",
    "        name = f'weights_{head}',\n",
    "        trainable = True\n",
    "      )\n",
    "\n",
    "      #Weight for self and neighbors\n",
    "\n",
    "      attn_self = self.add_weight(\n",
    "        shape = (self.F_, 1),\n",
    "        initializer = glorot_init,\n",
    "        dtype = tf.float32,\n",
    "        name = f'attn_self_{head}',\n",
    "        trainable = True\n",
    "      )\n",
    "      attn_neigh = self.add_weight(\n",
    "        shape = (self.F_, 1),\n",
    "        initializer = glorot_init,\n",
    "        dtype = tf.float32,\n",
    "        name = f'attn_neigh_{head}',\n",
    "        trainable = True\n",
    "      )\n",
    "\n",
    "      self.weights_list.append(w)\n",
    "      self.attn_self_weights.append(attn_self)\n",
    "      self.attn_neigh_weight.append(attn_neigh)\n",
    "\n",
    "    if self.use_bias:\n",
    "      self.bias = self.add_weight(\n",
    "        shape = (self.F_,),\n",
    "        initializer = zero_init,\n",
    "        name = 'bias',\n",
    "        trainable = True\n",
    "      )\n",
    "    else:\n",
    "      self.bias = None\n",
    "\n",
    "    super().build(input_shape) # mark layer as built\n",
    "\n",
    "  def call(self, inputs, trainning = None):\n",
    "    \"\"\"Foward pass\"\"\"\n",
    "    X, A = inputs # X: (batch size, node_num, input_dim(feature)), A: (batch_size, node_num, node_num)\n",
    "\n",
    "    if self.in_drop > 0.0 and trainning:\n",
    "      X = tf.nn.dropout(X, rate = self.in_drop)\n",
    "\n",
    "    outputs = [] # store output of each attention head\n",
    "    dense_masks = [] # store attention score masks\n",
    "\n",
    "    for head in range(self.attn_heads):\n",
    "      # Linear transformation\n",
    "      kernel = self.weights_list[head]\n",
    "      features = tf.matmul(X, kernel) # (batch_size, node_num, F_) F_ is a hyperpara -> choose later\n",
    "\n",
    "      # Compute attention scores\n",
    "      attn_self_kernel = self.attn_self_weights[head]\n",
    "      attn_neigh_kernel = self.attn_neigh_weight[head]\n",
    "      attn_for_self = tf.matmul(features, attn_self_kernel) # (batch_size, node_num, 1)\n",
    "      attn_for_neigh = tf.matmul(features, attn_neigh_kernel) # (batch_size, node_num, 1)\n",
    "\n",
    "      # Compute attention head (a(Wh_i, Wh_j))\n",
    "      dense = attn_for_self + tf.transpose(attn_for_neigh, [0,2,1]) # batch_size, node_num, node_num\n",
    "      #print(\"plus:\", dense.shape)\n",
    "      dense = tf.nn.leaky_relu(dense, alpha = 0.2) # non-linearity\n",
    "\n",
    "      # Mask non-edges using adj matrix\n",
    "      zero_vec = -9e15 * tf.ones_like(dense)\n",
    "      dense = tf.where(A > 0.0, dense, zero_vec)\n",
    "      dense_masks.append(dense)\n",
    "\n",
    "      # Softmax to get attn coef\n",
    "      dense = tf.nn.softmax(dense)\n",
    "\n",
    "      # Drop out on attention coef and features\n",
    "      if trainning and self.dropout_rate > 0.0:\n",
    "        dropout_attn = tf.nn.dropout(dense, rate = self.dropout_rate)\n",
    "        dropout_feat = tf.nn.dropout(features, rate = self.dropout_rate)\n",
    "      else:\n",
    "        dropout_attn = dense\n",
    "        dropout_feat = features\n",
    "\n",
    "      # Aggregate neighbor features\n",
    "      node_features = tf.matmul(dropout_attn, dropout_feat) # batch_size, node_num, F_\n",
    "\n",
    "      # add bias if need\n",
    "\n",
    "      if self.use_bias:\n",
    "        node_features += self.bias\n",
    "\n",
    "      # storing head output\n",
    "      if self.attn_heads_reduction == 'concat':\n",
    "        outputs.append(self.activation(node_features))\n",
    "      else:\n",
    "        outputs.append(node_features)\n",
    "    # Aggregate attention heads\n",
    "    if self.attn_heads_reduction == 'concat':\n",
    "      output = tf.concat(outputs, axis = -1)\n",
    "    else:\n",
    "      output = tf.add_n(outputs) / self.attn_heads # avg heads\n",
    "      output = self.activation(output)\n",
    "\n",
    "    return output, dense_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb076e7",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0243736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features shape: (2, 4, 5)\n",
      "Adjacency matrix shape: (2, 4, 4)\n",
      "\n",
      "GAT output shape: (2, 4, 16)\n",
      "Attention scores length (number of heads): 2\n",
      "Attention scores shape for first head: (2, 4, 4)\n",
      "\n",
      "Adjacency matrix (first sample):\n",
      "[[0. 1. 0. 0.]\n",
      " [1. 0. 1. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 1. 0.]]\n",
      "\n",
      "Attention scores (first sample, first head):\n",
      "[[-8.9999998e+15 -3.7271079e-01 -8.9999998e+15 -8.9999998e+15]\n",
      " [-3.7271079e-01 -8.9999998e+15  1.9667579e+00  2.0883536e+00]\n",
      " [-8.9999998e+15 -8.9999998e+15  4.1073794e+00 -8.9999998e+15]\n",
      " [ 3.9866316e-01 -8.9999998e+15  4.2289753e+00 -8.9999998e+15]]\n",
      "\n",
      "Non-edges are masked: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Create dummy data\n",
    "batch_size = 2\n",
    "num_nodes = 4\n",
    "input_dim = 5  # Features per node\n",
    "\n",
    "# Random node features\n",
    "X = tf.random.normal(shape=(batch_size, num_nodes, input_dim))\n",
    "\n",
    "# Random adjacency matrix (binary)\n",
    "A = tf.cast(tf.random.uniform(shape=(batch_size, num_nodes, num_nodes)) > 0.5, tf.float32)\n",
    "\n",
    "print(\"Input features shape:\", X.shape)  # (2, 4, 5)\n",
    "print(\"Adjacency matrix shape:\", A.shape)  # (2, 4, 4)\n",
    "\n",
    "# 2. Create a GAT layer\n",
    "gat = gat_layer(\n",
    "    input_dim=input_dim,\n",
    "    F_=8,                # Output features per node per head\n",
    "    attn_heads=2,        # Number of attention heads\n",
    "    attn_heads_reduction='concat',  # or 'average'\n",
    "    activation=tf.nn.leaky_relu,\n",
    "    use_bias=True,\n",
    "    dropout_rate=0.0,\n",
    "    in_drop=0.0,\n",
    "    name='test'\n",
    ")\n",
    "\n",
    "# 3. Forward pass\n",
    "output, attn_scores = gat([X, A], training=False)\n",
    "\n",
    "# 4. Check output shapes\n",
    "print(\"\\nGAT output shape:\", output.shape)\n",
    "# With concat: (batch_size, num_nodes, F_ * attn_heads) = (2, 4, 16)\n",
    "# With average: (batch_size, num_nodes, F_) = (2, 4, 8)\n",
    "\n",
    "print(\"Attention scores length (number of heads):\", len(attn_scores))\n",
    "print(\"Attention scores shape for first head:\", attn_scores[0].shape)  # (2, 4, 4)\n",
    "\n",
    "\n",
    "# Take first sample in batch, first attention head\n",
    "attn_head0 = attn_scores[0][0].numpy()\n",
    "A_sample = A[0].numpy()\n",
    "\n",
    "print(\"\\nAdjacency matrix (first sample):\")\n",
    "print(A_sample)\n",
    "\n",
    "print(\"\\nAttention scores (first sample, first head):\")\n",
    "print(attn_head0)\n",
    "\n",
    "# Check that non-edges are masked to -9e15\n",
    "print(\"\\nNon-edges are masked:\", np.all(attn_head0[A_sample == 0] == -9e15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cf7ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gat_layertest       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>),   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">gat_layer</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>),     │            │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)]     │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m5\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gat_layertest       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m16\u001b[0m),   │        \u001b[38;5;34m120\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mgat_layer\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m),     │            │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m)]     │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs = [tf.keras.Input(shape=(num_nodes, input_dim)),\n",
    "          tf.keras.Input(shape=(num_nodes, num_nodes))]\n",
    "x, _ = gat(inputs)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1cc8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_layer(tf.keras.layers.Layer): # Fully dense layer in paper\n",
    "  def __init__(self, input_dim, output_dim, dropout=0.0, act = tf.nn.relu, bias = False, name = ''):\n",
    "    super().__init__(name = f'fc_layer{name}')\n",
    "\n",
    "    # Hyperparameters\n",
    "    self.input_dim = input_dim\n",
    "    self.output_dim = output_dim\n",
    "    self.dropout = dropout\n",
    "    self.act = act\n",
    "    self.bias = bias\n",
    "\n",
    "  def build(self, input_shape):\n",
    "\n",
    "    glorot_init = initializers.GlorotUniform(seed = seed)\n",
    "    zero_init = initializers.Zeros()\n",
    "\n",
    "    #Linear weigh\n",
    "    self.weights = self.add_weight(\n",
    "      shape = (self.input_dim, self.output_dim),\n",
    "      initializer = glorot_init,\n",
    "      dtype = tf.float32,\n",
    "      name = 'weights',\n",
    "      trainable = True\n",
    "    )\n",
    "\n",
    "\n",
    "    if self.bias:\n",
    "      self.bias_term = self.add_weight(\n",
    "        shape = (self.output_dim,),\n",
    "        initializer = zero_init,\n",
    "        dtype = tf.float32,\n",
    "        name = 'bias',\n",
    "        trainable = True\n",
    "\n",
    "      )\n",
    "    else:\n",
    "      self.bias_term = None\n",
    "\n",
    "    super().build(input_shape)\n",
    "\n",
    "  def call(self, inputs, trainning = None): #foward pass\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    if self.dropout > 0.0 and trainning:\n",
    "      x = tf.nn.dropout(x, rate = self.dropout)\n",
    "\n",
    "    # Linear transformation\n",
    "    output = tf.matmul(x, self.weights)\n",
    "\n",
    "    if self.bias:\n",
    "      output += self.bias_term\n",
    "\n",
    "    return self.act(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d12b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT_Model(Model):\n",
    "  def __init__(self, input_dim, flags):\n",
    "    super().__init__(name = 'gat_mil')\n",
    "\n",
    "    self.flags = flags\n",
    "    self.input_dim = input_dim #input feature dimension (from data)\n",
    "\n",
    "    self._build_layers() #this get called after defining because python read all class function before calling init\n",
    "\n",
    "    #Init mask M using add_weight\n",
    "    self.M = self.add_weight(\n",
    "      shape=(self.config)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAT_Newest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c30a69b4",
   "metadata": {},
   "source": [
    "Test ABCD_Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15b9b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'50004'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 688\u001b[39m\n\u001b[32m    686\u001b[39m \u001b[38;5;66;03m# Run main\u001b[39;00m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m688\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 502\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    500\u001b[39m subject_IDs = np.genfromtxt(\u001b[33m'\u001b[39m\u001b[33m/Users/celery/Research/GAT-Li-Revisited/IDs/valid_subject_ids.txt\u001b[39m\u001b[33m'\u001b[39m, dtype=\u001b[38;5;28mstr\u001b[39m).tolist()\n\u001b[32m    501\u001b[39m label_dict = Reader.get_label(subject_IDs)  \u001b[38;5;66;03m# Assume Reader is compatible with TF2\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m label_list = np.array([\u001b[38;5;28mint\u001b[39m(\u001b[43mlabel_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m subject_IDs])\n\u001b[32m    504\u001b[39m \u001b[38;5;66;03m# Load features (connectivity) and adjacency (absolute connectivity)\u001b[39;00m\n\u001b[32m    505\u001b[39m X, Y = getconn_vector(subject_IDs, \u001b[33m\"\u001b[39m\u001b[33mcorrelation\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mho\u001b[39m\u001b[33m\"\u001b[39m, label_dict)\n",
      "\u001b[31mKeyError\u001b[39m: '50004'"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# Step 1: Replace TF1 imports & random seed with TF2 equivalents\n",
    "# ------------------------------------------------------\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_curve, auc, balanced_accuracy_score\n",
    "import scipy.io as sio\n",
    "import pickle as pkl\n",
    "import copy\n",
    "import scipy.spatial.distance\n",
    "from tqdm import tqdm\n",
    "# Use Keras directly from TF2 (no separate keras import)\n",
    "from tensorflow.keras import layers, Model, optimizers, initializers, metrics\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import scipy.sparse as sp\n",
    "import ABIDE_Parser as Reader\n",
    "\n",
    "# Set random seed (TF2 equivalent of tf.set_random_seed)\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 2: Remove tf.app.flags; use config dict for hyperparameters\n",
    "# ------------------------------------------------------\n",
    "# Replace TF1 flags with a config dictionary (easier to manage in TF2)\n",
    "config = {\n",
    "    'node_num': 110,          # Number of Graph nodes\n",
    "    'output_dim': 1,           # Number of output dimensions\n",
    "    'learning_rate': 0.0001,   # Initial learning rate for model\n",
    "    'learning_rate_mask': 0.01,# Learning rate for mask optimization\n",
    "    'batch_num': 10,           # Batch size (original 'batch_num' = batch size)\n",
    "    'epochs': 1000,            # Epochs for model training\n",
    "    'epochs_mask': 400,        # Epochs for mask optimization\n",
    "    'attn_heads': 5,           # Number of attention heads\n",
    "    'hidden1_gat': 24,         # GAT hidden layer 1 units\n",
    "    'output_gat': 3,           # GAT output layer units\n",
    "    'dropout': 0.0,            # Dropout rate (1 - keep prob)\n",
    "    'in_drop': 0.0,            # Input dropout rate\n",
    "    'weight_decay': 5e-4,      # L2 weight decay\n",
    "    'early_stopping': 15,      # Early stopping tolerance\n",
    "    'fold': 4                  # Target fold to train\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 3: Replace custom utils with TF2/Keras built-ins\n",
    "# ------------------------------------------------------\n",
    "def accuracy(preds, labels):\n",
    "    \"\"\"TF2-compatible accuracy function (matches original logic)\"\"\"\n",
    "    correct_prediction = tf.equal(tf.round(preds), labels)\n",
    "    return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 4: Convert TF1 gat_layer to TF2 Keras Layer subclass\n",
    "# ------------------------------------------------------\n",
    "class GATLayer(layers.Layer):\n",
    "    def __init__(self, input_dim, F_, attn_heads=1, attn_heads_reduction='concat',\n",
    "                 activation=tf.nn.relu, use_bias=True, dropout_rate=0.0, in_drop=0.0, name=''):\n",
    "        super().__init__(name=f'gat_layer{name}')  # Call parent Layer __init__\n",
    "\n",
    "        # Store hyperparameters\n",
    "        self.input_dim = input_dim          # Input feature dimension per node\n",
    "        self.F_ = F_                        # Output feature dimension per node\n",
    "        self.attn_heads = attn_heads        # Number of attention heads\n",
    "        self.attn_heads_reduction = attn_heads_reduction  # Head aggregation method\n",
    "        self.activation = activation        # Activation function\n",
    "        self.use_bias = use_bias            # Whether to use bias\n",
    "        self.dropout_rate = dropout_rate    # Attention dropout rate\n",
    "        self.in_drop = in_drop              # Input feature dropout rate\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build trainable weights (TF2 lazy initialization)\"\"\"\n",
    "        # Use Keras built-in GlorotUniform (replaces custom glorot function)\n",
    "        glorot_init = initializers.GlorotUniform(seed=seed)\n",
    "        zero_init = initializers.Zeros()\n",
    "\n",
    "        # Initialize weights for each attention head\n",
    "        self.weights_list = []              # Linear transformation weights: (input_dim, F_)\n",
    "        self.attn_self_weights = []         # Self-attention weights: (F_, 1)\n",
    "        self.attn_neigh_weights = []        # Neighbor attention weights: (F_, 1)\n",
    "\n",
    "        for i in range(self.attn_heads):\n",
    "            # Linear kernel (replaces custom glorot([input_dim, F_]))\n",
    "            w = self.add_weight(\n",
    "                shape=(self.input_dim, self.F_),\n",
    "                initializer=glorot_init,\n",
    "                dtype=tf.float32,\n",
    "                name=f'weights_{i}',\n",
    "                trainable=True\n",
    "            )\n",
    "            # Self-attention weight (replaces custom glorot([F_, 1]))\n",
    "            attn_self = self.add_weight(\n",
    "                shape=(self.F_, 1),\n",
    "                initializer=glorot_init,\n",
    "                dtype=tf.float32,\n",
    "                name=f'attn_self_weights_{i}',\n",
    "                trainable=True\n",
    "            )\n",
    "            # Neighbor attention weight (replaces custom glorot([F_, 1]))\n",
    "            attn_neigh = self.add_weight(\n",
    "                shape=(self.F_, 1),\n",
    "                initializer=glorot_init,\n",
    "                dtype=tf.float32,\n",
    "                name=f'attn_neighs_weights_{i}',\n",
    "                trainable=True\n",
    "            )\n",
    "\n",
    "            self.weights_list.append(w)\n",
    "            self.attn_self_weights.append(attn_self)\n",
    "            self.attn_neigh_weights.append(attn_neigh)\n",
    "\n",
    "        # Bias term (replaces custom zeros([F_]))\n",
    "        if self.use_bias:\n",
    "            self.bias = self.add_weight(\n",
    "                shape=(self.F_,),\n",
    "                initializer=zero_init,\n",
    "                dtype=tf.float32,\n",
    "                name='bias',\n",
    "                trainable=True\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # Mark layer as built (required for Keras)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"Forward pass (TF2 call method replaces __call__ in TF1)\"\"\"\n",
    "        # Unpack inputs: (node_features, adjacency_matrix)\n",
    "        X, A = inputs  # X: (batch_size, node_num, input_dim); A: (batch_size, node_num, node_num)\n",
    "\n",
    "        # Input dropout (only apply during training)\n",
    "        if self.in_drop > 0.0 and training:\n",
    "            X = tf.nn.dropout(X, rate=self.in_drop)\n",
    "\n",
    "        outputs = []    # Store output of each attention head\n",
    "        dense_masks = []# Store attention score masks (for debugging/analysis)\n",
    "\n",
    "        for head in range(self.attn_heads):\n",
    "            # Step 1: Linear transformation of node features\n",
    "            kernel = self.weights_list[head]\n",
    "            # TF2 tf.matmul replaces TF1 tf.tensordot (equivalent for 3D inputs)\n",
    "            features = tf.matmul(X, kernel)  # (batch_size, node_num, F_)\n",
    "\n",
    "            # Step 2: Compute attention scores\n",
    "            attn_self_kernel = self.attn_self_weights[head]\n",
    "            attn_neigh_kernel = self.attn_neigh_weights[head]\n",
    "            attn_for_self = tf.matmul(features, attn_self_kernel)  # (batch_size, node_num, 1)\n",
    "            attn_for_neigh = tf.matmul(features, attn_neigh_kernel)# (batch_size, node_num, 1)\n",
    "\n",
    "            # Step 3: Attention head calculation (a(Wh_i, Wh_j))\n",
    "            # Transpose to enable broadcasting: (batch_size, 1, node_num)\n",
    "            dense = attn_for_self + tf.transpose(attn_for_neigh, [0, 2, 1])  # (batch_size, node_num, node_num)\n",
    "            dense = tf.nn.leaky_relu(dense, alpha=0.2)  # Apply non-linearity\n",
    "\n",
    "            # Step 4: Mask non-edges (replace TF1 placeholder self.A with input A)\n",
    "            zero_vec = -9e15 * tf.ones_like(dense)\n",
    "            dense = tf.where(A > 0.0, dense, zero_vec)\n",
    "            dense_masks.append(dense)\n",
    "\n",
    "            # Step 5: Softmax to get attention coefficients\n",
    "            dense = tf.nn.softmax(dense)  # (batch_size, node_num, node_num)\n",
    "\n",
    "            # Step 6: Dropout on attention coefficients and features (training only)\n",
    "            if training and self.dropout_rate > 0.0:\n",
    "                dropout_attn = tf.nn.dropout(dense, rate=self.dropout_rate)\n",
    "                dropout_feat = tf.nn.dropout(features, rate=self.dropout_rate)\n",
    "            else:\n",
    "                dropout_attn = dense\n",
    "                dropout_feat = features\n",
    "\n",
    "            # Step 7: Aggregate neighbor features\n",
    "            node_features = tf.matmul(dropout_attn, dropout_feat)  # (batch_size, node_num, F_)\n",
    "\n",
    "            # Step 8: Add bias (if enabled)\n",
    "            if self.use_bias:\n",
    "                node_features += self.bias\n",
    "\n",
    "            # Step 9: Store head output\n",
    "            if self.attn_heads_reduction == 'concat':\n",
    "                outputs.append(self.activation(node_features))\n",
    "            else:\n",
    "                outputs.append(node_features)\n",
    "\n",
    "        # Step 10: Aggregate attention heads\n",
    "        if self.attn_heads_reduction == 'concat':\n",
    "            output = tf.concat(outputs, axis=-1)  # (batch_size, node_num, F_ * attn_heads)\n",
    "        else:\n",
    "            output = tf.add_n(outputs) / self.attn_heads  # Average heads\n",
    "            output = self.activation(output)\n",
    "\n",
    "        return output, dense_masks\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 5: Convert TF1 fc_layer to TF2 Keras Layer subclass\n",
    "# ------------------------------------------------------\n",
    "class FCLayer(layers.Layer):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.0, act=tf.nn.relu, bias=False, name=''):\n",
    "        super().__init__(name=f'fc_layer{name}')  # Call parent Layer __init__\n",
    "\n",
    "        # Store hyperparameters\n",
    "        self.input_dim = input_dim    # Input dimension\n",
    "        self.output_dim = output_dim  # Output dimension\n",
    "        self.dropout = dropout        # Dropout rate\n",
    "        self.act = act                # Activation function\n",
    "        self.bias = bias              # Whether to use bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build trainable weights (TF2 built-in initializers)\"\"\"\n",
    "        glorot_init = initializers.GlorotUniform(seed=seed)\n",
    "        zero_init = initializers.Zeros()\n",
    "\n",
    "        # Linear weight (replaces custom glorot([input_dim, output_dim]))\n",
    "        self.weights = self.add_weight(\n",
    "            shape=(self.input_dim, self.output_dim),\n",
    "            initializer=glorot_init,\n",
    "            dtype=tf.float32,\n",
    "            name='weights',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        # Bias term (replaces custom zeros([output_dim]))\n",
    "        if self.bias:\n",
    "            self.bias_term = self.add_weight(\n",
    "                shape=(self.output_dim,),\n",
    "                initializer=zero_init,\n",
    "                dtype=tf.float32,\n",
    "                name='bias',\n",
    "                trainable=True\n",
    "            )\n",
    "        else:\n",
    "            self.bias_term = None\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"Forward pass for fully connected layer\"\"\"\n",
    "        x = inputs\n",
    "\n",
    "        # Dropout (training only)\n",
    "        if self.dropout > 0.0 and training:\n",
    "            x = tf.nn.dropout(x, rate=self.dropout)\n",
    "\n",
    "        # Linear transformation (replace TF1 tf.tensordot)\n",
    "        output = tf.matmul(x, self.weights)\n",
    "\n",
    "        # Add bias (if enabled)\n",
    "        if self.bias:\n",
    "            output += self.bias_term\n",
    "\n",
    "        # Apply activation\n",
    "        return self.act(output)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 6: Convert TF1 Model to TF2 Keras Model subclass\n",
    "# ------------------------------------------------------\n",
    "class GATMILModel(Model):\n",
    "    def __init__(self, input_dim, config):\n",
    "        super().__init__(name='gat_mil')  # Call parent Model __init__\n",
    "\n",
    "        # Store config and input dimension\n",
    "        self.config = config\n",
    "        self.input_dim = input_dim  # Input feature dimension (from data)\n",
    "\n",
    "        # Initialize layers (replaces TF1 _build method)\n",
    "        self._build_layers()\n",
    "\n",
    "        # Initialize mask M (replaces TF1 tens function)\n",
    "        # Use add_weight to register as trainable variable\n",
    "        self.M = self.add_weight(\n",
    "            shape=(self.config['node_num'], self.config['node_num']),\n",
    "            initializer=initializers.Constant(value=10.0),  # TF1: tf.constant(10, ...)\n",
    "            dtype=tf.float32,\n",
    "            name='mask',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        # Initialize optimizers (TF2 replaces tf.train.AdamOptimizer)\n",
    "        self.optimizer = optimizers.Adam(learning_rate=self.config['learning_rate'])\n",
    "        self.optimizer_mask = optimizers.Adam(learning_rate=self.config['learning_rate_mask'])\n",
    "\n",
    "        # Metrics (for tracking during training)\n",
    "        self.train_loss_metric = metrics.Mean(name='train_loss')\n",
    "        self.train_acc_metric = metrics.Mean(name='train_acc')\n",
    "        self.val_loss_metric = metrics.Mean(name='val_loss')\n",
    "        self.val_acc_metric = metrics.Mean(name='val_acc')\n",
    "\n",
    "    def _build_layers(self):\n",
    "        \"\"\"Initialize GAT and FC layers (matches TF1 _build)\"\"\"\n",
    "        # GAT Layer 1: concat heads\n",
    "        self.gat1 = GATLayer(\n",
    "            input_dim=self.input_dim,\n",
    "            F_=self.config['hidden1_gat'],\n",
    "            attn_heads=self.config['attn_heads'],\n",
    "            attn_heads_reduction='concat',\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            use_bias=True,\n",
    "            dropout_rate=self.config['dropout'],\n",
    "            in_drop=self.config['in_drop'],\n",
    "            name='1'\n",
    "        )\n",
    "\n",
    "        # GAT Layer 2: average heads (input_dim = hidden1_gat * attn_heads)\n",
    "        self.gat2 = GATLayer(\n",
    "            input_dim=self.config['hidden1_gat'] * self.config['attn_heads'],\n",
    "            F_=self.config['output_gat'],\n",
    "            attn_heads=3,\n",
    "            attn_heads_reduction='average',\n",
    "            activation=tf.nn.leaky_relu,\n",
    "            use_bias=True,\n",
    "            dropout_rate=self.config['dropout'],\n",
    "            in_drop=self.config['in_drop'],\n",
    "            name='2'\n",
    "        )\n",
    "\n",
    "        # FC Layer 1: node-level probability (sigmoid)\n",
    "        self.fc1 = FCLayer(\n",
    "            input_dim=self.config['output_gat'],\n",
    "            output_dim=self.config['output_dim'],\n",
    "            dropout=self.config['dropout'],\n",
    "            act=tf.nn.sigmoid,\n",
    "            bias=False,\n",
    "            name='1'\n",
    "        )\n",
    "\n",
    "        # FC Layer 2: MIL attention (softmax)\n",
    "        self.fc2 = FCLayer(\n",
    "            input_dim=self.config['node_num'],\n",
    "            output_dim=self.config['node_num'],\n",
    "            dropout=self.config['dropout'],\n",
    "            act=tf.nn.softmax,\n",
    "            bias=False,\n",
    "            name='2'\n",
    "        )\n",
    "\n",
    "        # Store layers for easy access\n",
    "        self.gat_layers = [self.gat1, self.gat2]\n",
    "        self.fc_layers = [self.fc1, self.fc2]\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"Forward pass (matches TF1 build method)\"\"\"\n",
    "        # Unpack inputs: (node_features, adjacency_matrix)\n",
    "        X, A = inputs\n",
    "\n",
    "        # Step 1: Apply mask M (sigmoid to get [0,1] values)\n",
    "        sigmoid_M = tf.sigmoid(self.M)\n",
    "        # Broadcast mask to batch dimension: (batch_size, node_num, node_num)\n",
    "        X = tf.multiply(X, sigmoid_M[tf.newaxis, ...])\n",
    "\n",
    "        # Step 2: Pass through GAT layers\n",
    "        gcn_activations = [X]  # Track activations (matches TF1 self.gcn_activations)\n",
    "        dense_masks = []        # Track attention masks\n",
    "\n",
    "        for layer in self.gat_layers:\n",
    "            hidden, dense_mask = layer([gcn_activations[-1], A], training=training)\n",
    "            gcn_activations.append(hidden)\n",
    "            dense_masks.append(dense_mask)\n",
    "\n",
    "        # Step 3: Pass through FC layers (MIL logic)\n",
    "        # Node-level probability\n",
    "        node_prob = self.fc1(gcn_activations[-1], training=training)  # (batch_size, node_num, 1)\n",
    "        # Reshape for MIL attention: (batch_size, node_num)\n",
    "        tensor = tf.reshape(node_prob, shape=(-1, self.config['node_num']))\n",
    "        # MIL attention weights\n",
    "        attention_prob = self.fc2(tensor, training=training)  # (batch_size, node_num)\n",
    "\n",
    "        # Step 4: MIL pooling (weighted sum)\n",
    "        attention_mul = tf.multiply(tensor, attention_prob)  # (batch_size, node_num)\n",
    "        outputs = tf.reduce_sum(attention_mul, axis=1, keepdims=True)  # (batch_size, 1)\n",
    "\n",
    "        # Store masks for analysis (optional)\n",
    "        self.dense_masks = dense_masks\n",
    "        return outputs\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute total loss (matches TF1 _loss method)\"\"\"\n",
    "        # 1. L2 weight decay (only on GAT1 weights, same as TF1)\n",
    "        l2_loss = 0.0\n",
    "        for weight in self.gat1.trainable_weights:\n",
    "            l2_loss += self.config['weight_decay'] * tf.nn.l2_loss(weight)\n",
    "\n",
    "        # 2. Log loss (matches TF1 tf.losses.log_loss)\n",
    "        # TF2: binary_crossentropy with from_logits=False (since y_pred is sigmoid output)\n",
    "        log_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true, y_pred))\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = log_loss + l2_loss\n",
    "        return total_loss, log_loss, l2_loss\n",
    "\n",
    "    def train_step(self, data, training_mask=False):\n",
    "        \"\"\"Single training step (uses TF2 GradientTape)\"\"\"\n",
    "        X_batch, A_batch, y_batch = data\n",
    "\n",
    "        # Use GradientTape to track gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            y_pred = self([X_batch, A_batch], training=True)\n",
    "            # Compute loss\n",
    "            total_loss, _, _ = self.compute_loss(y_batch, y_pred)\n",
    "            # Compute accuracy\n",
    "            acc = accuracy(y_pred, y_batch)\n",
    "\n",
    "        # Determine which variables to update\n",
    "        if training_mask:\n",
    "            # Only update mask M (mask optimization phase)\n",
    "            trainable_vars = [self.M]\n",
    "        else:\n",
    "            # Update all variables except M (model training phase)\n",
    "            trainable_vars = [var for var in self.trainable_variables if var.name != 'mask:0']\n",
    "\n",
    "        # Compute gradients and apply updates\n",
    "        gradients = tape.gradient(total_loss, trainable_vars)\n",
    "        if training_mask:\n",
    "            self.optimizer_mask.apply_gradients(zip(gradients, trainable_vars))\n",
    "        else:\n",
    "            self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update metrics\n",
    "        self.train_loss_metric.update_state(total_loss)\n",
    "        self.train_acc_metric.update_state(acc)\n",
    "\n",
    "        return {'loss': self.train_loss_metric.result(), 'acc': self.train_acc_metric.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        \"\"\"Single validation/test step\"\"\"\n",
    "        X_batch, A_batch, y_batch = data\n",
    "\n",
    "        # Forward pass (training=False disables dropout)\n",
    "        y_pred = self([X_batch, A_batch], training=False)\n",
    "        # Compute loss and accuracy\n",
    "        total_loss, _, _ = self.compute_loss(y_batch, y_pred)\n",
    "        acc = accuracy(y_pred, y_batch)\n",
    "\n",
    "        # Update metrics\n",
    "        self.val_loss_metric.update_state(total_loss)\n",
    "        self.val_acc_metric.update_state(acc)\n",
    "\n",
    "        return {'loss': self.val_loss_metric.result(), 'acc': self.val_acc_metric.result()}\n",
    "\n",
    "    def reset_metrics(self):\n",
    "        \"\"\"Reset training/validation metrics\"\"\"\n",
    "        self.train_loss_metric.reset_states()\n",
    "        self.train_acc_metric.reset_states()\n",
    "        self.val_loss_metric.reset_states()\n",
    "        self.val_acc_metric.reset_states()\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 7: Data loading functions (keep same logic, TF2-compatible)\n",
    "# ------------------------------------------------------\n",
    "def load_connectivity(subject_list, kind, atlas_name='ho', data_folder='/Users/celery/Research/dataset/ABIDE/Outputs/cpac/filt_global/mat'):#home/celery/Data/ABIDE/ABIDE/Outputs/cpac/filt_global/mat\n",
    "    \"\"\"Load connectivity matrices (same as original)\"\"\"\n",
    "    all_networks = []\n",
    "    for subject in subject_list:\n",
    "        fl = os.path.join(data_folder, f\"{subject}_{atlas_name}_{kind}.mat\")\n",
    "        matrix = sio.loadmat(fl)['connectivity']\n",
    "        # Remove 83rd node (atlas-specific)\n",
    "        if atlas_name == 'ho':\n",
    "            matrix = np.delete(matrix, 82, axis=0)\n",
    "            matrix = np.delete(matrix, 82, axis=1)\n",
    "        all_networks.append(matrix)\n",
    "    return np.array(all_networks, dtype=np.float32)  # Use float32 for TF2\n",
    "\n",
    "def getconn_vector(subject_name0, kind, atlas, label_dict):\n",
    "    \"\"\"Get features (connectivity) and labels (same as original)\"\"\"\n",
    "    subject_name = np.array(subject_name0)\n",
    "    # Load connectivity features\n",
    "    conn_array = load_connectivity(subject_name, kind, atlas)\n",
    "    data_x = np.array(conn_array, dtype=np.float32)\n",
    "    # Load labels\n",
    "    data_y = np.array([[int(label_dict[subname])] for subname in subject_name], dtype=np.float32)\n",
    "    return data_x, data_y\n",
    "\n",
    "def shuffle(adjs, features, y):\n",
    "    \"\"\"Shuffle data (same as original)\"\"\"\n",
    "    shuffle_ix = np.random.permutation(np.arange(len(y)))\n",
    "    return adjs[shuffle_ix], features[shuffle_ix], y[shuffle_ix]\n",
    "\n",
    "def create_batches(X, A, y, batch_size):\n",
    "    \"\"\"Create batches for training (replaces original batch loop)\"\"\"\n",
    "    num_samples = len(X)\n",
    "    batches = []\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        end = min(i + batch_size, num_samples)\n",
    "        batches.append((X[i:end], A[i:end], y[i:end]))\n",
    "    return batches\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Step 8: Main training pipeline (TF2 custom loop)\n",
    "# ------------------------------------------------------\n",
    "def main():\n",
    "    # --------------------------\n",
    "    # 8.1 Load data\n",
    "    # --------------------------\n",
    "    # Subject IDs and labels\n",
    "    subject_IDs = np.genfromtxt('/Users/celery/Research/GAT-Li-Revisited/IDs/valid_subject_ids.txt', dtype=str).tolist()\n",
    "    label_dict = Reader.get_label(subject_IDs)  # Assume Reader is compatible with TF2\n",
    "    label_list = np.array([int(label_dict[x]) for x in subject_IDs])\n",
    "    # Add this after line 501\n",
    "\n",
    "\n",
    "    # Load features (connectivity) and adjacency (absolute connectivity)\n",
    "    X, Y = getconn_vector(subject_IDs, \"correlation\", \"ho\", label_dict)\n",
    "    adjs = np.abs(X)  # Adjacency matrix = absolute correlation\n",
    "\n",
    "    # --------------------------\n",
    "    # 8.2 K-fold setup (only run target fold)\n",
    "    # --------------------------\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    # Get train/test indices for target fold (config['fold'] = 4)\n",
    "    for fold_idx, (train_index, test_index) in enumerate(skf.split(subject_IDs, label_list)):\n",
    "        if fold_idx != config['fold']:\n",
    "            continue\n",
    "\n",
    "        # Split data into train/test\n",
    "        features_train, features_test = X[train_index], X[test_index]\n",
    "        support_train, support_test = adjs[train_index], adjs[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # Shuffle training data\n",
    "        support_train, features_train, y_train = shuffle(support_train, features_train, y_train)\n",
    "        support_test, features_test, y_test = shuffle(support_test, features_test, y_test)\n",
    "\n",
    "        print(f\"Fold {fold_idx} - Train shape: {features_train.shape}, Test shape: {features_test.shape}\")\n",
    "        print(f\"Y test sample: {y_test[:3]}\")\n",
    "\n",
    "        # --------------------------\n",
    "        # 8.3 Initialize model\n",
    "        # --------------------------\n",
    "        # Input dimension = number of features per node (last dim of features)\n",
    "        input_dim = features_train.shape[-1]\n",
    "        model = GATMILModel(input_dim=input_dim, config=config)\n",
    "\n",
    "        # Model save paths (matches original)\n",
    "        bestModelSavePath0 = '/Users/celery/Research/GAT-Li-Revisited/Model_save/fold_e_mask%s/gat_e%s_weights_best.ckpt' % (str(fold_idx), str(fold_idx)) #/home/celery/Data/ABIDE/ABIDE/Model_save/fold_e_mask%s/gat_e%s_weights_best.ckpt\n",
    "        bestModelSavePath1 = '/Users/celery/Research/GAT-Li-Revisited/Model_save/fold_e_mask%s/gat_e%s_weights_best.ckpt' % (str(fold_idx), str(fold_idx))\n",
    "        # Create directory if not exists\n",
    "        os.makedirs(os.path.dirname(bestModelSavePath0), exist_ok=True)\n",
    "\n",
    "        # --------------------------\n",
    "        # 8.4 Phase 1: Train model (exclude mask M)\n",
    "        # --------------------------\n",
    "        print(\"\\n=== Phase 1: Train Model (Exclude Mask) ===\")\n",
    "        batch_size = config['batch_num']\n",
    "        train_batches = create_batches(features_train, support_train, y_train, batch_size)\n",
    "        val_batches = create_batches(features_test, support_test, y_test, batch_size)\n",
    "\n",
    "        cost_val = []  # Track validation loss for early stopping\n",
    "        best_val_loss = float('inf')\n",
    "\n",
    "        for epoch in range(config['epochs']):\n",
    "            # Reset metrics\n",
    "            model.reset_metrics()\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Train on all batches\n",
    "            for batch in train_batches:\n",
    "                model.train_step(batch, training_mask=False)\n",
    "\n",
    "            # Validate on all batches\n",
    "            for batch in val_batches:\n",
    "                model.test_step(batch)\n",
    "\n",
    "            # Get metrics\n",
    "            train_loss = model.train_loss_metric.result()\n",
    "            train_acc = model.train_acc_metric.result()\n",
    "            val_loss = model.val_loss_metric.result()\n",
    "            val_acc = model.val_acc_metric.result()\n",
    "            cost_val.append(val_loss.numpy())\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Epoch: {epoch+1:04d} | Train Loss: {train_loss:.5f} | Train Acc: {train_acc:.5f} | \"\n",
    "                  f\"Val Loss: {val_loss:.5f} | Val Acc: {val_acc:.5f} | Time: {time.time()-start_time:.5f}\")\n",
    "\n",
    "            # Early stopping (matches original logic)\n",
    "            if epoch > config['early_stopping']:\n",
    "                recent_val_loss = cost_val[-(config['early_stopping']+1):-1]\n",
    "                if val_loss > np.mean(recent_val_loss):\n",
    "                    print(\"Early stopping triggered!\")\n",
    "                    break\n",
    "\n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                model.save_weights(bestModelSavePath0)  # TF2 model.save_weights\n",
    "\n",
    "        # Load best model weights\n",
    "        model.load_weights(bestModelSavePath0)\n",
    "        print(\"\\nOptimization Finished!\")\n",
    "\n",
    "        # --------------------------\n",
    "        # 8.5 Evaluate Phase 1 model\n",
    "        # --------------------------\n",
    "        print(\"\\n=== Evaluate Phase 1 Model ===\")\n",
    "        model.reset_metrics()\n",
    "        for batch in val_batches:\n",
    "            model.test_step(batch)\n",
    "        test_loss = model.val_loss_metric.result()\n",
    "        test_acc = model.val_acc_metric.result()\n",
    "        print(f\"Test Loss: {test_loss:.5f} | Test Acc: {test_acc:.5f}\")\n",
    "\n",
    "        # Predict on test set\n",
    "        y_pred = model([features_test, support_test], training=False).numpy()\n",
    "        y_pred_class = np.round(y_pred)\n",
    "\n",
    "        # Compute metrics (same as original)\n",
    "        [[TN, FP], [FN, TP]] = confusion_matrix(y_test, y_pred_class, labels=[0, 1]).astype(float)\n",
    "        acc = (TP + TN) / (TP + TN + FP + FN)\n",
    "        specificity = TN / (FP + TN)\n",
    "        sensitivity = recall = TP / (TP + FN)\n",
    "        fscore = f1_score(y_test, y_pred_class)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        print(f\"Accuracy: {acc:.5f} | Sensitivity: {sensitivity:.5f} | Specificity: {specificity:.5f} | \"\n",
    "              f\"F1: {fscore:.5f} | AUC: {roc_auc:.5f}\")\n",
    "        result = [acc, sensitivity, specificity, fscore, roc_auc]\n",
    "        l1 = [test_acc.numpy()]\n",
    "\n",
    "        # --------------------------\n",
    "        # 8.6 Phase 2: Train mask M (freeze other weights)\n",
    "        # --------------------------\n",
    "        print(\"\\n=== Phase 2: Train Mask M ===\")\n",
    "        cost_val_mask = []\n",
    "        for epoch in range(config['epochs_mask']):\n",
    "            model.reset_metrics()\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Train on all batches (only update M)\n",
    "            for batch in train_batches:\n",
    "                model.train_step(batch, training_mask=True)\n",
    "\n",
    "            # Validate on all batches\n",
    "            for batch in val_batches:\n",
    "                model.test_step(batch)\n",
    "\n",
    "            # Get metrics\n",
    "            train_loss = model.train_loss_metric.result()\n",
    "            train_acc = model.train_acc_metric.result()\n",
    "            val_loss = model.val_loss_metric.result()\n",
    "            val_acc = model.val_acc_metric.result()\n",
    "            cost_val_mask.append(val_loss.numpy())\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Epoch: {epoch+1:04d} | Train Loss: {train_loss:.5f} | Train Acc: {train_acc:.5f} | \"\n",
    "                  f\"Val Loss: {val_loss:.5f} | Val Acc: {val_acc:.5f} | Time: {time.time()-start_time:.5f}\")\n",
    "\n",
    "        # Save model with optimized mask\n",
    "        model.save_weights(bestModelSavePath1)\n",
    "        print(\"\\nMask Optimization Finished!\")\n",
    "\n",
    "        # --------------------------\n",
    "        # 8.7 Evaluate Phase 2 (masked model)\n",
    "        # --------------------------\n",
    "        print(\"\\n=== Evaluate Phase 2 (Masked Model) ===\")\n",
    "        model.reset_metrics()\n",
    "        for batch in val_batches:\n",
    "            model.test_step(batch)\n",
    "        test_loss_mask = model.val_loss_metric.result()\n",
    "        test_acc_mask = model.val_acc_metric.result()\n",
    "        print(f\"Test Loss (Masked): {test_loss_mask:.5f} | Test Acc (Masked): {test_acc_mask:.5f}\")\n",
    "        l2 = [test_acc_mask.numpy()]\n",
    "\n",
    "        # --------------------------\n",
    "        # 8.8 Save mask\n",
    "        # --------------------------\n",
    "        # Get sigmoid mask (same as original)\n",
    "        s_m = tf.sigmoid(model.M).numpy()\n",
    "        # Save directory\n",
    "        save_dir = \"/Users/celery/Research/GAT-Li-Revisited/weights\" #/home/celery/Data/ABIDE/ABIDE/weights\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f\"fold{fold_idx}_mask.pkl\")\n",
    "        # Save mask\n",
    "        with open(save_path, 'wb+') as f:\n",
    "            pkl.dump(s_m, f)\n",
    "        print(f\"\\nMask saved to: {save_path}\")\n",
    "\n",
    "        # Print final results\n",
    "        print(f\"\\nFinal Results - Phase 1 Acc: {l1} | Phase 2 Acc: {l2}\")\n",
    "        print(f\"Full Metrics: {result}\")\n",
    "\n",
    "        break  # Exit after target fold\n",
    "\n",
    "# Run main\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAT_Newest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
